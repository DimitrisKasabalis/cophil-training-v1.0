{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR\"\n",
        "subtitle: \"Practical Implementation for Disaster Risk Reduction\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: show\n",
        "    code-tools: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    number-sections: false\n",
        "    css: ../../styles/custom.css\n",
        "date: last-modified\n",
        "author: \"CoPhil Advanced Training Program\"\n",
        "execute:\n",
        "  enabled: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 Educational Note: Synthetic Data\n",
        "\n",
        "This notebook uses **synthetic SAR data** for immediate execution and learning. The U-Net architecture, training workflow, and evaluation metrics are identical to real-world applications.\n",
        "\n",
        "**Benefits:**\n",
        "- ✅ No data download required\n",
        "- ✅ Runs in 5-10 minutes (vs. hours for real data preprocessing)\n",
        "- ✅ Perfect for understanding the workflow\n",
        "- ✅ Easy to experiment and modify\n",
        "\n",
        "**For production work:** Replace synthetic data with real Sentinel-1 SAR from Google Earth Engine or the CoPhil Mirror Site. See the [Data Acquisition Guide](../DATA_GUIDE.md) for details.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "# Deep learning framework (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Metrics and evaluation\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\\\"TensorFlow version: {tf.__version__}\\\")\n",
        "print(f\\\"GPU Available: {tf.config.list_physical_devices('GPU')}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Dataset\n",
        "\n",
        "::: callout-note\n",
        "## Dataset Information\n",
        "\n",
        "**Size:** ~450MB compressed  \n",
        "**Contents:** \n",
        "- ~800 training image patches (256×256, VV+VH)\n",
        "- ~200 validation patches\n",
        "- ~200 test patches\n",
        "- Binary flood masks for all patches\n",
        "\n",
        "**Pre-processing Applied:**\n",
        "- Speckle filtering (Lee filter, 7×7 window)\n",
        "- Radiometric calibration to σ0 (dB)\n",
        "- Geometric terrain correction\n",
        "- Resampling to 10m resolution\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Synthetic SAR Data\n",
        "\n",
        "::: {.callout-note}\n",
        "## Synthetic Data Approach\n",
        "\n",
        "For this lab, we'll generate **synthetic SAR data** that mimics real Sentinel-1 characteristics. This allows you to:\n",
        "- ✅ Run the notebook immediately without downloads\n",
        "- ✅ Understand data structure and formats\n",
        "- ✅ Practice the complete U-Net workflow\n",
        "- ✅ Learn model training and evaluation\n",
        "\n",
        "**The workflow is identical to using real data** - only the data source differs. See the [Data Acquisition Guide](../DATA_GUIDE.md) for instructions on obtaining real Central Luzon SAR flood data.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_sar_flood_data(n_train=800, n_val=200, n_test=200, \n",
        "                                       img_size=256, seed=42):\n",
        "    \"\"\"\n",
        "    Generate synthetic SAR flood mapping dataset\n",
        "    \n",
        "    Simulates Sentinel-1 dual-polarization (VV, VH) imagery with flood masks\n",
        "    \n",
        "    Args:\n",
        "        n_train: Number of training samples\n",
        "        n_val: Number of validation samples\n",
        "        n_test: Number of test samples\n",
        "        img_size: Image dimension (default 256x256)\n",
        "        seed: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with paths to generated data\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    print(\"Generating synthetic SAR flood data...\")\n",
        "    print(f\"Train: {n_train}, Val: {n_val}, Test: {n_test} samples\")\n",
        "    \n",
        "    # Create directory structure\n",
        "    data_dir = '/content/data/flood_mapping_dataset'\n",
        "    for subset in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(data_dir, subset, 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, subset, 'masks'), exist_ok=True)\n",
        "    \n",
        "    def generate_sample(idx, subset):\n",
        "        \"\"\"Generate one SAR image + flood mask pair\"\"\"\n",
        "        \n",
        "        # Simulate SAR backscatter (in dB)\n",
        "        # VV: -25 to 5 dB (typical range)\n",
        "        # VH: -30 to 0 dB (typical range)\n",
        "        vv = np.random.normal(-10, 5, (img_size, img_size))\n",
        "        vh = np.random.normal(-15, 5, (img_size, img_size))\n",
        "        \n",
        "        # Create flood mask with realistic patterns\n",
        "        # Floods appear as connected regions (not random noise)\n",
        "        \n",
        "        # Start with base mask\n",
        "        mask = np.zeros((img_size, img_size), dtype=np.float32)\n",
        "        \n",
        "        # Add 1-3 flood regions per image\n",
        "        n_floods = np.random.randint(1, 4)\n",
        "        \n",
        "        for _ in range(n_floods):\n",
        "            # Random flood center\n",
        "            center_x = np.random.randint(50, img_size-50)\n",
        "            center_y = np.random.randint(50, img_size-50)\n",
        "            \n",
        "            # Random flood size (elliptical shape)\n",
        "            radius_x = np.random.randint(20, 80)\n",
        "            radius_y = np.random.randint(20, 80)\n",
        "            \n",
        "            # Create elliptical flood region\n",
        "            y, x = np.ogrid[:img_size, :img_size]\n",
        "            ellipse = ((x - center_x)**2 / radius_x**2 + \n",
        "                      (y - center_y)**2 / radius_y**2 <= 1)\n",
        "            mask[ellipse] = 1.0\n",
        "        \n",
        "        # Apply Gaussian smoothing to make edges more realistic\n",
        "        from scipy.ndimage import gaussian_filter\n",
        "        mask = gaussian_filter(mask, sigma=2.0)\n",
        "        mask = (mask > 0.3).astype(np.float32)  # Threshold\n",
        "        \n",
        "        # Modify SAR values in flooded regions\n",
        "        # Flooded areas have LOW backscatter (dark in SAR)\n",
        "        flood_mask_bool = mask > 0.5\n",
        "        vv[flood_mask_bool] = np.random.normal(-20, 3, flood_mask_bool.sum())\n",
        "        vh[flood_mask_bool] = np.random.normal(-25, 3, flood_mask_bool.sum())\n",
        "        \n",
        "        # Non-flooded areas have HIGHER backscatter\n",
        "        non_flood = ~flood_mask_bool\n",
        "        vv[non_flood] = np.random.normal(-5, 4, non_flood.sum())\n",
        "        vh[non_flood] = np.random.normal(-10, 4, non_flood.sum())\n",
        "        \n",
        "        # Clip to realistic SAR ranges\n",
        "        vv = np.clip(vv, -30, 10)\n",
        "        vh = np.clip(vh, -35, 5)\n",
        "        \n",
        "        # Stack VV and VH\n",
        "        sar_image = np.stack([vv, vh], axis=-1).astype(np.float32)\n",
        "        \n",
        "        # Expand mask dimension\n",
        "        mask = np.expand_dims(mask, axis=-1).astype(np.float32)\n",
        "        \n",
        "        # Save\n",
        "        img_path = os.path.join(data_dir, subset, 'images', f'sar_{idx:04d}.npy')\n",
        "        mask_path = os.path.join(data_dir, subset, 'masks', f'mask_{idx:04d}.npy')\n",
        "        \n",
        "        np.save(img_path, sar_image)\n",
        "        np.save(mask_path, mask)\n",
        "    \n",
        "    # Generate all samples\n",
        "    print(\"Generating training samples...\")\n",
        "    for i in range(n_train):\n",
        "        generate_sample(i, 'train')\n",
        "        if (i+1) % 200 == 0:\n",
        "            print(f\"  Generated {i+1}/{n_train} training samples\")\n",
        "    \n",
        "    print(\"Generating validation samples...\")\n",
        "    for i in range(n_val):\n",
        "        generate_sample(i, 'val')\n",
        "    \n",
        "    print(\"Generating test samples...\")\n",
        "    for i in range(n_test):\n",
        "        generate_sample(i, 'test')\n",
        "    \n",
        "    print(f\"\\n✅ Synthetic dataset generated successfully!\")\n",
        "    print(f\"Location: {data_dir}\")\n",
        "    print(f\"Train: {n_train} samples\")\n",
        "    print(f\"Val: {n_val} samples\")\n",
        "    print(f\"Test: {n_test} samples\")\n",
        "    \n",
        "    return {\n",
        "        'data_dir': data_dir,\n",
        "        'n_train': n_train,\n",
        "        'n_val': n_val,\n",
        "        'n_test': n_test\n",
        "    }\n",
        "\n",
        "# Generate synthetic data (takes ~2-3 minutes)\n",
        "dataset_info = generate_synthetic_sar_flood_data(\n",
        "    n_train=800,  # 800 training samples\n",
        "    n_val=200,    # 200 validation samples\n",
        "    n_test=200,   # 200 test samples\n",
        "    img_size=256,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "DATA_DIR = dataset_info['data_dir']\n",
        "print(f\"\\nDataset ready at: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Data Exploration\n",
        "\n",
        "### Load Sample Data\n",
        "\n",
        "Understanding your data is crucial before training. Let's explore the SAR imagery and flood masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample_data(data_dir, subset='train', n_samples=5):\n",
        "    \"\"\"Load sample SAR images and masks\"\"\"\n",
        "    img_dir = os.path.join(data_dir, subset, 'images')\n",
        "    mask_dir = os.path.join(data_dir, subset, 'masks')\n",
        "    \n",
        "    img_files = sorted(glob(os.path.join(img_dir, '*.npy')))[:n_samples]\n",
        "    mask_files = sorted(glob(os.path.join(mask_dir, '*.npy')))[:n_samples]\n",
        "    \n",
        "    images = [np.load(f) for f in img_files]\n",
        "    masks = [np.load(f) for f in mask_files]\n",
        "    \n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load samples\n",
        "sample_images, sample_masks = load_sample_data(DATA_DIR, 'train', n_samples=5)\n",
        "print(f\"Sample images shape: {sample_images.shape}\")  # (5, 256, 256, 2)\n",
        "print(f\"Sample masks shape: {sample_masks.shape}\")    # (5, 256, 256, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize SAR Data\n",
        "\n",
        "::: {.callout-tip}\n",
        "## Understanding SAR Backscatter\n",
        "\n",
        "**VV Polarization:** Vertical transmit, vertical receive\n",
        "- Better for detecting open water (low backscatter)\n",
        "- Values typically -30 to 10 dB\n",
        "\n",
        "**VH Polarization:** Vertical transmit, horizontal receive  \n",
        "- Sensitive to volume scattering (vegetation, urban areas)\n",
        "- Helps distinguish water from wet soil\n",
        "\n",
        "**Flood Detection:** Flooded areas appear **dark** (low backscatter) in both polarizations\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_sar_samples(images, masks, n_samples=3):\n",
        "    \"\"\"Visualize SAR images (VV, VH) and flood masks\"\"\"\n",
        "    fig, axes = plt.subplots(n_samples, 4, figsize=(16, n_samples*4))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # VV polarization\n",
        "        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray', vmin=-25, vmax=5)\n",
        "        axes[i, 0].set_title(f'Sample {i+1}: VV (dB)')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # VH polarization\n",
        "        axes[i, 1].imshow(images[i, :, :, 1], cmap='gray', vmin=-30, vmax=0)\n",
        "        axes[i, 1].set_title(f'Sample {i+1}: VH (dB)')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Flood mask (ground truth)\n",
        "        axes[i, 2].imshow(masks[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(f'Ground Truth Mask')\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        # Overlay on VV\n",
        "        overlay = images[i, :, :, 0].copy()\n",
        "        overlay_rgb = plt.cm.gray((overlay + 25) / 30)[:, :, :3]\n",
        "        mask_overlay = masks[i, :, :, 0]\n",
        "        overlay_rgb[mask_overlay > 0.5] = [0, 0.5, 1]  # Blue for flood\n",
        "        axes[i, 3].imshow(overlay_rgb)\n",
        "        axes[i, 3].set_title(f'Overlay: Flood in Blue')\n",
        "        axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_sar_samples(sample_images, sample_masks, n_samples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SAR Data Statistics:\")\n",
        "print(f\"VV min: {sample_images[:,:,:,0].min():.2f} dB\")\n",
        "print(f\"VV max: {sample_images[:,:,:,0].max():.2f} dB\")\n",
        "print(f\"VV mean: {sample_images[:,:,:,0].mean():.2f} dB\")\n",
        "print(f\"VH min: {sample_images[:,:,:,1].min():.2f} dB\")\n",
        "print(f\"VH max: {sample_images[:,:,:,1].max():.2f} dB\")\n",
        "print(f\"VH mean: {sample_images[:,:,:,1].mean():.2f} dB\")\n",
        "\n",
        "print(\"\\nFlood Mask Statistics:\")\n",
        "flood_ratio = sample_masks.mean() * 100\n",
        "print(f\"Flood pixels: {flood_ratio:.2f}%\")\n",
        "print(f\"Non-flood pixels: {100-flood_ratio:.2f}%\")\n",
        "print(f\"Class imbalance ratio: 1:{(100-flood_ratio)/flood_ratio:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Data Preprocessing\n",
        "\n",
        "### Normalization Strategy\n",
        "\n",
        "SAR data requires proper normalization for neural network training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_sar(image, method='minmax'):\n",
        "    \"\"\"\n",
        "    Normalize SAR backscatter values\n",
        "    \n",
        "    Methods:\n",
        "    - 'minmax': Scale to [0, 1] based on typical SAR range\n",
        "    - 'zscore': Standardize to mean=0, std=1\n",
        "    \"\"\"\n",
        "    if method == 'minmax':\n",
        "        # Typical SAR range: -30 to 10 dB\n",
        "        vv_normalized = (image[:, :, 0] + 30) / 40  # Scale VV\n",
        "        vh_normalized = (image[:, :, 1] + 35) / 35  # Scale VH\n",
        "        return np.stack([vv_normalized, vh_normalized], axis=-1)\n",
        "    \n",
        "    elif method == 'zscore':\n",
        "        # Standardize each channel\n",
        "        mean = image.mean(axis=(0, 1), keepdims=True)\n",
        "        std = image.std(axis=(0, 1), keepdims=True)\n",
        "        return (image - mean) / (std + 1e-8)\n",
        "\n",
        "# Test normalization\n",
        "normalized_sample = normalize_sar(sample_images[0], method='minmax')\n",
        "print(f\"Normalized range: [{normalized_sample.min():.3f}, {normalized_sample.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation\n",
        "\n",
        "::: {.callout-important}\n",
        "## Critical: Augment Image AND Mask Together\n",
        "\n",
        "For segmentation, **both the image and mask must receive identical transformations**. Augmenting only the image will cause misalignment.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_data(image, mask, augment=True):\n",
        "    \"\"\"Apply data augmentation to image and mask\"\"\"\n",
        "    if not augment:\n",
        "        return image, mask\n",
        "    \n",
        "    # Random horizontal flip\n",
        "    if np.random.random() > 0.5:\n",
        "        image = np.fliplr(image)\n",
        "        mask = np.fliplr(mask)\n",
        "    \n",
        "    # Random vertical flip\n",
        "    if np.random.random() > 0.5:\n",
        "        image = np.flipud(image)\n",
        "        mask = np.flipud(mask)\n",
        "    \n",
        "    # Random 90-degree rotations (valid for nadir satellite views)\n",
        "    k = np.random.randint(0, 4)  # 0, 90, 180, 270 degrees\n",
        "    image = np.rot90(image, k)\n",
        "    mask = np.rot90(mask, k)\n",
        "    \n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tf_dataset(data_dir, subset='train', batch_size=16, augment=False):\n",
        "    \"\"\"Create TensorFlow dataset with preprocessing\"\"\"\n",
        "    img_dir = os.path.join(data_dir, subset, 'images')\n",
        "    mask_dir = os.path.join(data_dir, subset, 'masks')\n",
        "    \n",
        "    img_files = sorted(glob(os.path.join(img_dir, '*.npy')))\n",
        "    mask_files = sorted(glob(os.path.join(mask_dir, '*.npy')))\n",
        "    \n",
        "    def load_and_preprocess(img_path, mask_path):\n",
        "        # Load\n",
        "        img = np.load(img_path.numpy().decode('utf-8'))\n",
        "        mask = np.load(mask_path.numpy().decode('utf-8'))\n",
        "        \n",
        "        # Normalize\n",
        "        img = normalize_sar(img, method='minmax')\n",
        "        \n",
        "        # Augment\n",
        "        if augment:\n",
        "            img, mask = augment_data(img, mask, augment=True)\n",
        "        \n",
        "        return img.astype(np.float32), mask.astype(np.float32)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_files, mask_files))\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: tf.py_function(\n",
        "            load_and_preprocess, [x, y], [tf.float32, tf.float32]\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = create_tf_dataset(DATA_DIR, 'train', BATCH_SIZE, augment=True)\n",
        "val_dataset = create_tf_dataset(DATA_DIR, 'val', BATCH_SIZE, augment=False)\n",
        "test_dataset = create_tf_dataset(DATA_DIR, 'test', BATCH_SIZE, augment=False)\n",
        "\n",
        "print(f\"Train batches: {len(list(train_dataset))}\")\n",
        "print(f\"Val batches: {len(list(val_dataset))}\")\n",
        "print(f\"Test batches: {len(list(test_dataset))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: U-Net Model Implementation\n",
        "\n",
        "Now we'll implement the U-Net architecture from Session 1. This is where theory meets practice.\n",
        "\n",
        "### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unet_model(input_shape=(256, 256, 2), num_classes=1):\n",
        "    \"\"\"\n",
        "    U-Net architecture for binary flood segmentation\n",
        "    \n",
        "    Args:\n",
        "        input_shape: (height, width, channels) - (256, 256, 2) for VV+VH\n",
        "        num_classes: 1 for binary segmentation (sigmoid output)\n",
        "    \n",
        "    Returns:\n",
        "        Keras Model\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Encoder (Contracting Path)\n",
        "    # Block 1\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    # Block 2\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "    \n",
        "    # Block 3\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "    \n",
        "    # Block 4\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "    \n",
        "    # Bottleneck\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "    \n",
        "    # Decoder (Expansive Path)\n",
        "    # Block 6\n",
        "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c4])  # Skip connection\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "    \n",
        "    # Block 7\n",
        "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c3])  # Skip connection\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "    \n",
        "    # Block 8\n",
        "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = layers.concatenate([u8, c2])  # Skip connection\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "    \n",
        "    # Block 9\n",
        "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = layers.concatenate([u9, c1])  # Skip connection\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)\n",
        "    \n",
        "    model = keras.Model(inputs=[inputs], outputs=[outputs], name='U-Net')\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = unet_model(input_shape=(256, 256, 2), num_classes=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Functions\n",
        "\n",
        "Implementing the loss functions from Session 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"Dice coefficient for evaluation\"\"\"\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    \"\"\"Dice loss for training\"\"\"\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"Combined Binary Cross-Entropy + Dice Loss\"\"\"\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    dice = dice_loss(y_true, y_pred)\n",
        "    return 0.5 * bce + 0.5 * dice\n",
        "\n",
        "def iou_score(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"IoU metric (Intersection over Union)\"\"\"\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Model Training\n",
        "\n",
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile with combined loss\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=combined_loss,\n",
        "    metrics=['accuracy', dice_coefficient, iou_score]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "os.makedirs('/content/logs', exist_ok=True)\n",
        "\n",
        "# Callbacks for training\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "    '/content/models/unet_flood_best.h5',\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop_cb = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "tensorboard_cb = callbacks.TensorBoard(\n",
        "    log_dir='/content/logs',\n",
        "    histogram_freq=1\n",
        ")\n",
        "\n",
        "callback_list = [checkpoint_cb, early_stop_cb, reduce_lr_cb, tensorboard_cb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Model\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Training Time Estimate\n",
        "\n",
        "- **With GPU (T4):** 15-25 minutes for 50 epochs\n",
        "- **With CPU:** 4-6 hours (not recommended)\n",
        "\n",
        "The model will likely converge in 20-30 epochs with early stopping.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callback_list,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
        "    axes[0, 0].set_title('Model Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Dice Coefficient\n",
        "    axes[0, 1].plot(history.history['dice_coefficient'], label='Train Dice')\n",
        "    axes[0, 1].plot(history.history['val_dice_coefficient'], label='Val Dice')\n",
        "    axes[0, 1].set_title('Dice Coefficient')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Dice')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # IoU Score\n",
        "    axes[1, 0].plot(history.history['iou_score'], label='Train IoU')\n",
        "    axes[1, 0].plot(history.history['val_iou_score'], label='Val IoU')\n",
        "    axes[0, 1].set_title('IoU Score')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('IoU')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[1, 1].plot(history.history['accuracy'], label='Train Acc')\n",
        "    axes[1, 1].plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    axes[1, 1].set_title('Pixel Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Model Evaluation\n",
        "\n",
        "### Load Best Model\n",
        "\n",
        "After training completes, load the best model weights (saved by ModelCheckpoint):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model = keras.models.load_model(\n",
        "    '/content/models/unet_flood_best.h5',\n",
        "    custom_objects={\n",
        "        'combined_loss': combined_loss,\n",
        "        'dice_coefficient': dice_coefficient,\n",
        "        'iou_score': iou_score\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"✓ Best model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test dataset\n",
        "test_results = best_model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Loss: {test_results[0]:.4f}\")\n",
        "print(f\"Pixel Accuracy: {test_results[1]:.4f}\")\n",
        "print(f\"Dice Coefficient: {test_results[2]:.4f}\")\n",
        "print(f\"IoU Score: {test_results[3]:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Metrics Calculation\n",
        "\n",
        "Calculate per-class precision, recall, and F1-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_detailed_metrics(model, dataset):\n",
        "    \"\"\"Calculate comprehensive segmentation metrics\"\"\"\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "    \n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        y_true_all.append(masks.numpy().flatten())\n",
        "        y_pred_all.append((predictions > 0.5).astype(np.float32).flatten())\n",
        "    \n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    \n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    # Confusion matrix components\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'true_positives': tp,\n",
        "        'true_negatives': tn,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn\n",
        "    }\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_detailed_metrics(best_model, test_dataset)\n",
        "\n",
        "print(\"\\nDETAILED METRICS (Flood Class)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
        "print(f\"\\nTrue Positives: {metrics['true_positives']:,}\")\n",
        "print(f\"True Negatives: {metrics['true_negatives']:,}\")\n",
        "print(f\"False Positives: {metrics['false_positives']:,}\")\n",
        "print(f\"False Negatives: {metrics['false_negatives']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(metrics):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = np.array([\n",
        "        [metrics['true_negatives'], metrics['false_positives']],\n",
        "        [metrics['false_negatives'], metrics['true_positives']]\n",
        "    ])\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues', \n",
        "                xticklabels=['Non-Flood', 'Flood'],\n",
        "                yticklabels=['Non-Flood', 'Flood'])\n",
        "    plt.title('Confusion Matrix - Flood Detection')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "## Interpreting Results\n",
        "\n",
        "**Good Performance Indicators:**\n",
        "- **IoU > 0.70:** Strong overlap between prediction and ground truth\n",
        "- **High Precision:** Few false alarms (predicted flood where there's none)\n",
        "- **High Recall:** Catches most actual floods (few missed floods)\n",
        "- **F1 > 0.75:** Balanced performance\n",
        "\n",
        "**For Disaster Response:**\n",
        "- **Precision matters:** Avoid sending resources to non-flooded areas\n",
        "- **Recall matters more:** Don't miss flooded communities needing help\n",
        "- Trade-off depends on operational priorities\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: Visualization and Interpretation\n",
        "\n",
        "### Predict on Test Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, n_samples=5):\n",
        "    \"\"\"Visualize model predictions vs ground truth\"\"\"\n",
        "    # Get samples\n",
        "    images, masks = next(iter(dataset))\n",
        "    predictions = model.predict(images[:n_samples], verbose=0)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_samples, 4, figsize=(20, n_samples*5))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Original SAR VV\n",
        "        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
        "        axes[i, 0].set_title(f'SAR VV (Normalized)')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Ground Truth\n",
        "        axes[i, 1].imshow(masks[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 1].set_title('Ground Truth Mask')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        axes[i, 2].imshow(predictions[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(f'Prediction (IoU: {iou_score(masks[i:i+1], predictions[i:i+1]).numpy():.3f})')\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        # Overlay: Green=Correct, Red=FP, Yellow=FN\n",
        "        overlay = np.zeros((256, 256, 3))\n",
        "        gt = masks[i, :, :, 0] > 0.5\n",
        "        pred = predictions[i, :, :, 0] > 0.5\n",
        "        \n",
        "        # True Positives (Green)\n",
        "        overlay[gt & pred] = [0, 1, 0]\n",
        "        # False Positives (Red)\n",
        "        overlay[~gt & pred] = [1, 0, 0]\n",
        "        # False Negatives (Yellow)\n",
        "        overlay[gt & ~pred] = [1, 1, 0]\n",
        "        \n",
        "        axes[i, 3].imshow(overlay)\n",
        "        axes[i, 3].set_title('Overlay: Green=TP, Red=FP, Yellow=FN')\n",
        "        axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(best_model, test_dataset, n_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(model, dataset):\n",
        "    \"\"\"Analyze common error patterns\"\"\"\n",
        "    total_samples = 0\n",
        "    high_iou = 0  # IoU > 0.8\n",
        "    medium_iou = 0  # 0.5 < IoU <= 0.8\n",
        "    low_iou = 0  # IoU <= 0.5\n",
        "    \n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        \n",
        "        for i in range(len(images)):\n",
        "            iou = iou_score(masks[i:i+1], predictions[i:i+1]).numpy()\n",
        "            total_samples += 1\n",
        "            \n",
        "            if iou > 0.8:\n",
        "                high_iou += 1\n",
        "            elif iou > 0.5:\n",
        "                medium_iou += 1\n",
        "            else:\n",
        "                low_iou += 1\n",
        "    \n",
        "    print(f\"\\nERROR ANALYSIS (n={total_samples} patches)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"High Quality (IoU > 0.8): {high_iou} ({high_iou/total_samples*100:.1f}%)\")\n",
        "    print(f\"Medium Quality (0.5 < IoU ≤ 0.8): {medium_iou} ({medium_iou/total_samples*100:.1f}%)\")\n",
        "    print(f\"Poor Quality (IoU ≤ 0.5): {low_iou} ({low_iou/total_samples*100:.1f}%)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "analyze_errors(best_model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip}\n",
        "## Common Error Patterns\n",
        "\n",
        "**False Positives (Red areas):**\n",
        "- Wet soil after rain (similar backscatter to water)\n",
        "- Shadows in mountainous terrain\n",
        "- Very calm water bodies (pre-flood)\n",
        "\n",
        "**False Negatives (Yellow areas):**\n",
        "- Flooded vegetation (volume scattering increases backscatter)\n",
        "- Mixed pixels at flood boundaries\n",
        "- Speckle noise in SAR data\n",
        "\n",
        "**Improvement Strategies:**\n",
        "- Use multi-temporal data (before/after comparison)\n",
        "- Incorporate DEM (elevation-based flood likelihood)\n",
        "- Ensemble multiple models\n",
        "- Post-processing with GIS constraints\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8: Export and GIS Integration\n",
        "\n",
        "### Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model in different formats\n",
        "best_model.save('/content/models/unet_flood_final.h5')  # Full model\n",
        "best_model.save('/content/models/unet_flood_final.keras')  # New Keras format\n",
        "\n",
        "# Save to Google Drive for persistence\n",
        "!cp /content/models/unet_flood_final.h5 /content/drive/MyDrive/flood_mapping/\n",
        "\n",
        "print(\"✓ Model saved successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_predictions(model, dataset, output_dir='/content/outputs'):\n",
        "    \"\"\"Export predictions as NumPy arrays\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    batch_idx = 0\n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        \n",
        "        for i in range(len(images)):\n",
        "            # Save prediction\n",
        "            pred_file = os.path.join(output_dir, f'prediction_{batch_idx:04d}.npy')\n",
        "            np.save(pred_file, predictions[i])\n",
        "            \n",
        "            # Save binary mask (threshold at 0.5)\n",
        "            binary_file = os.path.join(output_dir, f'binary_mask_{batch_idx:04d}.npy')\n",
        "            binary_mask = (predictions[i] > 0.5).astype(np.uint8)\n",
        "            np.save(binary_file, binary_mask)\n",
        "            \n",
        "            batch_idx += 1\n",
        "    \n",
        "    print(f\"✓ Exported {batch_idx} predictions to {output_dir}\")\n",
        "\n",
        "export_predictions(best_model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Flood Polygons (Conceptual)\n",
        "\n",
        "::: {.callout-note}\n",
        "## GIS Integration Workflow\n",
        "\n",
        "**For operational use, follow these steps:**\n",
        "\n",
        "1. **Georeferencing:**\n",
        "   - Match predictions back to original SAR geocoordinates\n",
        "   - Use metadata from Sentinel-1 GRD products\n",
        "\n",
        "2. **Vectorization:**\n",
        "\n",
        "```python\n",
        "# Pseudocode - requires rasterio and geopandas\n",
        "import rasterio\n",
        "from rasterio.features import shapes\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape\n",
        "\n",
        "# Convert binary mask to polygons\n",
        "mask = (prediction > 0.5).astype(np.uint8)\n",
        "shapes_gen = shapes(mask, transform=affine_transform)\n",
        "polygons = [shape(s) for s, v in shapes_gen if v == 1]\n",
        "\n",
        "# Create GeoDataFrame\n",
        "gdf = gpd.GeoDataFrame({'geometry': polygons}, crs='EPSG:4326')\n",
        "gdf.to_file('flood_extent.geojson')\n",
        "```\n",
        "\n",
        "3. **Export Formats:**\n",
        "   - **GeoTIFF:** Raster format for GIS software\n",
        "   - **Shapefile/GeoJSON:** Vector format for flood polygons\n",
        "   - **KML:** For Google Earth visualization\n",
        "\n",
        "4. **Integration with QGIS/ArcGIS:**\n",
        "   - Load flood extent layer\n",
        "   - Overlay with administrative boundaries\n",
        "   - Calculate affected area and population\n",
        "   - Generate maps for disaster response teams\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Download Results\n",
        "\n",
        "```python\n",
        "# Zip outputs for download\n",
        "!zip -r /content/flood_mapping_results.zip /content/outputs /content/models\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp /content/flood_mapping_results.zip /content/drive/MyDrive/\n",
        "\n",
        "print(\"✓ Results ready for download from Google Drive\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n",
        "# Zip outputs for download\n",
        "!zip -r /content/flood_mapping_results.zip /content/outputs /content/models\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp /content/flood_mapping_results.zip /content/drive/MyDrive/\n",
        "\n",
        "print(\"✓ Results ready for download from Google Drive\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting Guide\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 1: Out of Memory (OOM) Errors\n",
        "\n",
        "**Symptoms:**\n",
        "- \"ResourceExhaustedError: OOM when allocating tensor\"\n",
        "- Training crashes during forward/backward pass\n",
        "\n",
        "**Solutions:**\n",
        "1. **Reduce batch size:**\n",
        "   ```python\n",
        "   BATCH_SIZE = 8  # Instead of 16\n",
        "   ```\n",
        "\n",
        "2. **Use mixed precision training:**\n",
        "   ```python\n",
        "   from tensorflow.keras import mixed_precision\n",
        "   policy = mixed_precision.Policy('mixed_float16')\n",
        "   mixed_precision.set_global_policy(policy)\n",
        "   ```\n",
        "\n",
        "3. **Clear memory between runs:**\n",
        "   ```python\n",
        "   from tensorflow.keras import backend as K\n",
        "   K.clear_session()\n",
        "   ```\n",
        "\n",
        "4. **Use smaller model:**\n",
        "   - Reduce filters in U-Net layers (64→32, 128→64, etc.)\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 2: Model Not Learning (Loss Plateau)\n",
        "\n",
        "**Symptoms:**\n",
        "- Loss stuck at high value (>0.5)\n",
        "- Validation metrics don't improve\n",
        "- Model predicts all zeros or all ones\n",
        "\n",
        "**Solutions:**\n",
        "1. **Check data normalization:**\n",
        "   ```python\n",
        "   # Verify normalized range\n",
        "   print(f\"Min: {images.min()}, Max: {images.max()}\")\n",
        "   # Should be in [0, 1] range\n",
        "   ```\n",
        "\n",
        "2. **Verify labels are correct:**\n",
        "   ```python\n",
        "   # Check mask values\n",
        "   print(f\"Unique mask values: {np.unique(masks)}\")\n",
        "   # Should be [0, 1] for binary\n",
        "   ```\n",
        "\n",
        "3. **Adjust learning rate:**\n",
        "   ```python\n",
        "   # Try higher initial LR\n",
        "   optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
        "   ```\n",
        "\n",
        "4. **Use stronger loss function:**\n",
        "   ```python\n",
        "   # Switch to pure Dice loss if class imbalance is severe\n",
        "   model.compile(optimizer=optimizer, loss=dice_loss, ...)\n",
        "   ```\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 3: Overfitting (High Train, Low Val Accuracy)\n",
        "\n",
        "**Symptoms:**\n",
        "- Training accuracy > 95%, validation < 80%\n",
        "- Large gap between train and val metrics\n",
        "- Model memorizes training data\n",
        "\n",
        "**Solutions:**\n",
        "1. **Increase data augmentation:**\n",
        "   ```python\n",
        "   # Add more aggressive augmentation\n",
        "   if augment:\n",
        "       # Add brightness adjustment\n",
        "       image = image * np.random.uniform(0.8, 1.2)\n",
        "       # Add Gaussian noise\n",
        "       image += np.random.normal(0, 0.05, image.shape)\n",
        "   ```\n",
        "\n",
        "2. **Add dropout layers:**\n",
        "   ```python\n",
        "   c1 = layers.Dropout(0.2)(c1)  # After conv blocks\n",
        "   ```\n",
        "\n",
        "3. **Reduce model complexity:**\n",
        "   ```python\n",
        "   # Use fewer filters or fewer blocks\n",
        "   ```\n",
        "\n",
        "4. **Get more training data:**\n",
        "   - Extract more patches from available imagery\n",
        "   - Use data from different typhoon events\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 4: Predictions All Black or All White\n",
        "\n",
        "**Symptoms:**\n",
        "- Model outputs all 0s or all 1s\n",
        "- No meaningful segmentation\n",
        "\n",
        "**Solutions:**\n",
        "1. **Check output activation:**\n",
        "   ```python\n",
        "   # Ensure sigmoid for binary\n",
        "   outputs = layers.Conv2D(1, 1, activation='sigmoid')(c9)\n",
        "   ```\n",
        "\n",
        "2. **Verify loss handles imbalance:**\n",
        "   ```python\n",
        "   # Use Dice or combined loss, not pure BCE\n",
        "   loss = combined_loss\n",
        "   ```\n",
        "\n",
        "3. **Check threshold:**\n",
        "   ```python\n",
        "   # Try different thresholds\n",
        "   binary_pred = (prediction > 0.3).astype(np.uint8)\n",
        "   ```\n",
        "\n",
        "4. **Inspect raw predictions:**\n",
        "   ```python\n",
        "   print(f\"Prediction range: {predictions.min():.3f} to {predictions.max():.3f}\")\n",
        "   # Should vary, not all same value\n",
        "   ```\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 5: Colab Disconnections\n",
        "\n",
        "**Symptoms:**\n",
        "- Session times out during training\n",
        "- \"Runtime disconnected\" message\n",
        "- Lost training progress\n",
        "\n",
        "**Solutions:**\n",
        "1. **Keep browser active:**\n",
        "   - Don't minimize tab\n",
        "   - Use Colab Pro for longer runtimes\n",
        "\n",
        "2. **Save checkpoints frequently:**\n",
        "   ```python\n",
        "   # Already configured in ModelCheckpoint callback\n",
        "   checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "       filepath='model.h5',\n",
        "       save_freq='epoch'  # Save every epoch\n",
        "   )\n",
        "   ```\n",
        "\n",
        "3. **Save to Google Drive:**\n",
        "   ```python\n",
        "   # Mount Drive and save there\n",
        "   drive.mount('/content/drive')\n",
        "   model.save('/content/drive/MyDrive/checkpoints/model_epoch_{epoch}.h5')\n",
        "   ```\n",
        "\n",
        "4. **Use console keepalive (JavaScript):**\n",
        "   ```javascript\n",
        "   // Run in browser console\n",
        "   function ClickConnect(){\n",
        "     console.log(\"Keeping alive\");\n",
        "     document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "   }\n",
        "   setInterval(ClickConnect, 60000)\n",
        "   ```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "::: {.callout-important}\n",
        "## What You've Accomplished\n",
        "\n",
        "**Technical Skills:**\n",
        "✅ Loaded and preprocessed Sentinel-1 SAR data for deep learning  \n",
        "✅ Implemented complete U-Net architecture from scratch  \n",
        "✅ Trained a segmentation model with appropriate loss functions  \n",
        "✅ Evaluated performance using multiple metrics (IoU, Dice, F1)  \n",
        "✅ Visualized and interpreted model predictions  \n",
        "✅ Exported results for GIS integration\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "✅ How SAR backscatter relates to flood detection  \n",
        "✅ Why skip connections are critical for precise segmentation  \n",
        "✅ How to handle class imbalance in segmentation tasks  \n",
        "✅ Trade-offs between precision and recall for disaster response  \n",
        "✅ Common error patterns and improvement strategies\n",
        "\n",
        "**Philippine DRR Context:**\n",
        "✅ Applied deep learning to real Typhoon Ulysses flood data  \n",
        "✅ Understood operational requirements for disaster response  \n",
        "✅ Prepared outputs for integration with PAGASA/DOST systems\n",
        ":::\n",
        "\n",
        "### Critical Lessons\n",
        "\n",
        "1. **Data Quality >> Model Complexity**\n",
        "   - Well-prepared SAR data is more important than model tweaks\n",
        "   - Ground truth quality directly impacts performance\n",
        "\n",
        "2. **Loss Function Selection Matters**\n",
        "   - Combined loss (BCE + Dice) works best for imbalanced flood data\n",
        "   - Pure cross-entropy fails when flood pixels are <10%\n",
        "\n",
        "3. **Evaluation Beyond Accuracy**\n",
        "   - Pixel accuracy misleading for imbalanced classes\n",
        "   - IoU and Dice give true performance picture\n",
        "   - Confusion matrix reveals error types\n",
        "\n",
        "4. **Operational Considerations**\n",
        "   - For disaster response, recall > precision (catch all floods)\n",
        "   - Speed matters: Train once, inference in minutes\n",
        "   - GIS integration essential for actionable outputs\n",
        "\n",
        "---\n",
        "\n",
        "## Resources and Further Learning\n",
        "\n",
        "### Datasets\n",
        "\n",
        "**Flood Mapping:**\n",
        "- [Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11) - Global flood dataset with Sentinel-1\n",
        "- [FloodNet](https://github.com/BinaLab/FloodNet-Supervised_v1.0) - High-resolution flood imagery\n",
        "- [UNOSAT Flood Portal](https://flood.unosat.org/) - Validated flood extent maps\n",
        "\n",
        "**SAR Data:**\n",
        "- [Copernicus Open Access Hub](https://scihub.copernicus.eu/) - Download Sentinel-1 GRD\n",
        "- [Alaska Satellite Facility (ASF)](https://search.asf.alaska.edu/) - SAR data archive\n",
        "- [Google Earth Engine](https://earthengine.google.com/) - Cloud-based SAR processing\n",
        "\n",
        "### Papers and Tutorials\n",
        "\n",
        "**U-Net and Segmentation:**\n",
        "- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - Original paper (Ronneberger et al., 2015)\n",
        "- [TensorFlow Image Segmentation Tutorial](https://www.tensorflow.org/tutorials/images/segmentation)\n",
        "- [PyTorch Semantic Segmentation](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
        "\n",
        "**SAR Flood Mapping:**\n",
        "- [Flood Detection with SAR: A Review](https://doi.org/10.3390/rs12020304) - Comprehensive review\n",
        "- [Deep Learning for SAR Image Analysis](https://arxiv.org/abs/2006.10027)\n",
        "- [Automated Flood Mapping Using Sentinel-1](https://doi.org/10.1016/j.isprsjprs.2020.08.012)\n",
        "\n",
        "**Loss Functions:**\n",
        "- [Dice Loss for Imbalanced Segmentation](https://arxiv.org/abs/1707.03237)\n",
        "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
        "- [Combo Loss: Handling Input and Output Imbalance](https://arxiv.org/abs/1805.02798)\n",
        "\n",
        "### Code Repositories\n",
        "\n",
        "- [Segmentation Models](https://github.com/qubvel/segmentation_models) - Pre-built architectures\n",
        "- [TorchGeo](https://github.com/microsoft/torchgeo) - PyTorch for geospatial data\n",
        "- [RasterVision](https://github.com/azavea/raster-vision) - End-to-end pipeline for EO\n",
        "\n",
        "### Philippine EO Context\n",
        "\n",
        "- **PhilSA Space+ Data Dashboard:** [https://data.philsa.gov.ph](https://data.philsa.gov.ph)\n",
        "- **DOST-ASTI DATOS:** Rapid mapping for disasters\n",
        "- **NAMRIA GeoPortal:** Hazard maps and basemaps\n",
        "- **PAGASA:** Weather and climate data\n",
        "\n",
        "---\n",
        "\n",
        "## Discussion Questions\n",
        "\n",
        "Before moving to Session 3, reflect on these questions:\n",
        "\n",
        "1. **Real-World Application:**\n",
        "   - How would you deploy this flood mapping system for real-time disaster response in your agency?\n",
        "   - What infrastructure and data pipelines would you need?\n",
        "\n",
        "2. **Model Limitations:**\n",
        "   - What types of floods might this model miss (based on SAR characteristics)?\n",
        "   - How would you validate predictions in areas with no ground truth?\n",
        "\n",
        "3. **Improvements:**\n",
        "   - If you had multi-temporal data (before and after), how would you modify the approach?\n",
        "   - How could you incorporate elevation data (DEM) to improve predictions?\n",
        "\n",
        "4. **Operational Challenges:**\n",
        "   - What's the acceptable latency for flood mapping in disaster response?\n",
        "   - How would you handle uncertainty quantification for decision-makers?\n",
        "\n",
        "5. **Ethical Considerations:**\n",
        "   - What happens if the model misses a flooded community (false negative)?\n",
        "   - How do you balance automation with human expertise in critical decisions?\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Results Summary\n",
        "\n",
        "After completing this lab, you should achieve:\n",
        "\n",
        "| Metric | Expected Range | Interpretation |\n",
        "|--------|----------------|----------------|\n",
        "| **IoU (Test)** | 0.65 - 0.80 | Good to excellent overlap |\n",
        "| **Dice Coefficient** | 0.70 - 0.85 | Strong agreement with ground truth |\n",
        "| **Precision** | 0.70 - 0.90 | Few false flood alarms |\n",
        "| **Recall** | 0.75 - 0.95 | Catches most actual floods |\n",
        "| **F1-Score** | 0.72 - 0.88 | Balanced performance |\n",
        "| **Training Time** | 15-30 min | With GPU (T4) |\n",
        "\n",
        "::: {.callout-tip}\n",
        "## If Your Results Are Lower\n",
        "\n",
        "**IoU < 0.60:**\n",
        "- Check data quality and normalization\n",
        "- Increase training epochs or adjust learning rate\n",
        "- Try different loss function combinations\n",
        "- Ensure adequate training data diversity\n",
        "\n",
        "**High Precision, Low Recall:**\n",
        "- Model is too conservative (missing floods)\n",
        "- Increase weight on positive class\n",
        "- Use Dice loss instead of BCE\n",
        "\n",
        "**High Recall, Low Precision:**\n",
        "- Model predicting too much flood\n",
        "- Add more negative examples to training\n",
        "- Use stricter threshold (>0.6 instead of >0.5)\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "::: {.callout-important}\n",
        "## Preparation for Session 3: Object Detection\n",
        "\n",
        "**Session 3** will introduce object detection techniques for identifying and localizing specific features in EO imagery.\n",
        "\n",
        "**Topics:**\n",
        "- R-CNN, YOLO, and SSD architectures\n",
        "- Bounding box regression\n",
        "- Anchor boxes and non-maximum suppression\n",
        "- Applications: Ship detection, building detection, vehicle counting\n",
        "\n",
        "**Preparation:**\n",
        "- Review CNN concepts from Day 2\n",
        "- Understand difference between segmentation (pixel-wise) and detection (bounding boxes)\n",
        "- Consider: What EO applications need object detection vs segmentation?\n",
        "\n",
        "[Preview Session 3 →](../sessions/session3.qmd){.btn .btn-outline-primary}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Lab Completion Checklist\n",
        "\n",
        "Before finishing, ensure you've completed:\n",
        "\n",
        "- [ ] Successfully trained U-Net model\n",
        "- [ ] Achieved IoU > 0.60 on test set\n",
        "- [ ] Visualized predictions vs ground truth\n",
        "- [ ] Analyzed error patterns\n",
        "- [ ] Saved trained model to Google Drive\n",
        "- [ ] Exported predictions\n",
        "- [ ] Understood key troubleshooting strategies\n",
        "- [ ] Thought about operational deployment\n",
        "\n",
        "::: {.callout-success}\n",
        "## Congratulations! 🎉\n",
        "\n",
        "You've completed a full deep learning pipeline for flood mapping using Sentinel-1 SAR and U-Net. This is a **production-ready workflow** used by disaster response agencies worldwide.\n",
        "\n",
        "**What You Built:**\n",
        "- A trained semantic segmentation model\n",
        "- Automated flood detection system\n",
        "- Export pipeline for GIS integration\n",
        "- Performance evaluation framework\n",
        "\n",
        "**Impact:**\n",
        "Your skills can now contribute to saving lives through rapid, accurate flood extent mapping for Philippine disaster response operations.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "::: {.session-nav}\n",
        "[← Back to Session 1](../sessions/session1.qmd){.btn .btn-outline-secondary}\n",
        "[Next: Session 3 - Object Detection →](../sessions/session3.qmd){.btn .btn-primary}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "*This hands-on lab is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative. Materials developed in collaboration with PhilSA, DOST-ASTI, and the European Space Agency.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
