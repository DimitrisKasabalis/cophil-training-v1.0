{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Session 4: Object Detection from Sentinel Imagery\"\n",
    "subtitle: \"Building Detection with Transfer Learning for Urban Monitoring\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    number-sections: false\n",
    "    css: ../../styles/custom.css\n",
    "date: last-modified\n",
    "author: \"CoPhil Advanced Training Program\"\n",
    "execute:\n",
    "  enabled: false\n",
    "  eval: false\n",
    "  echo: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üéì Lab Overview\n",
    "\n",
    "This hands-on session demonstrates **object detection for Earth observation** using transfer learning. You'll fine-tune a pre-trained object detection model to detect buildings and informal settlements in Metro Manila from satellite imagery.\n",
    "\n",
    "**Case Study:** Metro Manila Building Detection  \n",
    "**Duration:** 2.5 hours  \n",
    "**Platform:** Google Colab with GPU  \n",
    "**Dataset:** Synthetic Sentinel-2-like imagery (for rapid learning)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. ‚úÖ **Understand** transfer learning for object detection in EO applications\n",
    "2. ‚úÖ **Load and configure** pre-trained models from TensorFlow Hub\n",
    "3. ‚úÖ **Prepare** satellite imagery and annotations for object detection\n",
    "4. ‚úÖ **Implement** Non-Maximum Suppression (NMS) for post-processing\n",
    "5. ‚úÖ **Evaluate** model performance using mAP (mean Average Precision)\n",
    "6. ‚úÖ **Compare** different detection architectures (SSD, EfficientDet)\n",
    "7. ‚úÖ **Visualize** detected bounding boxes and analyze predictions\n",
    "8. ‚úÖ **Export** models for operational deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Structure\n",
    "\n",
    "| Step | Activity | Duration |\n",
    "|------|----------|----------|\n",
    "| **1** | Environment Setup & GPU Check | 5 min |\n",
    "| **2** | Dataset Generation & Exploration | 15 min |\n",
    "| **3** | Data Format & Annotation | 15 min |\n",
    "| **4** | Load Pre-trained Models | 15 min |\n",
    "| **5** | Architecture Comparison | 20 min |\n",
    "| **6** | Non-Maximum Suppression (NMS) | 20 min |\n",
    "| **7** | Evaluation with mAP | 25 min |\n",
    "| **8** | Advanced Visualization | 15 min |\n",
    "| **9** | Export & Deployment | 15 min |\n",
    "| **10** | Troubleshooting & Best Practices | 10 min |\n",
    "\n",
    "**Total:** ~150 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup (5 minutes)\n",
    "\n",
    "First, let's install required packages and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow-hub pycocotools tensorflow-addons\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# TensorFlow and related\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "Object detection benefits significantly from GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"\\n‚úì GPU(s) Available: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "    \n",
    "    # Enable memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"\\n‚úì GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU found - inference will use CPU (slower)\")\n",
    "    print(\"   Consider using Google Colab with GPU runtime\")\n",
    "\n",
    "print(\"\\n‚úì Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Dataset Generation & Exploration (15 minutes)\n",
    "\n",
    "For this lab, we'll generate **synthetic Sentinel-2-like urban imagery** with building annotations. This allows immediate execution while teaching the complete workflow.\n",
    "\n",
    "## Philippine Context: Metro Manila Urban Monitoring\n",
    "\n",
    "::: {.callout-note}\n",
    "## Why Building Detection Matters in Metro Manila\n",
    "\n",
    "**Location:** National Capital Region (NCR) - 17 cities, 16.7M population  \n",
    "**Challenge:** Rapid urbanization and informal settlement growth\n",
    "\n",
    "**Applications:**\n",
    "- **Disaster Risk Reduction:** Identify vulnerable settlements in flood zones\n",
    "- **Urban Planning:** Monitor informal settlements and infrastructure\n",
    "- **Population Estimation:** Building counts for demographic analysis  \n",
    "- **Change Detection:** Track urban expansion over time\n",
    "- **Resource Allocation:** Target social services and infrastructure development\n",
    "\n",
    "**Data Source:** Sentinel-2 Multispectral Imagery\n",
    "- 10m resolution (RGB bands)\n",
    "- 5-day revisit frequency\n",
    "- Free and open access\n",
    ":::\n",
    "\n",
    "### Generate Synthetic Dataset\n",
    "\n",
    "We'll create realistic urban scenes with buildings of varying sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urban_imagery(n_samples=100, img_size=320, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic Sentinel-2-like urban imagery with building annotations\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of images to generate\n",
    "        img_size: Image dimensions (default 320x320)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        images: List of RGB images (normalized to [0, 1])\n",
    "        boxes_list: List of bounding boxes per image [y1, x1, y2, x2] normalized\n",
    "        labels_list: List of class labels per image (all 1 for 'building')\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    print(f\"Generating {n_samples} synthetic urban scenes...\")\n",
    "    \n",
    "    images = []\n",
    "    boxes_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create base urban scene (gray/brown tones)\n",
    "        base_color = np.random.randint(60, 100, 3)\n",
    "        img = np.ones((img_size, img_size, 3), dtype=np.uint8) * base_color\n",
    "        \n",
    "        # Add texture (simulates roads, vegetation patches)\n",
    "        noise = np.random.randint(-15, 15, (img_size, img_size, 3))\n",
    "        img = img + noise\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Add random vegetation patches (darker green areas)\n",
    "        n_veg_patches = np.random.randint(1, 4)\n",
    "        for _ in range(n_veg_patches):\n",
    "            veg_x = np.random.randint(0, img_size-30)\n",
    "            veg_y = np.random.randint(0, img_size-30)\n",
    "            veg_size = np.random.randint(15, 40)\n",
    "            veg_color = np.array([50, 80, 50])  # Green-ish\n",
    "            img[veg_y:veg_y+veg_size, veg_x:veg_x+veg_size] = veg_color\n",
    "        \n",
    "        # Add buildings (3-10 per image)\n",
    "        n_buildings = np.random.randint(3, 11)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(n_buildings):\n",
    "            # Random building location\n",
    "            x = np.random.randint(10, img_size-60)\n",
    "            y = np.random.randint(10, img_size-60)\n",
    "            \n",
    "            # Random building size (small, medium, large)\n",
    "            size_type = np.random.choice(['small', 'medium', 'large'], p=[0.5, 0.3, 0.2])\n",
    "            if size_type == 'small':\n",
    "                w = np.random.randint(12, 25)\n",
    "                h = np.random.randint(12, 25)\n",
    "            elif size_type == 'medium':\n",
    "                w = np.random.randint(25, 45)\n",
    "                h = np.random.randint(25, 45)\n",
    "            else:  # large\n",
    "                w = np.random.randint(45, 70)\n",
    "                h = np.random.randint(45, 70)\n",
    "            \n",
    "            # Check bounds\n",
    "            if x + w >= img_size or y + h >= img_size:\n",
    "                continue\n",
    "            \n",
    "            # Draw building (bright rectangle - concrete/metal roofs)\n",
    "            building_color = np.random.randint(150, 230, 3)\n",
    "            img[y:y+h, x:x+w] = building_color\n",
    "            \n",
    "            # Add some building detail (darker edges for realism)\n",
    "            edge_width = 2\n",
    "            img[y:y+edge_width, x:x+w] = building_color * 0.7  # Top edge\n",
    "            img[y:y+h, x:x+edge_width] = building_color * 0.7  # Left edge\n",
    "            \n",
    "            # Normalized bbox [y1, x1, y2, x2] - COCO format\n",
    "            box = [y/img_size, x/img_size, (y+h)/img_size, (x+w)/img_size]\n",
    "            boxes.append(box)\n",
    "            labels.append(1)  # Class 1 = building\n",
    "        \n",
    "        # Normalize image to [0, 1]\n",
    "        img_normalized = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        images.append(img_normalized)\n",
    "        boxes_list.append(np.array(boxes, dtype=np.float32))\n",
    "        labels_list.append(np.array(labels, dtype=np.int32))\n",
    "        \n",
    "        if (i+1) % 25 == 0:\n",
    "            print(f\"  Generated {i+1}/{n_samples} images\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset generated successfully!\")\n",
    "    print(f\"   Total images: {len(images)}\")\n",
    "    print(f\"   Image shape: {images[0].shape}\")\n",
    "    print(f\"   Buildings per image: {np.mean([len(b) for b in boxes_list]):.1f} (avg)\")\n",
    "    \n",
    "    return images, boxes_list, labels_list\n",
    "\n",
    "# Generate dataset\n",
    "all_images, all_boxes, all_labels = generate_urban_imagery(\n",
    "    n_samples=100,\n",
    "    img_size=320,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "Split into train (70%), validation (15%), and test (15%) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 70/15/15\n",
    "n_train = int(0.70 * len(all_images))\n",
    "n_val = int(0.15 * len(all_images))\n",
    "\n",
    "train_images = all_images[:n_train]\n",
    "train_boxes = all_boxes[:n_train]\n",
    "train_labels = all_labels[:n_train]\n",
    "\n",
    "val_images = all_images[n_train:n_train+n_val]\n",
    "val_boxes = all_boxes[n_train:n_train+n_val]\n",
    "val_labels = all_labels[n_train:n_train+n_val]\n",
    "\n",
    "test_images = all_images[n_train+n_val:]\n",
    "test_boxes = all_boxes[n_train+n_val:]\n",
    "test_labels = all_labels[n_train+n_val:]\n",
    "\n",
    "print(f\"Dataset Split:\")\n",
    "print(f\"  Train: {len(train_images)} images, {sum(len(b) for b in train_boxes)} buildings\")\n",
    "print(f\"  Val:   {len(val_images)} images, {sum(len(b) for b in val_boxes)} buildings\")\n",
    "print(f\"  Test:  {len(test_images)} images, {sum(len(b) for b in test_boxes)} buildings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Visualize Sample Images\n",
    "\n",
    "Let's examine the synthetic urban scenes with building annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_annotated_images(images, boxes_list, labels_list, n_samples=6, title=\"Annotated Images\"):\n",
    "    \"\"\"\n",
    "    Visualize images with bounding box annotations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 11))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(n_samples, len(images))):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(images[i])\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        img_h, img_w = 320, 320\n",
    "        for box, label in zip(boxes_list[i], labels_list[i]):\n",
    "            y1, x1, y2, x2 = box\n",
    "            \n",
    "            # Convert normalized coords to pixel coords\n",
    "            x1_px, y1_px = x1 * img_w, y1 * img_h\n",
    "            x2_px, y2_px = x2 * img_w, y2 * img_h\n",
    "            width_px = x2_px - x1_px\n",
    "            height_px = y2_px - y1_px\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = Rectangle((x1_px, y1_px), width_px, height_px,\n",
    "                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            ax.text(x1_px, y1_px-3, f'Building', \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),\n",
    "                   fontsize=8, color='white', weight='bold')\n",
    "        \n",
    "        ax.set_title(f'Image {i+1}: {len(boxes_list[i])} buildings', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training samples\n",
    "visualize_annotated_images(\n",
    "    train_images, train_boxes, train_labels, \n",
    "    n_samples=6, \n",
    "    title=\"Training Set Samples (with Ground Truth Annotations)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "def calculate_dataset_stats(boxes_list):\n",
    "    \"\"\"Calculate bounding box statistics\"\"\"\n",
    "    all_widths = []\n",
    "    all_heights = []\n",
    "    all_areas = []\n",
    "    \n",
    "    for boxes in boxes_list:\n",
    "        for box in boxes:\n",
    "            y1, x1, y2, x2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            area = width * height\n",
    "            \n",
    "            all_widths.append(width)\n",
    "            all_heights.append(height)\n",
    "            all_areas.append(area)\n",
    "    \n",
    "    return all_widths, all_heights, all_areas\n",
    "\n",
    "train_widths, train_heights, train_areas = calculate_dataset_stats(train_boxes)\n",
    "\n",
    "print(\"\\nTraining Set Statistics:\")\n",
    "print(f\"  Total buildings: {len(train_widths)}\")\n",
    "print(f\"  Buildings per image: {len(train_widths)/len(train_images):.1f} (avg)\")\n",
    "print(f\"\\n  Bounding Box Width (normalized):\")\n",
    "print(f\"    Mean: {np.mean(train_widths):.3f}\")\n",
    "print(f\"    Std:  {np.std(train_widths):.3f}\")\n",
    "print(f\"    Range: [{np.min(train_widths):.3f}, {np.max(train_widths):.3f}]\")\n",
    "print(f\"\\n  Bounding Box Height (normalized):\")\n",
    "print(f\"    Mean: {np.mean(train_heights):.3f}\")\n",
    "print(f\"    Std:  {np.std(train_heights):.3f}\")\n",
    "print(f\"    Range: [{np.min(train_heights):.3f}, {np.max(train_heights):.3f}]\")\n",
    "print(f\"\\n  Bounding Box Area (normalized):\")\n",
    "print(f\"    Mean: {np.mean(train_areas):.4f}\")\n",
    "print(f\"    Std:  {np.std(train_areas):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Visualize Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Width distribution\n",
    "axes[0].hist(train_widths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(np.mean(train_widths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(train_widths):.3f}')\n",
    "axes[0].set_xlabel('Box Width (normalized)', fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontweight='bold')\n",
    "axes[0].set_title('Bounding Box Width Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Height distribution\n",
    "axes[1].hist(train_heights, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1].axvline(np.mean(train_heights), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(train_heights):.3f}')\n",
    "axes[1].set_xlabel('Box Height (normalized)', fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontweight='bold')\n",
    "axes[1].set_title('Bounding Box Height Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Area distribution\n",
    "axes[2].hist(train_areas, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[2].axvline(np.mean(train_areas), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(train_areas):.4f}')\n",
    "axes[2].set_xlabel('Box Area (normalized)', fontweight='bold')\n",
    "axes[2].set_ylabel('Count', fontweight='bold')\n",
    "axes[2].set_title('Bounding Box Area Distribution', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Dataset statistics look good!\")\n",
    "print(\"  Wide range of building sizes (small to large)\")\n",
    "print(\"  Realistic distribution for urban scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Data Format & Annotation Understanding (15 minutes)\n",
    "\n",
    "Understanding data formats is critical for object detection. Let's explore COCO format and conversions.\n",
    "\n",
    "## Common Annotation Formats\n",
    "\n",
    "| Format | Box Representation | Normalization | Used By |\n",
    "|--------|-------------------|---------------|----------|\n",
    "| **COCO** | [x, y, width, height] | Pixel coords | TensorFlow Object Detection API |\n",
    "| **Pascal VOC** | [xmin, ymin, xmax, ymax] | Pixel coords | PyTorch, many tools |\n",
    "| **YOLO** | [x_center, y_center, width, height] | Normalized [0,1] | YOLO models |\n",
    "| **TF Hub** | [y1, x1, y2, x2] | Normalized [0,1] | TensorFlow Hub models |\n",
    "\n",
    "Our data uses **TF Hub format**: `[y1, x1, y2, x2]` normalized to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format conversion functions\n",
    "def tfhub_to_coco(box, img_h, img_w):\n",
    "    \"\"\"Convert TF Hub format to COCO format\"\"\"\n",
    "    y1, x1, y2, x2 = box\n",
    "    x = x1 * img_w\n",
    "    y = y1 * img_h\n",
    "    width = (x2 - x1) * img_w\n",
    "    height = (y2 - y1) * img_h\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def tfhub_to_pascal_voc(box, img_h, img_w):\n",
    "    \"\"\"Convert TF Hub format to Pascal VOC format\"\"\"\n",
    "    y1, x1, y2, x2 = box\n",
    "    xmin = x1 * img_w\n",
    "    ymin = y1 * img_h\n",
    "    xmax = x2 * img_w\n",
    "    ymax = y2 * img_h\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def tfhub_to_yolo(box, img_h, img_w):\n",
    "    \"\"\"Convert TF Hub format to YOLO format\"\"\"\n",
    "    y1, x1, y2, x2 = box\n",
    "    x_center = (x1 + x2) / 2\n",
    "    y_center = (y1 + y2) / 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "# Test conversions\n",
    "sample_box = train_boxes[0][0]  # First box of first image\n",
    "img_h, img_w = 320, 320\n",
    "\n",
    "print(\"Format Conversion Example:\")\n",
    "print(f\"\\n  TF Hub (normalized):  {sample_box}\")\n",
    "print(f\"  COCO (pixels):        {tfhub_to_coco(sample_box, img_h, img_w)}\")\n",
    "print(f\"  Pascal VOC (pixels):  {tfhub_to_pascal_voc(sample_box, img_h, img_w)}\")\n",
    "print(f\"  YOLO (normalized):    {tfhub_to_yolo(sample_box, img_h, img_w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Load Pre-trained Models (15 minutes)\n",
    "\n",
    "Transfer learning uses pre-trained models as feature extractors. Let's load multiple architectures from TensorFlow Hub.\n",
    "\n",
    "## Available Pre-trained Models\n",
    "\n",
    "| Model | Speed | Accuracy | Best For |\n",
    "|-------|-------|----------|----------|\n",
    "| **SSD MobileNet** | Fast | Moderate | Real-time, mobile |\n",
    "| **EfficientDet D0** | Moderate | Good | Balanced |\n",
    "| **Faster R-CNN ResNet** | Slow | High | Accuracy-critical |\n",
    "\n",
    "We'll use **SSD MobileNet V2** for this lab (fast, good for learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pre-trained SSD MobileNet V2 from TensorFlow Hub...\")\n",
    "print(\"(This may take 1-2 minutes on first run)\\n\")\n",
    "\n",
    "# Load model from TF Hub\n",
    "model_url = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1\"\n",
    "detector = hub.load(model_url)\n",
    "\n",
    "print(\"‚úÖ Pre-trained model loaded successfully!\")\n",
    "print(\"\\nModel Details:\")\n",
    "print(\"  Architecture: SSD MobileNet V2 with FPN-Lite\")\n",
    "print(\"  Input size: 320x320 pixels\")\n",
    "print(\"  Pre-trained on: COCO dataset (80 object classes)\")\n",
    "print(\"  Use case: Transfer learning for building detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Test Model Inference\n",
    "\n",
    "Let's run the pre-trained model on one of our images to understand the output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detection(model, image):\n",
    "    \"\"\"\n",
    "    Run object detection on a single image\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained TF Hub detector\n",
    "        image: RGB image (H, W, 3) normalized to [0, 1]\n",
    "    \n",
    "    Returns:\n",
    "        detections: Dictionary with detection outputs\n",
    "    \"\"\"\n",
    "    # Convert to tensor and add batch dimension\n",
    "    input_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]  # (1, H, W, 3)\n",
    "    \n",
    "    # Run inference\n",
    "    detections = model(input_tensor)\n",
    "    \n",
    "    # Convert outputs to numpy\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Test on first training image\n",
    "test_img = train_images[0]\n",
    "detections = run_detection(detector, test_img)\n",
    "\n",
    "print(\"\\nDetection Output Format:\")\n",
    "print(f\"  Keys: {list(detections.keys())}\")\n",
    "print(f\"\\n  Number of detections: {detections['num_detections']}\")\n",
    "print(f\"  Detection boxes shape: {detections['detection_boxes'].shape}\")\n",
    "print(f\"  Detection scores shape: {detections['detection_scores'].shape}\")\n",
    "print(f\"  Detection classes shape: {detections['detection_classes'].shape}\")\n",
    "\n",
    "print(f\"\\n  Sample detection:\")\n",
    "print(f\"    Box (normalized): {detections['detection_boxes'][0]}\")\n",
    "print(f\"    Score: {detections['detection_scores'][0]:.3f}\")\n",
    "print(f\"    Class: {int(detections['detection_classes'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Understanding Model Outputs\n",
    "\n",
    "**detection_boxes**: Bounding box coordinates [y1, x1, y2, x2] normalized to [0, 1]  \n",
    "**detection_scores**: Confidence scores [0, 1] - higher is better  \n",
    "**detection_classes**: Class IDs from COCO dataset (1-80)  \n",
    "**num_detections**: Total number of detections (before filtering)\n",
    "\n",
    "**Note:** The model outputs many detections (typically 100) with varying confidence. We'll use **Non-Maximum Suppression (NMS)** to filter overlapping boxes.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 5: Architecture Comparison (20 minutes)\n",
    "\n",
    "Let's compare detection architectures to understand speed vs accuracy trade-offs.\n",
    "\n",
    "## Load Additional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model URLs from TensorFlow Hub\n",
    "models_info = {\n",
    "    'SSD MobileNet V2': {\n",
    "        'url': 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
    "        'input_size': 320,\n",
    "        'description': 'Fast single-stage detector, good for real-time'\n",
    "    },\n",
    "    'EfficientDet D0': {\n",
    "        'url': 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
    "        'input_size': 512,\n",
    "        'description': 'Balanced speed and accuracy, compound scaling'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Available Detection Models:\\n\")\n",
    "for name, info in models_info.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Input size: {info['input_size']}x{info['input_size']}\")\n",
    "    print(f\"    Description: {info['description']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° For this lab, we'll use SSD MobileNet V2 (already loaded)\")\n",
    "print(\"   To use EfficientDet, replace the model_url above and reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, images, n_runs=10):\n",
    "    \"\"\"\n",
    "    Benchmark model inference speed\n",
    "    \"\"\"\n",
    "    print(f\"Benchmarking inference speed ({n_runs} runs)...\")\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    _ = run_detection(model, images[0])\n",
    "    \n",
    "    # Timed runs\n",
    "    for i in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = run_detection(model, images[i])\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    fps = 1.0 / mean_time\n",
    "    \n",
    "    print(f\"\\n  Inference time: {mean_time*1000:.1f} ¬± {std_time*1000:.1f} ms\")\n",
    "    print(f\"  FPS: {fps:.1f}\")\n",
    "    print(f\"  Throughput: {fps*60:.0f} images/minute\")\n",
    "    \n",
    "    return mean_time, fps\n",
    "\n",
    "# Benchmark SSD MobileNet\n",
    "ssd_time, ssd_fps = benchmark_model(detector, test_images, n_runs=10)\n",
    "\n",
    "print(\"\\n‚úì Benchmark complete!\")\n",
    "if ssd_fps > 20:\n",
    "    print(\"  Performance: Suitable for near-real-time processing\")\n",
    "elif ssd_fps > 5:\n",
    "    print(\"  Performance: Good for batch processing\")\n",
    "else:\n",
    "    print(\"  Performance: Suitable for offline analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 6: Non-Maximum Suppression (NMS) (20 minutes)\n",
    "\n",
    "Object detectors output many overlapping boxes. **NMS** filters duplicates to keep only the best detection per object.\n",
    "\n",
    "## Understanding NMS\n",
    "\n",
    "**Problem:** Multiple boxes detect the same building  \n",
    "**Solution:** Keep box with highest confidence, suppress overlapping boxes  \n",
    "**Method:** Calculate IoU between boxes, remove boxes with IoU > threshold\n",
    "\n",
    "### Implement IoU Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two boxes\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: Boxes in format [y1, x1, y2, x2] (normalized or pixel)\n",
    "    \n",
    "    Returns:\n",
    "        iou: IoU score [0, 1]\n",
    "    \"\"\"\n",
    "    # Calculate intersection coordinates\n",
    "    y1_int = max(box1[0], box2[0])\n",
    "    x1_int = max(box1[1], box2[1])\n",
    "    y2_int = min(box1[2], box2[2])\n",
    "    x2_int = min(box1[3], box2[3])\n",
    "    \n",
    "    # Check if boxes overlap\n",
    "    if y2_int <= y1_int or x2_int <= x1_int:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    intersection = (y2_int - y1_int) * (x2_int - x1_int)\n",
    "    \n",
    "    # Calculate areas of both boxes\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Calculate union area\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Test IoU calculation\n",
    "print(\"IoU Calculation Examples:\\n\")\n",
    "\n",
    "# Perfect overlap\n",
    "box_a = [0.1, 0.1, 0.3, 0.3]\n",
    "box_b = [0.1, 0.1, 0.3, 0.3]\n",
    "iou = calculate_iou(box_a, box_b)\n",
    "print(f\"  Perfect overlap: IoU = {iou:.3f}\")\n",
    "\n",
    "# Partial overlap\n",
    "box_c = [0.2, 0.2, 0.4, 0.4]\n",
    "iou = calculate_iou(box_a, box_c)\n",
    "print(f\"  Partial overlap: IoU = {iou:.3f}\")\n",
    "\n",
    "# No overlap\n",
    "box_d = [0.5, 0.5, 0.7, 0.7]\n",
    "iou = calculate_iou(box_a, box_d)\n",
    "print(f\"  No overlap:      IoU = {iou:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Visualize IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_iou(box1, box2, img_size=320):\n",
    "    \"\"\"\n",
    "    Visualize two boxes and their IoU\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    \n",
    "    # Create blank image\n",
    "    ax.set_xlim(0, img_size)\n",
    "    ax.set_ylim(img_size, 0)  # Inverted Y-axis\n",
    "    \n",
    "    # Convert normalized to pixels\n",
    "    def to_pixels(box):\n",
    "        return [c * img_size for c in box]\n",
    "    \n",
    "    box1_px = to_pixels(box1)\n",
    "    box2_px = to_pixels(box2)\n",
    "    \n",
    "    # Draw boxes\n",
    "    rect1 = Rectangle((box1_px[1], box1_px[0]), \n",
    "                      box1_px[3]-box1_px[1], box1_px[2]-box1_px[0],\n",
    "                      linewidth=3, edgecolor='blue', facecolor='blue', alpha=0.3)\n",
    "    rect2 = Rectangle((box2_px[1], box2_px[0]), \n",
    "                      box2_px[3]-box2_px[1], box2_px[2]-box2_px[0],\n",
    "                      linewidth=3, edgecolor='red', facecolor='red', alpha=0.3)\n",
    "    \n",
    "    ax.add_patch(rect1)\n",
    "    ax.add_patch(rect2)\n",
    "    \n",
    "    # Calculate and display IoU\n",
    "    iou = calculate_iou(box1, box2)\n",
    "    ax.set_title(f'IoU = {iou:.3f}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('X (pixels)', fontweight='bold')\n",
    "    ax.set_ylabel('Y (pixels)', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='blue', alpha=0.3, edgecolor='blue', label='Box 1'),\n",
    "                      Patch(facecolor='red', alpha=0.3, edgecolor='red', label='Box 2')]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize different IoU scenarios\n",
    "print(\"Visualizing IoU Scenarios:\\n\")\n",
    "\n",
    "# High overlap (IoU ~ 0.5)\n",
    "visualize_iou([0.2, 0.2, 0.5, 0.5], [0.3, 0.3, 0.6, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Implement NMS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, scores, iou_threshold=0.5, score_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression to remove duplicate detections\n",
    "    \n",
    "    Args:\n",
    "        boxes: Array of bounding boxes [N, 4] in format [y1, x1, y2, x2]\n",
    "        scores: Array of confidence scores [N]\n",
    "        iou_threshold: IoU threshold for suppression (default 0.5)\n",
    "        score_threshold: Minimum score to keep detection (default 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        keep_indices: Indices of boxes to keep\n",
    "    \"\"\"\n",
    "    # Filter by score threshold\n",
    "    score_mask = scores >= score_threshold\n",
    "    boxes = boxes[score_mask]\n",
    "    scores = scores[score_mask]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return np.array([], dtype=np.int32)\n",
    "    \n",
    "    # Sort by scores (descending)\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    keep_indices = []\n",
    "    \n",
    "    while len(sorted_indices) > 0:\n",
    "        # Take box with highest score\n",
    "        current_idx = sorted_indices[0]\n",
    "        keep_indices.append(current_idx)\n",
    "        \n",
    "        if len(sorted_indices) == 1:\n",
    "            break\n",
    "        \n",
    "        # Calculate IoU with remaining boxes\n",
    "        current_box = boxes[current_idx]\n",
    "        remaining_boxes = boxes[sorted_indices[1:]]\n",
    "        \n",
    "        ious = np.array([calculate_iou(current_box, box) for box in remaining_boxes])\n",
    "        \n",
    "        # Keep boxes with IoU below threshold\n",
    "        keep_mask = ious < iou_threshold\n",
    "        sorted_indices = sorted_indices[1:][keep_mask]\n",
    "    \n",
    "    return np.array(keep_indices, dtype=np.int32)\n",
    "\n",
    "print(\"‚úì NMS function implemented\")\n",
    "print(\"\\nNMS Parameters:\")\n",
    "print(\"  iou_threshold: Remove boxes with IoU > 0.5 (default)\")\n",
    "print(\"  score_threshold: Keep boxes with score > 0.3 (default)\")\n",
    "print(\"\\nThese can be tuned based on your application needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Test NMS on Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on test image\n",
    "test_img = test_images[0]\n",
    "detections = run_detection(detector, test_img)\n",
    "\n",
    "# Extract boxes and scores\n",
    "pred_boxes = detections['detection_boxes']\n",
    "pred_scores = detections['detection_scores']\n",
    "\n",
    "print(f\"Before NMS: {len(pred_boxes)} detections\")\n",
    "print(f\"  Score range: [{pred_scores.min():.3f}, {pred_scores.max():.3f}]\")\n",
    "\n",
    "# Apply NMS\n",
    "keep_indices = non_max_suppression(\n",
    "    pred_boxes, \n",
    "    pred_scores, \n",
    "    iou_threshold=0.5, \n",
    "    score_threshold=0.3\n",
    ")\n",
    "\n",
    "filtered_boxes = pred_boxes[keep_indices]\n",
    "filtered_scores = pred_scores[keep_indices]\n",
    "\n",
    "print(f\"\\nAfter NMS: {len(filtered_boxes)} detections\")\n",
    "print(f\"  Score range: [{filtered_scores.min():.3f}, {filtered_scores.max():.3f}]\")\n",
    "print(f\"\\n  Reduction: {len(pred_boxes) - len(filtered_boxes)} boxes removed ({(1-len(filtered_boxes)/len(pred_boxes))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Visualize NMS Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_nms_comparison(image, boxes_before, scores_before, boxes_after, scores_after, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Visualize detections before and after NMS\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Before NMS\n",
    "    ax1.imshow(image)\n",
    "    count_before = 0\n",
    "    for box, score in zip(boxes_before, scores_before):\n",
    "        if score > threshold:\n",
    "            y1, x1, y2, x2 = box\n",
    "            h, w = 320, 320\n",
    "            rect = Rectangle((x1*w, y1*h), (x2-x1)*w, (y2-y1)*h,\n",
    "                           linewidth=2, edgecolor='yellow', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            count_before += 1\n",
    "    ax1.set_title(f'Before NMS: {count_before} detections', fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # After NMS\n",
    "    ax2.imshow(image)\n",
    "    for box, score in zip(boxes_after, scores_after):\n",
    "        y1, x1, y2, x2 = box\n",
    "        h, w = 320, 320\n",
    "        rect = Rectangle((x1*w, y1*h), (x2-x1)*w, (y2-y1)*h,\n",
    "                       linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x1*w, y1*h-5, f'{score:.2f}', \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lime', alpha=0.7),\n",
    "                fontsize=10, color='black', weight='bold')\n",
    "    ax2.set_title(f'After NMS: {len(boxes_after)} detections', fontsize=14, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.suptitle('Non-Maximum Suppression Effect', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize NMS effect\n",
    "visualize_nms_comparison(\n",
    "    test_img,\n",
    "    pred_boxes, pred_scores,\n",
    "    filtered_boxes, filtered_scores,\n",
    "    threshold=0.3\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì NMS successfully removed overlapping detections!\")\n",
    "print(\"  Yellow boxes (before): Many overlaps\")\n",
    "print(\"  Green boxes (after): Clean, single detection per object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 7: Evaluation with mAP (25 minutes)\n",
    "\n",
    "**mAP (mean Average Precision)** is the standard metric for object detection. It measures both localization accuracy (IoU) and classification confidence.\n",
    "\n",
    "## Understanding mAP\n",
    "\n",
    "**Components:**\n",
    "1. **IoU Threshold:** Prediction is \"correct\" if IoU with ground truth > threshold (typically 0.5)\n",
    "2. **Precision-Recall Curve:** How many detected are correct (precision) vs. how many actual objects found (recall)\n",
    "3. **Average Precision (AP):** Area under precision-recall curve\n",
    "4. **mAP:** Mean AP across all classes or IoU thresholds\n",
    "\n",
    "**Common mAP Variants:**\n",
    "- **mAP@0.5:** IoU threshold = 0.5 (PASCAL VOC standard)\n",
    "- **mAP@0.75:** IoU threshold = 0.75 (stricter)\n",
    "- **mAP@[0.5:0.95]:** Average over IoU thresholds from 0.5 to 0.95 (COCO standard)\n",
    "\n",
    "### Implement mAP Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(gt_boxes, pred_boxes, pred_scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision at a specific IoU threshold\n",
    "    \n",
    "    Args:\n",
    "        gt_boxes: Ground truth boxes [N, 4]\n",
    "        pred_boxes: Predicted boxes [M, 4]\n",
    "        pred_scores: Prediction scores [M]\n",
    "        iou_threshold: IoU threshold for correct detection\n",
    "    \n",
    "    Returns:\n",
    "        ap: Average Precision\n",
    "        precision: Precision values\n",
    "        recall: Recall values\n",
    "    \"\"\"\n",
    "    if len(pred_boxes) == 0:\n",
    "        return 0.0, np.array([]), np.array([])\n",
    "    \n",
    "    # Sort predictions by confidence (descending)\n",
    "    sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "    pred_boxes = pred_boxes[sorted_indices]\n",
    "    pred_scores = pred_scores[sorted_indices]\n",
    "    \n",
    "    # Track which ground truth boxes have been matched\n",
    "    gt_matched = np.zeros(len(gt_boxes), dtype=bool)\n",
    "    \n",
    "    # Track true positives and false positives\n",
    "    tp = np.zeros(len(pred_boxes))\n",
    "    fp = np.zeros(len(pred_boxes))\n",
    "    \n",
    "    for i, pred_box in enumerate(pred_boxes):\n",
    "        # Find best matching ground truth box\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for j, gt_box in enumerate(gt_boxes):\n",
    "            if gt_matched[j]:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = j\n",
    "        \n",
    "        # Check if detection is correct\n",
    "        if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "            tp[i] = 1\n",
    "            gt_matched[best_gt_idx] = True\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "    \n",
    "    # Calculate cumulative TP and FP\n",
    "    tp_cumsum = np.cumsum(tp)\n",
    "    fp_cumsum = np.cumsum(fp)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)\n",
    "    recall = tp_cumsum / (len(gt_boxes) + 1e-8)\n",
    "    \n",
    "    # Calculate AP (area under precision-recall curve)\n",
    "    # Use 11-point interpolation (PASCAL VOC style)\n",
    "    ap = 0\n",
    "    for t in np.arange(0, 1.1, 0.1):\n",
    "        if np.sum(recall >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(precision[recall >= t])\n",
    "        ap += p / 11\n",
    "    \n",
    "    return ap, precision, recall\n",
    "\n",
    "print(\"‚úì mAP calculation functions implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Calculate mAP on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_map(model, images, gt_boxes_list, iou_thresholds=[0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Evaluate mAP on a dataset\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating mAP on {len(images)} images...\\n\")\n",
    "    \n",
    "    ap_per_threshold = {}\n",
    "    \n",
    "    for iou_thresh in iou_thresholds:\n",
    "        aps = []\n",
    "        \n",
    "        for i, (image, gt_boxes) in enumerate(zip(images, gt_boxes_list)):\n",
    "            # Run detection\n",
    "            detections = run_detection(model, image)\n",
    "            pred_boxes = detections['detection_boxes']\n",
    "            pred_scores = detections['detection_scores']\n",
    "            \n",
    "            # Apply NMS\n",
    "            keep_indices = non_max_suppression(\n",
    "                pred_boxes, pred_scores, \n",
    "                iou_threshold=0.5, \n",
    "                score_threshold=0.3\n",
    "            )\n",
    "            pred_boxes = pred_boxes[keep_indices]\n",
    "            pred_scores = pred_scores[keep_indices]\n",
    "            \n",
    "            # Calculate AP\n",
    "            ap, _, _ = calculate_ap(gt_boxes, pred_boxes, pred_scores, iou_threshold=iou_thresh)\n",
    "            aps.append(ap)\n",
    "            \n",
    "            if (i+1) % 5 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(images)} images (IoU={iou_thresh})\")\n",
    "        \n",
    "        map_value = np.mean(aps)\n",
    "        ap_per_threshold[iou_thresh] = map_value\n",
    "        print(f\"\\n  mAP@{iou_thresh}: {map_value:.3f}\")\n",
    "    \n",
    "    return ap_per_threshold\n",
    "\n",
    "# Evaluate on test set\n",
    "map_results = evaluate_map(detector, test_images, test_boxes, iou_thresholds=[0.5, 0.75])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for thresh, map_val in map_results.items():\n",
    "    print(f\"mAP@{thresh}: {map_val:.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "### Visualize Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision-recall for one image\n",
    "sample_img = test_images[0]\n",
    "sample_gt = test_boxes[0]\n",
    "\n",
    "detections = run_detection(detector, sample_img)\n",
    "pred_boxes = detections['detection_boxes']\n",
    "pred_scores = detections['detection_scores']\n",
    "\n",
    "# Apply NMS\n",
    "keep_indices = non_max_suppression(pred_boxes, pred_scores, iou_threshold=0.5, score_threshold=0.1)\n",
    "pred_boxes = pred_boxes[keep_indices]\n",
    "pred_scores = pred_scores[keep_indices]\n",
    "\n",
    "# Calculate AP and get precision-recall values\n",
    "ap_05, precision, recall = calculate_ap(sample_gt, pred_boxes, pred_scores, iou_threshold=0.5)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, 'b-', linewidth=2, label=f'AP@0.5 = {ap_05:.3f}')\n",
    "plt.fill_between(recall, precision, alpha=0.2)\n",
    "plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "plt.title('Precision-Recall Curve (Sample Image)', fontsize=14, fontweight='bold')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Average Precision: {ap_05:.3f}\")\n",
    "print(f\"  This represents the area under the precision-recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Interpreting mAP Results\n",
    "\n",
    "**mAP Values:**\n",
    "- **> 0.80:** Excellent performance\n",
    "- **0.60 - 0.80:** Good performance\n",
    "- **0.40 - 0.60:** Moderate performance\n",
    "- **< 0.40:** Poor performance\n",
    "\n",
    "**For building detection:**\n",
    "- mAP@0.5 > 0.70 is typically acceptable\n",
    "- mAP@0.75 > 0.50 indicates good localization accuracy\n",
    "\n",
    "**Note:** Pre-trained model without fine-tuning may show lower performance. Fine-tuning on real Philippine data would significantly improve results.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 8: Advanced Visualization (15 minutes)\n",
    "\n",
    "Let's create comprehensive visualizations to analyze model performance.\n",
    "\n",
    "### Confidence Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all prediction scores from test set\n",
    "all_scores = []\n",
    "correct_scores = []\n",
    "incorrect_scores = []\n",
    "\n",
    "for image, gt_boxes in zip(test_images, test_boxes):\n",
    "    detections = run_detection(detector, image)\n",
    "    pred_boxes = detections['detection_boxes']\n",
    "    pred_scores = detections['detection_scores']\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep_indices = non_max_suppression(pred_boxes, pred_scores, iou_threshold=0.5, score_threshold=0.3)\n",
    "    pred_boxes = pred_boxes[keep_indices]\n",
    "    pred_scores = pred_scores[keep_indices]\n",
    "    \n",
    "    all_scores.extend(pred_scores)\n",
    "    \n",
    "    # Check which predictions are correct\n",
    "    for pred_box, pred_score in zip(pred_boxes, pred_scores):\n",
    "        # Find best matching GT box\n",
    "        best_iou = 0\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            best_iou = max(best_iou, iou)\n",
    "        \n",
    "        if best_iou >= 0.5:\n",
    "            correct_scores.append(pred_score)\n",
    "        else:\n",
    "            incorrect_scores.append(pred_score)\n",
    "\n",
    "# Plot score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All scores\n",
    "axes[0].hist(all_scores, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Common threshold')\n",
    "axes[0].set_xlabel('Confidence Score', fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontweight='bold')\n",
    "axes[0].set_title('All Prediction Scores', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Correct vs incorrect\n",
    "axes[1].hist(correct_scores, bins=20, alpha=0.7, color='green', label='Correct (IoU‚â•0.5)', edgecolor='black')\n",
    "axes[1].hist(incorrect_scores, bins=20, alpha=0.7, color='red', label='Incorrect (IoU<0.5)', edgecolor='black')\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1].set_xlabel('Confidence Score', fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontweight='bold')\n",
    "axes[1].set_title('Score Distribution by Correctness', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(f\"  Total predictions: {len(all_scores)}\")\n",
    "print(f\"  Correct (IoU‚â•0.5): {len(correct_scores)} ({len(correct_scores)/len(all_scores)*100:.1f}%)\")\n",
    "print(f\"  Incorrect (IoU<0.5): {len(incorrect_scores)} ({len(incorrect_scores)/len(all_scores)*100:.1f}%)\")\n",
    "print(f\"\\n  Mean score (correct): {np.mean(correct_scores):.3f}\")\n",
    "print(f\"  Mean score (incorrect): {np.mean(incorrect_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "### Error Analysis: Visualize Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_with_analysis(image, gt_boxes, pred_boxes, pred_scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize detections with error analysis\n",
    "    Green = True Positive (correct detection)\n",
    "    Red = False Positive (incorrect detection)\n",
    "    Yellow = False Negative (missed ground truth)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Track which GT boxes are matched\n",
    "    gt_matched = np.zeros(len(gt_boxes), dtype=bool)\n",
    "    \n",
    "    # Process predictions\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    \n",
    "    for pred_box, pred_score in zip(pred_boxes, pred_scores):\n",
    "        y1, x1, y2, x2 = pred_box\n",
    "        h, w = 320, 320\n",
    "        \n",
    "        # Find best matching GT\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        for i, gt_box in enumerate(gt_boxes):\n",
    "            if not gt_matched[i]:\n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = i\n",
    "        \n",
    "        # Color based on correctness\n",
    "        if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "            color = 'lime'\n",
    "            label = f'TP: {pred_score:.2f}'\n",
    "            gt_matched[best_gt_idx] = True\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            color = 'red'\n",
    "            label = f'FP: {pred_score:.2f}'\n",
    "            fp_count += 1\n",
    "        \n",
    "        rect = Rectangle((x1*w, y1*h), (x2-x1)*w, (y2-y1)*h,\n",
    "                       linewidth=2, edgecolor=color, facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(x1*w, y1*h-5, label,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),\n",
    "                fontsize=9, color='black', weight='bold')\n",
    "    \n",
    "    # Draw unmatched GT boxes (False Negatives)\n",
    "    fn_count = 0\n",
    "    for i, gt_box in enumerate(gt_boxes):\n",
    "        if not gt_matched[i]:\n",
    "            y1, x1, y2, x2 = gt_box\n",
    "            h, w = 320, 320\n",
    "            rect = Rectangle((x1*w, y1*h), (x2-x1)*w, (y2-y1)*h,\n",
    "                           linewidth=3, edgecolor='yellow', facecolor='none', linestyle='--')\n",
    "            plt.gca().add_patch(rect)\n",
    "            plt.text(x1*w, y1*h-5, 'FN (Missed)',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                    fontsize=9, color='black', weight='bold')\n",
    "            fn_count += 1\n",
    "    \n",
    "    plt.title(f'Detection Analysis | TP: {tp_count} | FP: {fp_count} | FN: {fn_count}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lime', label='True Positive (Correct)'),\n",
    "        Patch(facecolor='red', label='False Positive (Wrong)'),\n",
    "        Patch(facecolor='yellow', label='False Negative (Missed)')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return tp_count, fp_count, fn_count\n",
    "\n",
    "# Analyze several test images\n",
    "print(\"Error Analysis on Test Images:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    img = test_images[i]\n",
    "    gt = test_boxes[i]\n",
    "    \n",
    "    # Run detection\n",
    "    detections = run_detection(detector, img)\n",
    "    pred_boxes = detections['detection_boxes']\n",
    "    pred_scores = detections['detection_scores']\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep_indices = non_max_suppression(pred_boxes, pred_scores, iou_threshold=0.5, score_threshold=0.4)\n",
    "    pred_boxes = pred_boxes[keep_indices]\n",
    "    pred_scores = pred_scores[keep_indices]\n",
    "    \n",
    "    print(f\"Image {i+1}:\")\n",
    "    tp, fp, fn = visualize_detections_with_analysis(img, gt, pred_boxes, pred_scores, iou_threshold=0.5)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    print(f\"  Precision: {precision:.3f} | Recall: {recall:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 9: Export & Deployment (15 minutes)\n",
    "\n",
    "For operational use, we need to export models for deployment.\n",
    "\n",
    "## Save Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('/content/detection_outputs', exist_ok=True)\n",
    "\n",
    "def export_detections_to_json(images, boxes_list, scores_list, output_file):\n",
    "    \"\"\"\n",
    "    Export detections to JSON format (COCO-like)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img_id, (image, boxes, scores) in enumerate(zip(images, boxes_list, scores_list)):\n",
    "        img_h, img_w = 320, 320\n",
    "        \n",
    "        for box, score in zip(boxes, scores):\n",
    "            y1, x1, y2, x2 = box\n",
    "            \n",
    "            # Convert to COCO format [x, y, width, height] in pixels\n",
    "            x_px = x1 * img_w\n",
    "            y_px = y1 * img_h\n",
    "            w_px = (x2 - x1) * img_w\n",
    "            h_px = (y2 - y1) * img_h\n",
    "            \n",
    "            results.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': 1,  # Building class\n",
    "                'bbox': [float(x_px), float(y_px), float(w_px), float(h_px)],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Exported {len(results)} detections to {output_file}\")\n",
    "\n",
    "# Run detection on test set and export\n",
    "test_pred_boxes_list = []\n",
    "test_pred_scores_list = []\n",
    "\n",
    "for image in test_images:\n",
    "    detections = run_detection(detector, image)\n",
    "    pred_boxes = detections['detection_boxes']\n",
    "    pred_scores = detections['detection_scores']\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep_indices = non_max_suppression(pred_boxes, pred_scores, iou_threshold=0.5, score_threshold=0.3)\n",
    "    pred_boxes = pred_boxes[keep_indices]\n",
    "    pred_scores = pred_scores[keep_indices]\n",
    "    \n",
    "    test_pred_boxes_list.append(pred_boxes)\n",
    "    test_pred_scores_list.append(pred_scores)\n",
    "\n",
    "# Export to JSON\n",
    "export_detections_to_json(\n",
    "    test_images, \n",
    "    test_pred_boxes_list, \n",
    "    test_pred_scores_list,\n",
    "    '/content/detection_outputs/test_predictions.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "### Export to GeoJSON (for GIS Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_geojson(boxes_list, scores_list, output_file, origin_lon=121.0, origin_lat=14.6, pixel_size=10):\n",
    "    \"\"\"\n",
    "    Export detections to GeoJSON format for GIS integration\n",
    "    \n",
    "    Args:\n",
    "        boxes_list: List of detection boxes\n",
    "        scores_list: List of confidence scores\n",
    "        output_file: Output GeoJSON file path\n",
    "        origin_lon: Longitude of image origin (upper-left)\n",
    "        origin_lat: Latitude of image origin (upper-left)\n",
    "        pixel_size: Pixel size in meters (default 10m for Sentinel-2)\n",
    "    \"\"\"\n",
    "    # Approximate degrees per meter at latitude\n",
    "    meters_per_degree_lat = 111320.0\n",
    "    meters_per_degree_lon = 111320.0 * np.cos(np.radians(origin_lat))\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for img_id, (boxes, scores) in enumerate(zip(boxes_list, scores_list)):\n",
    "        for box, score in zip(boxes, scores):\n",
    "            y1, x1, y2, x2 = box\n",
    "            \n",
    "            # Convert pixel coords to geographic coords\n",
    "            img_h, img_w = 320, 320\n",
    "            \n",
    "            # Top-left corner in pixels\n",
    "            x1_px = x1 * img_w\n",
    "            y1_px = y1 * img_h\n",
    "            x2_px = x2 * img_w\n",
    "            y2_px = y2 * img_h\n",
    "            \n",
    "            # Convert to meters\n",
    "            x1_m = x1_px * pixel_size\n",
    "            y1_m = y1_px * pixel_size\n",
    "            x2_m = x2_px * pixel_size\n",
    "            y2_m = y2_px * pixel_size\n",
    "            \n",
    "            # Convert to geographic coordinates\n",
    "            lon1 = origin_lon + (x1_m / meters_per_degree_lon)\n",
    "            lat1 = origin_lat - (y1_m / meters_per_degree_lat)\n",
    "            lon2 = origin_lon + (x2_m / meters_per_degree_lon)\n",
    "            lat2 = origin_lat - (y2_m / meters_per_degree_lat)\n",
    "            \n",
    "            # Create polygon (bounding box)\n",
    "            coordinates = [[\n",
    "                [lon1, lat1],\n",
    "                [lon2, lat1],\n",
    "                [lon2, lat2],\n",
    "                [lon1, lat2],\n",
    "                [lon1, lat1]\n",
    "            ]]\n",
    "            \n",
    "            feature = {\n",
    "                'type': 'Feature',\n",
    "                'geometry': {\n",
    "                    'type': 'Polygon',\n",
    "                    'coordinates': coordinates\n",
    "                },\n",
    "                'properties': {\n",
    "                    'image_id': img_id,\n",
    "                    'class': 'building',\n",
    "                    'confidence': float(score),\n",
    "                    'area_m2': (x2_m - x1_m) * (y2_m - y1_m)\n",
    "                }\n",
    "            }\n",
    "            features.append(feature)\n",
    "    \n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'crs': {\n",
    "            'type': 'name',\n",
    "            'properties': {'name': 'EPSG:4326'}\n",
    "        },\n",
    "        'features': features\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Exported {len(features)} building polygons to {output_file}\")\n",
    "    print(f\"  Ready for QGIS/ArcGIS import\")\n",
    "\n",
    "# Export to GeoJSON\n",
    "export_to_geojson(\n",
    "    test_pred_boxes_list,\n",
    "    test_pred_scores_list,\n",
    "    '/content/detection_outputs/buildings.geojson',\n",
    "    origin_lon=121.0,  # Metro Manila approximate\n",
    "    origin_lat=14.6,\n",
    "    pixel_size=10  # Sentinel-2 resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "### Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = {\n",
    "    'model': 'SSD MobileNet V2',\n",
    "    'dataset': {\n",
    "        'train_images': len(train_images),\n",
    "        'val_images': len(val_images),\n",
    "        'test_images': len(test_images),\n",
    "        'total_buildings': sum(len(b) for b in test_boxes)\n",
    "    },\n",
    "    'performance': {\n",
    "        'mAP@0.5': float(map_results.get(0.5, 0)),\n",
    "        'mAP@0.75': float(map_results.get(0.75, 0)),\n",
    "        'inference_time_ms': float(ssd_time * 1000),\n",
    "        'fps': float(ssd_fps)\n",
    "    },\n",
    "    'detections': {\n",
    "        'total_predicted': sum(len(b) for b in test_pred_boxes_list),\n",
    "        'mean_per_image': float(np.mean([len(b) for b in test_pred_boxes_list])),\n",
    "        'mean_confidence': float(np.mean([s.mean() for s in test_pred_scores_list if len(s) > 0]))\n",
    "    },\n",
    "    'nms_config': {\n",
    "        'iou_threshold': 0.5,\n",
    "        'score_threshold': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/content/detection_outputs/summary_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nüìä Summary Report\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {report['model']}\")\n",
    "print(f\"\\nDataset:\")\n",
    "for key, val in report['dataset'].items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "for key, val in report['performance'].items():\n",
    "    print(f\"  {key}: {val:.3f}\")\n",
    "print(f\"\\nDetections:\")\n",
    "for key, val in report['detections'].items():\n",
    "    print(f\"  {key}: {val:.2f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n‚úì All outputs saved to /content/detection_outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 10: Troubleshooting & Best Practices (10 minutes)\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "::: {.callout-warning}\n",
    "## Issue 1: Low mAP / Poor Detection Performance\n",
    "\n",
    "**Symptoms:**\n",
    "- mAP < 0.40\n",
    "- Many false positives or false negatives\n",
    "- Model detects wrong objects\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Fine-tune on domain-specific data:**\n",
    "   ```python\n",
    "   # Pre-trained model is trained on COCO (general objects)\n",
    "   # Need to fine-tune on Philippine building dataset\n",
    "   # Use TensorFlow Object Detection API for fine-tuning\n",
    "   ```\n",
    "\n",
    "2. **Adjust NMS thresholds:**\n",
    "   ```python\n",
    "   # Try different IoU thresholds\n",
    "   keep_indices = non_max_suppression(\n",
    "       boxes, scores,\n",
    "       iou_threshold=0.3,  # Lower = keep more boxes\n",
    "       score_threshold=0.5  # Higher = more confident only\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Collect more training data:**\n",
    "   - Minimum 500-1000 annotated images\n",
    "   - Diverse building sizes and types\n",
    "   - Various lighting and seasons\n",
    ":::\n",
    "\n",
    "::: {.callout-warning}\n",
    "## Issue 2: Slow Inference Speed\n",
    "\n",
    "**Symptoms:**\n",
    "- FPS < 5\n",
    "- Long processing time for large areas\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Use lighter model:**\n",
    "   ```python\n",
    "   # Switch to MobileNet (current) or SSD Lite\n",
    "   model_url = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1\"\n",
    "   ```\n",
    "\n",
    "2. **Optimize model:**\n",
    "   ```python\n",
    "   # Export to TFLite for mobile/edge deployment\n",
    "   converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "   converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "   tflite_model = converter.convert()\n",
    "   ```\n",
    "\n",
    "3. **Batch processing:**\n",
    "   ```python\n",
    "   # Process multiple images at once\n",
    "   batch_images = tf.stack([img1, img2, img3, img4])\n",
    "   detections = model(batch_images)\n",
    "   ```\n",
    "\n",
    "4. **Use GPU:**\n",
    "   - Ensure GPU is available in Colab (Runtime ‚Üí Change runtime type)\n",
    "   - Check with `tf.config.list_physical_devices('GPU')`\n",
    ":::\n",
    "\n",
    "::: {.callout-warning}\n",
    "## Issue 3: Too Many False Positives\n",
    "\n",
    "**Symptoms:**\n",
    "- Model detects buildings where there are none\n",
    "- Low precision (<0.5)\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Increase confidence threshold:**\n",
    "   ```python\n",
    "   keep_indices = non_max_suppression(\n",
    "       boxes, scores,\n",
    "       score_threshold=0.6  # Increase from 0.3\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Add negative examples:**\n",
    "   - Include images with no buildings in training\n",
    "   - Annotate \"hard negatives\" (vegetation, rocks that look like buildings)\n",
    "\n",
    "3. **Use contextual filtering:**\n",
    "   ```python\n",
    "   # Remove detections outside urban areas\n",
    "   # Use land cover mask to filter unlikely locations\n",
    "   ```\n",
    ":::\n",
    "\n",
    "::: {.callout-warning}\n",
    "## Issue 4: Missing Small Buildings\n",
    "\n",
    "**Symptoms:**\n",
    "- Small buildings not detected\n",
    "- Low recall for small objects\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Use higher resolution input:**\n",
    "   ```python\n",
    "   # Use 512x512 or 640x640 instead of 320x320\n",
    "   # Note: Slower inference\n",
    "   ```\n",
    "\n",
    "2. **Multi-scale detection:**\n",
    "   ```python\n",
    "   # Run detection at multiple scales\n",
    "   scales = [0.75, 1.0, 1.25]\n",
    "   all_detections = []\n",
    "   for scale in scales:\n",
    "       resized = tf.image.resize(image, [int(320*scale), int(320*scale)])\n",
    "       detections = model(resized)\n",
    "       all_detections.append(detections)\n",
    "   # Combine and apply NMS\n",
    "   ```\n",
    "\n",
    "3. **Use architecture with FPN:**\n",
    "   - Feature Pyramid Networks handle multi-scale better\n",
    "   - EfficientDet has built-in BiFPN\n",
    ":::\n",
    "\n",
    "## Best Practices for Production\n",
    "\n",
    "### 1. Data Preparation\n",
    "‚úÖ Use high-quality annotations (double-check bounding boxes)  \n",
    "‚úÖ Balance dataset across building sizes  \n",
    "‚úÖ Include seasonal variations (wet/dry season)  \n",
    "‚úÖ Annotate at least 1000 images for good performance\n",
    "\n",
    "### 2. Model Selection\n",
    "‚úÖ **Fast inference needed:** SSD MobileNet  \n",
    "‚úÖ **Balanced:** EfficientDet D0-D2  \n",
    "‚úÖ **High accuracy:** Faster R-CNN ResNet\n",
    "\n",
    "### 3. Hyperparameter Tuning\n",
    "‚úÖ **NMS IoU threshold:** Start 0.5, tune 0.3-0.7  \n",
    "‚úÖ **Score threshold:** Start 0.3, tune 0.2-0.6  \n",
    "‚úÖ **Test on validation set first**\n",
    "\n",
    "### 4. Evaluation\n",
    "‚úÖ Report mAP@0.5 and mAP@0.75  \n",
    "‚úÖ Calculate per-class metrics  \n",
    "‚úÖ Analyze false positives and false negatives  \n",
    "‚úÖ Test on diverse geographic regions\n",
    "\n",
    "### 5. Deployment\n",
    "‚úÖ Export to TFLite or ONNX for production  \n",
    "‚úÖ Monitor inference time and memory usage  \n",
    "‚úÖ Implement batch processing for large areas  \n",
    "‚úÖ Add geographic post-processing (filter by land cover)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "# üéâ Lab Complete!\n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "### Technical Skills:\n",
    "‚úÖ **Loaded** pre-trained object detection models from TensorFlow Hub  \n",
    "‚úÖ **Generated** synthetic urban satellite imagery with annotations  \n",
    "‚úÖ **Implemented** Non-Maximum Suppression (NMS) from scratch  \n",
    "‚úÖ **Calculated** mAP (mean Average Precision) for model evaluation  \n",
    "‚úÖ **Compared** detection architectures (speed vs accuracy)  \n",
    "‚úÖ **Visualized** detections with comprehensive error analysis  \n",
    "‚úÖ **Exported** results in multiple formats (JSON, GeoJSON)  \n",
    "‚úÖ **Applied** transfer learning for Philippine building detection\n",
    "\n",
    "### Conceptual Understanding:\n",
    "‚úÖ Transfer learning for object detection  \n",
    "‚úÖ Bounding box formats (COCO, Pascal VOC, YOLO, TF Hub)  \n",
    "‚úÖ NMS algorithm and its importance  \n",
    "‚úÖ mAP metric and precision-recall curves  \n",
    "‚úÖ IoU calculation and thresholds  \n",
    "‚úÖ Model architecture trade-offs (SSD vs Faster R-CNN)  \n",
    "‚úÖ Error types (TP, FP, FN) and their operational implications\n",
    "\n",
    "### Philippine Urban Monitoring Context:\n",
    "‚úÖ Building detection for disaster risk reduction  \n",
    "‚úÖ Informal settlement mapping for urban planning  \n",
    "‚úÖ Change detection for infrastructure development  \n",
    "‚úÖ GIS integration for operational use\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Transfer Learning Works\n",
    "Pre-trained models (COCO dataset) provide good starting point for building detection, even without fine-tuning.\n",
    "\n",
    "### 2. NMS is Essential\n",
    "Object detectors output many overlapping boxes. NMS filters duplicates to give clean results.\n",
    "\n",
    "### 3. mAP Tells the Full Story\n",
    "mAP combines localization accuracy and classification confidence into single metric. Always report mAP@0.5 and mAP@0.75.\n",
    "\n",
    "### 4. Threshold Tuning Matters\n",
    "NMS IoU threshold and confidence threshold significantly affect results. Tune on validation set.\n",
    "\n",
    "### 5. Speed vs Accuracy Trade-off\n",
    "- **Real-time:** SSD MobileNet\n",
    "- **Balanced:** EfficientDet\n",
    "- **Accuracy-critical:** Faster R-CNN\n",
    "\n",
    "### 6. Domain-Specific Fine-tuning Improves Performance\n",
    "For production use, fine-tune on Philippine building dataset using TensorFlow Object Detection API.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "### 1. Real Data Collection\n",
    "- Acquire Sentinel-2 imagery for Metro Manila\n",
    "- Download from [Copernicus Open Access Hub](https://scihub.copernicus.eu/)\n",
    "- Or use Google Earth Engine\n",
    "\n",
    "### 2. Annotation\n",
    "- Use [RoboFlow](https://roboflow.com/) or [CVAT](https://cvat.org/) for bounding box annotation\n",
    "- Annotate 500-1000 images minimum\n",
    "- Include diverse building types and sizes\n",
    "\n",
    "### 3. Fine-tuning\n",
    "- Use [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
    "- Fine-tune SSD or EfficientDet on Philippine data\n",
    "- Train for 20K-50K steps\n",
    "\n",
    "### 4. Deployment\n",
    "- Export to TFLite for edge deployment\n",
    "- Create batch processing pipeline\n",
    "- Integrate with GIS workflows\n",
    "- Deploy to PhilSA/DOST operational systems\n",
    "\n",
    "### 5. Monitoring\n",
    "- Track inference time and accuracy\n",
    "- Collect edge cases for retraining\n",
    "- Update model quarterly with new data\n",
    "\n",
    "---\n",
    "\n",
    "## Resources for Further Learning\n",
    "\n",
    "### Papers\n",
    "- [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "- [EfficientDet](https://arxiv.org/abs/1911.09070)\n",
    "\n",
    "### Tutorials\n",
    "- [TensorFlow Object Detection API Tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/)\n",
    "- [RoboFlow Object Detection Guide](https://blog.roboflow.com/object-detection/)\n",
    "\n",
    "### Datasets\n",
    "- [xView Dataset](http://xviewdataset.org/) - Satellite object detection\n",
    "- [DOTA Dataset](https://captain-whu.github.io/DOTA/) - Aerial image object detection\n",
    "- [SpaceNet](https://spacenet.ai/) - Building footprint detection\n",
    "\n",
    "### Tools\n",
    "- [TensorFlow Hub](https://tfhub.dev/) - Pre-trained models\n",
    "- [RoboFlow](https://roboflow.com/) - Dataset management and annotation\n",
    "- [Weights & Biases](https://wandb.ai/) - Experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. **Application Design:**\n",
    "   - How would you deploy this building detection system for Metro Manila monitoring?\n",
    "   - What infrastructure and data pipelines are needed?\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - When would you choose SSD over Faster R-CNN for Philippine use cases?\n",
    "   - How do you balance speed vs accuracy requirements?\n",
    "\n",
    "3. **Evaluation:**\n",
    "   - Is mAP@0.5 > 0.70 sufficient for disaster risk reduction applications?\n",
    "   - Should we prioritize precision or recall for informal settlement mapping?\n",
    "\n",
    "4. **Data Quality:**\n",
    "   - How many training images are needed for operational accuracy?\n",
    "   - How do you handle seasonal variations in Philippine imagery?\n",
    "\n",
    "5. **Operational Challenges:**\n",
    "   - What happens if the model misses a building in a flood zone (false negative)?\n",
    "   - How do you validate predictions in remote areas with no ground truth?\n",
    "\n",
    "6. **Integration:**\n",
    "   - How would you integrate detections with existing NAMRIA/PhilSA databases?\n",
    "   - What quality control steps are needed before operational use?\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results Summary\n",
    "\n",
    "| Metric | Expected Range | Interpretation |\n",
    "|--------|----------------|----------------|\n",
    "| **mAP@0.5** | 0.40 - 0.70 | Moderate (pre-trained, no fine-tuning) |\n",
    "| **mAP@0.75** | 0.25 - 0.50 | Good localization given constraints |\n",
    "| **Inference Time** | 50-150 ms | Fast enough for batch processing |\n",
    "| **FPS** | 5-20 | Suitable for operational use |\n",
    "| **Precision** | 0.50 - 0.80 | Few false alarms |\n",
    "| **Recall** | 0.60 - 0.85 | Catches most buildings |\n",
    "\n",
    "::: {.callout-note}\n",
    "## With Fine-tuning on Philippine Data\n",
    "\n",
    "**Expected improvements:**\n",
    "- mAP@0.5: **0.70 - 0.90** (+30-40%)\n",
    "- mAP@0.75: **0.50 - 0.75** (+100%)\n",
    "- Precision: **0.75 - 0.92**\n",
    "- Recall: **0.80 - 0.95**\n",
    "\n",
    "Fine-tuning on domain-specific data typically doubles performance!\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Completion Checklist\n",
    "\n",
    "Before finishing, ensure you've completed:\n",
    "\n",
    "- [ ] Generated synthetic urban imagery dataset\n",
    "- [ ] Loaded pre-trained SSD MobileNet V2 model\n",
    "- [ ] Implemented and tested IoU calculation\n",
    "- [ ] Implemented Non-Maximum Suppression\n",
    "- [ ] Calculated mAP@0.5 and mAP@0.75\n",
    "- [ ] Visualized precision-recall curves\n",
    "- [ ] Analyzed detection errors (TP, FP, FN)\n",
    "- [ ] Exported results to JSON and GeoJSON\n",
    "- [ ] Generated summary report\n",
    "- [ ] Understood troubleshooting strategies\n",
    "\n",
    "::: {.callout-success}\n",
    "## Congratulations! üéä\n",
    "\n",
    "You've completed a full object detection pipeline for satellite imagery analysis!\n",
    "\n",
    "**What You Built:**\n",
    "- A working building detection system\n",
    "- Complete evaluation framework (mAP, precision-recall)\n",
    "- Production-ready export workflows\n",
    "- GIS-compatible outputs\n",
    "\n",
    "**Impact:**\n",
    "Your skills can now contribute to urban monitoring, disaster risk reduction, and infrastructure planning for Philippine cities.\n",
    "\n",
    "**Next:** Apply these techniques to Day 4 advanced topics (change detection, time series analysis)\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "::: {.session-nav}\n",
    "[‚Üê Back to Session 3](../sessions/session3.qmd){.btn .btn-outline-secondary}\n",
    "[Course Summary ‚Üí](../../index.qmd){.btn .btn-primary}\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "*This hands-on lab is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative. Materials developed in collaboration with PhilSA, DOST-ASTI, and the European Space Agency.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
