<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-16">

<title>Session 2: Core Concepts of AI/ML for Earth Observation – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day1/sessions/session3.html" rel="next">
<link href="../../day1/sessions/session1.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-4a074efccdeff27617fbc72d37c1244e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a15f5dce5650fb3fe5aba34e3b6df9a9.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cophil-training-v1.0"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day1/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day1/sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1: EO Data &amp; AI/ML Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 2: Core Concepts of AI/ML for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Hands-on Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Introduction to Google Earth Engine</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/notebook1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notebook 1: Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/notebook2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notebook 2: Google Earth Engine</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/Day1_Session3_Python_Geospatial_Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1, Session 3: Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/Day1_Session4_Google_Earth_Engine.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1, Session 4: Google Earth Engine Python API</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link active" data-scroll-target="#session-overview">Session Overview</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  </ul></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#part-1-what-is-aiml" id="toc-part-1-what-is-aiml" class="nav-link" data-scroll-target="#part-1-what-is-aiml">Part 1: What is AI/ML?</a>
  <ul class="collapse">
  <li><a href="#defining-the-terms" id="toc-defining-the-terms" class="nav-link" data-scroll-target="#defining-the-terms">Defining the Terms</a></li>
  <li><a href="#why-ml-for-earth-observation" id="toc-why-ml-for-earth-observation" class="nav-link" data-scroll-target="#why-ml-for-earth-observation">Why ML for Earth Observation?</a></li>
  </ul></li>
  <li><a href="#part-2-the-aiml-workflow-for-earth-observation" id="toc-part-2-the-aiml-workflow-for-earth-observation" class="nav-link" data-scroll-target="#part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#step-1-problem-definition" id="toc-step-1-problem-definition" class="nav-link" data-scroll-target="#step-1-problem-definition">Step 1: Problem Definition</a></li>
  <li><a href="#step-2-data-acquisition" id="toc-step-2-data-acquisition" class="nav-link" data-scroll-target="#step-2-data-acquisition">Step 2: Data Acquisition</a></li>
  <li><a href="#step-3-data-pre-processing" id="toc-step-3-data-pre-processing" class="nav-link" data-scroll-target="#step-3-data-pre-processing">Step 3: Data Pre-processing</a></li>
  <li><a href="#step-4-feature-engineering" id="toc-step-4-feature-engineering" class="nav-link" data-scroll-target="#step-4-feature-engineering">Step 4: Feature Engineering</a></li>
  <li><a href="#step-5-model-selection-and-training" id="toc-step-5-model-selection-and-training" class="nav-link" data-scroll-target="#step-5-model-selection-and-training">Step 5: Model Selection and Training</a></li>
  <li><a href="#step-6-validation-and-evaluation" id="toc-step-6-validation-and-evaluation" class="nav-link" data-scroll-target="#step-6-validation-and-evaluation">Step 6: Validation and Evaluation</a></li>
  <li><a href="#step-7-deployment-and-operationalization" id="toc-step-7-deployment-and-operationalization" class="nav-link" data-scroll-target="#step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</a></li>
  </ul></li>
  <li><a href="#part-3-types-of-machine-learning" id="toc-part-3-types-of-machine-learning" class="nav-link" data-scroll-target="#part-3-types-of-machine-learning">Part 3: Types of Machine Learning</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised Learning</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning">Unsupervised Learning</a></li>
  </ul></li>
  <li><a href="#part-4-deep-learning-architectures-for-earth-observation" id="toc-part-4-deep-learning-architectures-for-earth-observation" class="nav-link" data-scroll-target="#part-4-deep-learning-architectures-for-earth-observation">Part 4: Deep Learning Architectures for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#what-is-deep-learning" id="toc-what-is-deep-learning" class="nav-link" data-scroll-target="#what-is-deep-learning">What is Deep Learning?</a></li>
  <li><a href="#neural-network-fundamentals" id="toc-neural-network-fundamentals" class="nav-link" data-scroll-target="#neural-network-fundamentals">Neural Network Fundamentals</a></li>
  <li><a href="#convolutional-neural-networks-cnns" id="toc-convolutional-neural-networks-cnns" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a></li>
  <li><a href="#u-net-and-semantic-segmentation" id="toc-u-net-and-semantic-segmentation" class="nav-link" data-scroll-target="#u-net-and-semantic-segmentation">U-Net and Semantic Segmentation</a></li>
  <li><a href="#deeplab-family" id="toc-deeplab-family" class="nav-link" data-scroll-target="#deeplab-family">DeepLab Family</a></li>
  <li><a href="#vision-transformers-vits" id="toc-vision-transformers-vits" class="nav-link" data-scroll-target="#vision-transformers-vits">Vision Transformers (ViTs)</a></li>
  <li><a href="#hybrid-architectures-unetformer" id="toc-hybrid-architectures-unetformer" class="nav-link" data-scroll-target="#hybrid-architectures-unetformer">Hybrid Architectures: UNetFormer</a></li>
  <li><a href="#temporal-models-for-time-series" id="toc-temporal-models-for-time-series" class="nav-link" data-scroll-target="#temporal-models-for-time-series">Temporal Models for Time Series</a></li>
  <li><a href="#object-detection-architectures" id="toc-object-detection-architectures" class="nav-link" data-scroll-target="#object-detection-architectures">Object Detection Architectures</a></li>
  <li><a href="#multi-modal-architectures" id="toc-multi-modal-architectures" class="nav-link" data-scroll-target="#multi-modal-architectures">Multi-Modal Architectures</a></li>
  <li><a href="#foundation-models-for-earth-observation" id="toc-foundation-models-for-earth-observation" class="nav-link" data-scroll-target="#foundation-models-for-earth-observation">Foundation Models for Earth Observation</a></li>
  <li><a href="#training-strategies" id="toc-training-strategies" class="nav-link" data-scroll-target="#training-strategies">Training Strategies</a></li>
  </ul></li>
  <li><a href="#part-5-benchmark-datasets-for-training-and-validation" id="toc-part-5-benchmark-datasets-for-training-and-validation" class="nav-link" data-scroll-target="#part-5-benchmark-datasets-for-training-and-validation">Part 5: Benchmark Datasets for Training and Validation</a>
  <ul class="collapse">
  <li><a href="#patch-level-classification-datasets" id="toc-patch-level-classification-datasets" class="nav-link" data-scroll-target="#patch-level-classification-datasets">Patch-Level Classification Datasets</a></li>
  <li><a href="#object-detection-datasets" id="toc-object-detection-datasets" class="nav-link" data-scroll-target="#object-detection-datasets">Object Detection Datasets</a></li>
  <li><a href="#semantic-segmentation-datasets" id="toc-semantic-segmentation-datasets" class="nav-link" data-scroll-target="#semantic-segmentation-datasets">Semantic Segmentation Datasets</a></li>
  <li><a href="#scene-classification-datasets" id="toc-scene-classification-datasets" class="nav-link" data-scroll-target="#scene-classification-datasets">Scene Classification Datasets</a></li>
  <li><a href="#time-series-datasets" id="toc-time-series-datasets" class="nav-link" data-scroll-target="#time-series-datasets">Time Series Datasets</a></li>
  <li><a href="#philippine-specific-data-resources" id="toc-philippine-specific-data-resources" class="nav-link" data-scroll-target="#philippine-specific-data-resources">Philippine-Specific Data Resources</a></li>
  </ul></li>
  <li><a href="#part-6-data-centric-ai-in-earth-observation" id="toc-part-6-data-centric-ai-in-earth-observation" class="nav-link" data-scroll-target="#part-6-data-centric-ai-in-earth-observation">Part 6: Data-Centric AI in Earth Observation</a>
  <ul class="collapse">
  <li><a href="#the-paradigm-shift-2025" id="toc-the-paradigm-shift-2025" class="nav-link" data-scroll-target="#the-paradigm-shift-2025">The Paradigm Shift (2025)</a></li>
  <li><a href="#pillar-1-data-quality" id="toc-pillar-1-data-quality" class="nav-link" data-scroll-target="#pillar-1-data-quality">Pillar 1: Data Quality</a></li>
  <li><a href="#pillar-2-data-quantity" id="toc-pillar-2-data-quantity" class="nav-link" data-scroll-target="#pillar-2-data-quantity">Pillar 2: Data Quantity</a></li>
  <li><a href="#pillar-3-data-diversity" id="toc-pillar-3-data-diversity" class="nav-link" data-scroll-target="#pillar-3-data-diversity">Pillar 3: Data Diversity</a></li>
  <li><a href="#pillar-4-annotation-strategy" id="toc-pillar-4-annotation-strategy" class="nav-link" data-scroll-target="#pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</a></li>
  <li><a href="#examples-data-centric-success-stories" id="toc-examples-data-centric-success-stories" class="nav-link" data-scroll-target="#examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</a></li>
  </ul></li>
  <li><a href="#part-7-explainable-ai-xai-for-earth-observation" id="toc-part-7-explainable-ai-xai-for-earth-observation" class="nav-link" data-scroll-target="#part-7-explainable-ai-xai-for-earth-observation">Part 7: Explainable AI (XAI) for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#why-xai-matters-in-eo" id="toc-why-xai-matters-in-eo" class="nav-link" data-scroll-target="#why-xai-matters-in-eo">Why XAI Matters in EO</a></li>
  <li><a href="#xai-methods-for-eo" id="toc-xai-methods-for-eo" class="nav-link" data-scroll-target="#xai-methods-for-eo">XAI Methods for EO</a></li>
  <li><a href="#applications-in-eo" id="toc-applications-in-eo" class="nav-link" data-scroll-target="#applications-in-eo">Applications in EO</a></li>
  <li><a href="#challenges-and-trade-offs" id="toc-challenges-and-trade-offs" class="nav-link" data-scroll-target="#challenges-and-trade-offs">Challenges and Trade-Offs</a></li>
  <li><a href="#best-practices-for-xai-in-eo" id="toc-best-practices-for-xai-in-eo" class="nav-link" data-scroll-target="#best-practices-for-xai-in-eo">Best Practices for XAI in EO</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions">Discussion Questions</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a>
  <ul class="collapse">
  <li><a href="#foundational-concepts" id="toc-foundational-concepts" class="nav-link" data-scroll-target="#foundational-concepts">Foundational Concepts</a></li>
  <li><a href="#deep-learning-architectures-1" id="toc-deep-learning-architectures-1" class="nav-link" data-scroll-target="#deep-learning-architectures-1">Deep Learning Architectures</a></li>
  <li><a href="#deep-learning-for-eo" id="toc-deep-learning-for-eo" class="nav-link" data-scroll-target="#deep-learning-for-eo">Deep Learning for EO</a></li>
  <li><a href="#foundation-models" id="toc-foundation-models" class="nav-link" data-scroll-target="#foundation-models">Foundation Models</a></li>
  <li><a href="#self-supervised-learning-1" id="toc-self-supervised-learning-1" class="nav-link" data-scroll-target="#self-supervised-learning-1">Self-Supervised Learning</a></li>
  <li><a href="#data-centric-ai" id="toc-data-centric-ai" class="nav-link" data-scroll-target="#data-centric-ai">Data-Centric AI</a></li>
  <li><a href="#explainable-ai-1" id="toc-explainable-ai-1" class="nav-link" data-scroll-target="#explainable-ai-1">Explainable AI</a></li>
  <li><a href="#eo-specific-ml" id="toc-eo-specific-ml" class="nav-link" data-scroll-target="#eo-specific-ml">EO-Specific ML</a></li>
  <li><a href="#benchmark-datasets-1" id="toc-benchmark-datasets-1" class="nav-link" data-scroll-target="#benchmark-datasets-1">Benchmark Datasets</a></li>
  <li><a href="#philippine-ai-initiatives" id="toc-philippine-ai-initiatives" class="nav-link" data-scroll-target="#philippine-ai-initiatives">Philippine AI Initiatives</a></li>
  <li><a href="#recent-advances" id="toc-recent-advances" class="nav-link" data-scroll-target="#recent-advances">Recent Advances</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day1/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day1/sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Session 2: Core Concepts of AI/ML for Earth Observation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Understanding the fundamentals of machine learning for satellite data analysis</p>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 1</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 2</span>
</nav>
<div class="session-info">
<p><strong>Duration:</strong> 2 hours | <strong>Format:</strong> Lecture + Conceptual Exercises | <strong>Platform:</strong> Presentation</p>
</div>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p>This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You’ll learn the complete AI/ML workflow, understand different learning paradigms, explore deep learning architectures including CNNs, Vision Transformers, and temporal models, discover benchmark datasets, and understand why data quality matters more than model complexity in 2025’s data-centric AI paradigm.</p>
<hr>
<p>The session integrates cutting-edge methodologies with concrete Philippine case studies, from DOST-ASTI’s 10-20 minute flood detection using AI to PhilSA’s mangrove mapping efforts, demonstrating operational AI/ML systems already serving disaster risk reduction and natural resource management needs.</p>
<section id="learning-objectives" class="level3 learning-objectives">
<h3 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h3>
<p>By the end of this session, you will be able to:</p>
<ul>
<li><strong>Define</strong> AI and ML in the context of Earth Observation</li>
<li><strong>Describe</strong> the complete AI/ML workflow from problem definition to deployment</li>
<li><strong>Distinguish</strong> between supervised and unsupervised learning with EO examples</li>
<li><strong>Explain</strong> classification vs.&nbsp;regression tasks in satellite data analysis</li>
<li><strong>Identify</strong> major deep learning architectures: CNNs, U-Net, Vision Transformers, RNNs/LSTMs, object detection networks</li>
<li><strong>Understand</strong> key components: neurons, layers, activation functions, loss functions, optimizers</li>
<li><strong>Compare</strong> different model architectures and when to apply each</li>
<li><strong>Recognize</strong> benchmark datasets used for training and evaluation</li>
<li><strong>Articulate</strong> the data-centric AI paradigm and its importance for EO</li>
<li><strong>Apply</strong> best practices for data quality, quantity, diversity, and annotation</li>
<li><strong>Explain</strong> Explainable AI (XAI) and why it matters for operational systems</li>
</ul>
</section>
<hr>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/02_session2_ai_ml_fundamentals.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
</section>
<section id="part-1-what-is-aiml" class="level2">
<h2 class="anchored" data-anchor-id="part-1-what-is-aiml">Part 1: What is AI/ML?</h2>
<section id="defining-the-terms" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-terms">Defining the Terms</h3>
<p><strong>Artificial Intelligence (AI):</strong></p>
<ul>
<li>Broad field focused on creating intelligent machines</li>
<li>Systems that can perceive, reason, learn, and act</li>
<li>Includes everything from rule-based systems to machine learning</li>
<li>In EO: Enables automated interpretation of petabytes of satellite data</li>
</ul>
<p><strong>Machine Learning (ML):</strong></p>
<ul>
<li>Subset of AI focused on learning from data</li>
<li>Algorithms that improve performance through experience</li>
<li><strong>Key distinction:</strong> No explicit programming of rules</li>
<li><strong>Deep Learning:</strong> Subset of ML using multi-layered neural networks</li>
</ul>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph AI["ARTIFICIAL INTELLIGENCE&lt;br/&gt;Creating intelligent machines"]
        subgraph ML["MACHINE LEARNING&lt;br/&gt;Learning from data without explicit programming"]
            subgraph DL["DEEP LEARNING&lt;br/&gt;Multi-layered neural networks"]
                DL1[Convolutional&lt;br/&gt;Neural Networks&lt;br/&gt;CNNs]
                DL2[Recurrent&lt;br/&gt;Neural Networks&lt;br/&gt;RNNs/LSTMs]
                DL3[Transformers&lt;br/&gt;Vision Transformers]
                DL4[GANs &amp; VAEs&lt;br/&gt;Generative Models]
            end
            ML1[Random Forest]
            ML2[Support Vector&lt;br/&gt;Machines]
            ML3[Decision Trees]
            ML4[K-Means&lt;br/&gt;Clustering]
        end
        AI1[Expert Systems&lt;br/&gt;Rule-based]
        AI2[Fuzzy Logic]
        AI3[Genetic&lt;br/&gt;Algorithms]
    end

    DL1 -.-&gt;|EO Apps| EO1[Land Cover&lt;br/&gt;Classification]
    DL1 -.-&gt;|EO Apps| EO2[Semantic&lt;br/&gt;Segmentation]
    DL2 -.-&gt;|EO Apps| EO3[Time Series&lt;br/&gt;Crop Monitoring]
    DL3 -.-&gt;|EO Apps| EO4[Change&lt;br/&gt;Detection]

    style AI fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style ML fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style DL fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style DL1 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
    style DL2 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
    style DL3 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
</pre>
</div>
<p></p><figcaption> AI, Machine Learning, and Deep Learning Relationship</figcaption> </figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The ML Difference
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Traditional Programming:</strong></p>
<pre><code>Rules + Data → Output</code></pre>
<p><strong>Machine Learning:</strong></p>
<pre><code>Data + Desired Output → Rules (Model)</code></pre>
<p>In EO: Instead of coding “if NIR &gt; 0.6 and Red &lt; 0.3, then forest”, ML learns the pattern from labeled examples.</p>
</div>
</div>
</section>
<section id="why-ml-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="why-ml-for-earth-observation">Why ML for Earth Observation?</h3>
<p><strong>Challenges that ML addresses:</strong></p>
<ol type="1">
<li><strong>Scale:</strong> NASA’s Earth Science Data Systems exceeded 148 PB in 2023, projected 250 PB in 2025 - impossible to manually analyze</li>
<li><strong>Complexity:</strong> Multispectral, multi-temporal, spatial patterns humans can’t easily detect</li>
<li><strong>Consistency:</strong> Automated processing ensures reproducible results across time and space</li>
<li><strong>Speed:</strong> Real-time disaster mapping requires immediate analysis (DOST-ASTI DATOS: 10-20 minute flood response)</li>
<li><strong>Multi-modal fusion:</strong> Integrating optical, SAR, LiDAR data for robust monitoring</li>
</ol>
<p><strong>Traditional vs.&nbsp;ML approaches:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Traditional</th>
<th>ML Approach</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Water detection</strong></td>
<td>Manual NDWI threshold</td>
<td>Learn optimal threshold + texture from examples</td>
</tr>
<tr class="even">
<td><strong>Land cover</strong></td>
<td>Rule-based classification</td>
<td>Random Forest or CNN with training samples</td>
</tr>
<tr class="odd">
<td><strong>Flood mapping</strong></td>
<td>Expert visual interpretation</td>
<td>U-Net segmentation trained on labeled floods</td>
</tr>
<tr class="even">
<td><strong>Crop monitoring</strong></td>
<td>Fixed vegetation index thresholds</td>
<td>LSTM time series model learning phenology</td>
</tr>
<tr class="odd">
<td><strong>Building detection</strong></td>
<td>Manual digitization</td>
<td>YOLO or Faster R-CNN object detection</td>
</tr>
<tr class="even">
<td><strong>Deforestation</strong></td>
<td>Visual comparison of dates</td>
<td>Siamese networks for change detection</td>
</tr>
</tbody>
</table>
<div class="philippine-context">
<p><strong>Philippine Operational Systems:</strong></p>
<p>The Philippines demonstrates successful AI/ML deployment:</p>
<ul>
<li><strong>DATOS (DOST-ASTI):</strong> AI-powered flood mapping from Sentinel-1 SAR achieves 10-20 minute response time during typhoons</li>
<li><strong>PRiSM (PhilRice-IRRI):</strong> Operational since 2014, first satellite-based rice monitoring in Southeast Asia</li>
<li><strong>SkAI-Pinas (DOST):</strong> National AI framework addressing the gap between abundant remote sensing data and sustainable AI pipelines</li>
<li><strong>DIMER Model Repository (DOST-ASTI):</strong> Democratizing access to trained models for Philippine contexts</li>
</ul>
</div>
<hr>
</section>
</section>
<section id="part-2-the-aiml-workflow-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</h2>
<p>Understanding the complete workflow is essential for successful EO projects. Each step matters, and according to 2024 research, most underperforming models suffer from data issues rather than algorithm deficiencies.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[1. Problem Definition&lt;br/&gt;What question?&lt;br/&gt;What output?] --&gt; B[2. Data Acquisition&lt;br/&gt;Satellite imagery&lt;br/&gt;Ground truth&lt;br/&gt;Ancillary data]

    B --&gt; C[3. Data Preprocessing&lt;br/&gt;Atmospheric correction&lt;br/&gt;Cloud masking&lt;br/&gt;Normalization]

    C --&gt; D[4. Data Annotation&lt;br/&gt;Label training samples&lt;br/&gt;Quality control&lt;br/&gt;Class balancing]

    D --&gt; E[5. Feature Engineering&lt;br/&gt;Spectral indices&lt;br/&gt;Texture features&lt;br/&gt;Temporal metrics]

    E --&gt; F{6. Train/Val/Test&lt;br/&gt;Split}
    F --&gt;|70%| G[Training Set]
    F --&gt;|15%| H[Validation Set]
    F --&gt;|15%| I[Test Set]

    G --&gt; J[7. Model Training&lt;br/&gt;Select architecture&lt;br/&gt;Set hyperparameters&lt;br/&gt;Train on GPU]

    H --&gt; K[8. Model Validation&lt;br/&gt;Tune hyperparameters&lt;br/&gt;Monitor overfitting&lt;br/&gt;Early stopping]

    J --&gt; K
    K --&gt;|Iterate| J

    K --&gt; L{Performance&lt;br/&gt;Acceptable?}
    L --&gt;|No| M[Improve Data&lt;br/&gt;More samples&lt;br/&gt;Better labels&lt;br/&gt;Data augmentation]
    M --&gt; D

    L --&gt;|Yes| N[9. Model Testing&lt;br/&gt;Final evaluation&lt;br/&gt;Unseen test set&lt;br/&gt;Confusion matrix]

    I --&gt; N

    N --&gt; O[10. Deployment&lt;br/&gt;Production system&lt;br/&gt;Monitoring&lt;br/&gt;Maintenance]

    O --&gt; P{Model Drift?&lt;br/&gt;Performance&lt;br/&gt;degraded?}
    P --&gt;|Yes| Q[Retrain with&lt;br/&gt;new data]
    Q --&gt; D
    P --&gt;|No| O

    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style B fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style C fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style D fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
    style J fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff
    style K fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff
    style O fill:#009999,stroke:#006666,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> Complete AI/ML Workflow for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
<section id="step-1-problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="step-1-problem-definition">Step 1: Problem Definition</h3>
<p><strong>Define clearly what you want to achieve:</strong></p>
<ul>
<li>What question are you answering? (e.g., “Where are mangroves declining?”)</li>
<li>What output do you need? (map, time series, alert system?)</li>
<li>What accuracy is acceptable?</li>
<li>What constraints exist? (time, computational resources, data availability)</li>
<li>What is the operational context and who will use the outputs?</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Example: PRiSM Rice Monitoring</strong></p>
<p><strong>Problem:</strong> Provide timely rice area and production estimates for food security planning and disaster response</p>
<p><strong>Clear definition:</strong> - Multi-class: rice wet season, rice dry season, non-rice agriculture, non-agriculture - 10m spatial resolution (Sentinel-1 SAR + Sentinel-2 optical) - Temporal: Per-season mapping (wet: June-Nov, dry: Dec-May) - Accuracy target: &gt;90% for policy-level decisions - Operational: Automated processing, quarterly updates to DA/PCIC - Cloud-penetrating capability essential for monsoon season</p>
</div>
</section>
<section id="step-2-data-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="step-2-data-acquisition">Step 2: Data Acquisition</h3>
<p><strong>Gather all necessary data:</strong></p>
<ul>
<li><strong>Satellite imagery:</strong> Sentinel-1/2, Landsat, commercial VHR, hyperspectral</li>
<li><strong>Ground truth:</strong> Field surveys, high-res imagery interpretation, existing maps, crowdsourced data</li>
<li><strong>Ancillary data:</strong> DEM, climate, administrative boundaries, road networks, socioeconomic data</li>
</ul>
<p><strong>Data volume considerations:</strong> - NASA’s Earth Science Data Systems: 148 PB (2023) → 205 PB (2024) → 250 PB (2025) - Sentinel constellation generates thousands of terabytes daily - 1,052 active EO satellites as of 2024</p>
<p><strong>Data sources for Philippines:</strong></p>
<ul>
<li><strong>CoPhil Mirror Site (2025):</strong> Local, high-bandwidth access to Sentinel data covering entire archipelago</li>
<li><strong>Copernicus Data Space Ecosystem:</strong> STAC-compliant catalogues, API-driven access</li>
<li><strong>Google Earth Engine:</strong> Harmonized Sentinel-2 surface reflectance, Sentinel-1 GRD collections</li>
<li><strong>PhilSA SIYASAT:</strong> NovaSAR-1 X-band SAR data</li>
<li><strong>Diwata-1/2 microsatellites:</strong> Philippine-operated disaster monitoring</li>
<li><strong>NAMRIA Geoportal:</strong> Land cover basemaps, topographic data</li>
<li><strong>PAGASA:</strong> Climate and meteorological data</li>
</ul>
</section>
<section id="step-3-data-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="step-3-data-pre-processing">Step 3: Data Pre-processing</h3>
<p><strong>Critical step - “Garbage in, garbage out”</strong></p>
<p>Data pre-processing is foundational and directly impacts downstream analysis accuracy. Proper preprocessing ensures data quality, consistency, and comparability across time and sensors.</p>
<p><strong>For satellite imagery:</strong></p>
<p><strong>Atmospheric Correction:</strong> - <strong>Purpose:</strong> Remove atmospheric effects (scattering, absorption) - <strong>Convert:</strong> Top-of-Atmosphere (TOA) reflectance → Surface reflectance (SR) - <strong>Sentinel-2:</strong> Use Level-2A products (Sen2Cor algorithm) - <strong>HLS Products:</strong> NASA’s Harmonized Landsat Sentinel-2 applies LaSRC + BRDF normalization - <strong>Essential for:</strong> Multi-temporal comparisons, quantitative biophysical parameter retrieval</p>
<p><strong>Cloud Masking:</strong> - <strong>Sentinel-2 SCL:</strong> Scene Classification Layer (clouds, shadows, snow, water, vegetation) - <strong>Machine learning approaches:</strong> U-Net architectures for pixel-wise cloud segmentation - <strong>Multi-temporal approaches:</strong> Leverage temporal patterns to identify clouds - <strong>Gap filling:</strong> Temporal interpolation, spatial interpolation, deep learning reconstruction (Prithvi-EO-2.0)</p>
<p><strong>SAR-Specific Preprocessing (Sentinel-1):</strong> - <strong>Orbit file application:</strong> Precise geolocation - <strong>Radiometric calibration:</strong> Convert DN to sigma nought (σ⁰), beta nought (β⁰), or gamma nought (γ⁰) - <strong>De-bursting:</strong> Remove black boundaries between sub-swaths in TOPS mode - <strong>Speckle filtering:</strong> Lee, Frost, Gamma-MAP filters; CNN-based despecklers preserve edges - <strong>Terrain correction (RTC):</strong> Orthorectification using DEM (SRTM, Copernicus DEM) - <strong>Multi-temporal filtering:</strong> Leverage temporal stack to reduce speckle</p>
<p><strong>Normalization and Scaling:</strong> - <strong>Min-max normalization:</strong> Scale to [0, 1] range - <strong>Z-score standardization:</strong> Center to mean=0, std=1 (common for deep learning) - <strong>Percentile clipping:</strong> Reduce impact of outliers (e.g., 2nd and 98th percentiles) - <strong>Per-band normalization:</strong> Account for different dynamic ranges across spectral bands</p>
<p><strong>For training labels:</strong></p>
<ul>
<li><strong>Quality control:</strong> Verify label accuracy through multiple reviewers</li>
<li><strong>Coordinate alignment:</strong> Ensure labels match imagery timing and location</li>
<li><strong>Class balancing:</strong> Ensure adequate samples per class</li>
<li><strong>Format standardization:</strong> Convert to ML-ready format (GeoTIFF, TFRecord, COG)</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Pre-processing Pitfalls
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common errors that degrade model performance:</strong></p>
<ul>
<li>Using Top-of-Atmosphere instead of surface reflectance</li>
<li>Temporal mismatch: 2020 imagery with 2018 labels</li>
<li>Incomplete cloud masking leaving cloud shadows</li>
<li>Mixed pixels at boundaries (especially for validation)</li>
<li>Inconsistent band ordering across scenes</li>
<li>Ignoring spatial autocorrelation (random train-test splits can lead to 28% overoptimistic performance)</li>
<li>Not applying same preprocessing to training and deployment data</li>
</ul>
</div>
</div>
</section>
<section id="step-4-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="step-4-feature-engineering">Step 4: Feature Engineering</h3>
<p><strong>Deriving informative variables from raw data</strong></p>
<p>Feature engineering transforms raw satellite data into informative representations that enhance model performance. This step is crucial for traditional ML algorithms and beneficial even for deep learning.</p>
<p><strong>For traditional ML (Random Forest, SVM):</strong></p>
<p><strong>Spectral Indices:</strong> - <strong>Vegetation:</strong> NDVI, EVI, SAVI, NDRE, GNDVI, LAI - <strong>Water:</strong> NDWI, MNDWI, NDMI - <strong>Built-up:</strong> NDBI, Bare Soil Index (BSI) - <strong>Burn:</strong> NBR, dNBR</p>
<p><strong>Textural Features (GLCM):</strong> - Contrast, Correlation, Energy, Homogeneity, Entropy, Dissimilarity, Variance - Window sizes: 3×3, 5×5, 7×7 - Multiple directions: 0°, 45°, 90°, 135°</p>
<p><strong>Temporal Features:</strong> - Statistical: Mean, median, std dev, min, max, percentiles (10th, 25th, 75th, 90th) - Coefficient of Variation (CV): Normalized variability measure - Amplitude: Difference between peak and minimum - Phenological: Start of season (SOS), Peak of season (POS), End of season (EOS) - Trends: Linear regression slopes, breakpoint detection</p>
<p><strong>Multi-Modal Features:</strong> - <strong>Optical-SAR fusion:</strong> Concatenate optical indices with SAR backscatter - <strong>Derived ratios:</strong> VV/VH polarization ratio, optical/SAR combinations - <strong>SAR texture:</strong> GLCM features from backscatter - <strong>Interferometric:</strong> Coherence from InSAR</p>
<p><strong>Example: Forest classification features</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Spectral indices</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>NDVI <span class="op">=</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> Red)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>NDWI <span class="op">=</span> (Green <span class="op">-</span> NIR) <span class="op">/</span> (Green <span class="op">+</span> NIR)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>EVI <span class="op">=</span> <span class="fl">2.5</span> <span class="op">*</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> <span class="dv">6</span><span class="op">*</span>Red <span class="op">-</span> <span class="fl">7.5</span><span class="op">*</span>Blue <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># SAR features</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>VV_VH_ratio <span class="op">=</span> VV_backscatter <span class="op">/</span> VH_backscatter</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>SAR_texture <span class="op">=</span> GLCM_contrast(VH_backscatter)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Temporal</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>NDVI_mean <span class="op">=</span> mean(NDVI_time_series)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>NDVI_cv <span class="op">=</span> std(NDVI_time_series) <span class="op">/</span> NDVI_mean</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Topographic</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>Elevation, Slope, Aspect</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Input feature vector per pixel</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, EVI, NDWI,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>     VV, VH, VV_VH_ratio, SAR_texture, NDVI_mean, NDVI_cv,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>     Elevation, Slope, Aspect]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>For deep learning (CNNs, U-Net, Vision Transformers):</strong></p>
<ul>
<li>Less manual feature engineering needed</li>
<li>Networks automatically learn features from raw pixels</li>
<li>Still benefit from good input data (cloud-free, calibrated, normalized)</li>
<li>Multi-spectral bands as input channels</li>
<li>Consider temporal stacking for multi-date analysis</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>2024 Research Insight: Feature Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Seven unsupervised dimensionality reduction algorithms tested on hyperspectral data from HYPSO-1 satellite showed that:</p>
<ul>
<li>Careful feature selection can achieve optimal accuracy with &lt;20% of temporal instances</li>
<li>Single band from single sensor can be sufficient for specific tasks</li>
<li><strong>Implication:</strong> Smart data selection &gt; brute force data collection</li>
<li>Use PCA, MNF, or tree-based feature importance for efficient selection</li>
</ul>
</div>
</div>
</section>
<section id="step-5-model-selection-and-training" class="level3">
<h3 class="anchored" data-anchor-id="step-5-model-selection-and-training">Step 5: Model Selection and Training</h3>
<p><strong>Choose appropriate algorithm:</strong></p>
<p><strong>Consider:</strong></p>
<ul>
<li>Task type (classification, regression, segmentation, object detection)</li>
<li>Data size (deep learning needs more data; transfer learning reduces requirements)</li>
<li>Interpretability requirements (operational systems often need explainability)</li>
<li>Computational resources (edge devices vs.&nbsp;cloud platforms)</li>
<li>Deployment constraints (inference speed, model size)</li>
</ul>
<p><strong>Common EO algorithms:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 11%">
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Type</th>
<th>Best For</th>
<th>Data Needs</th>
<th>Key Strengths</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Random Forest</strong></td>
<td>Ensemble</td>
<td>Classification, feature importance, baseline</td>
<td>Medium (100s-1000s)</td>
<td>Robust, interpretable, handles high dimensions</td>
</tr>
<tr class="even">
<td><strong>SVM</strong></td>
<td>Kernel</td>
<td>Binary classification, small data</td>
<td>Small-Medium</td>
<td>Effective high-dimensional spaces</td>
</tr>
<tr class="odd">
<td><strong>XGBoost/LightGBM</strong></td>
<td>Gradient Boosting</td>
<td>Tabular features, yield prediction</td>
<td>Medium</td>
<td>High performance on structured data</td>
</tr>
<tr class="even">
<td><strong>CNN</strong></td>
<td>Deep Learning</td>
<td>Image classification, automatic features</td>
<td>Large (10,000s+)</td>
<td>Spatial awareness, hierarchical learning</td>
</tr>
<tr class="odd">
<td><strong>U-Net</strong></td>
<td>Deep Learning</td>
<td>Semantic segmentation (pixel-wise)</td>
<td>Large</td>
<td>Skip connections, works with limited data</td>
</tr>
<tr class="even">
<td><strong>ResNet</strong></td>
<td>Deep Learning</td>
<td>Very deep networks, complex classification</td>
<td>Large</td>
<td>Residual connections avoid vanishing gradients</td>
</tr>
<tr class="odd">
<td><strong>Vision Transformer</strong></td>
<td>Deep Learning</td>
<td>Global context, spatial relationships</td>
<td>Very Large</td>
<td>Attention mechanisms, long-range dependencies</td>
</tr>
<tr class="even">
<td><strong>LSTM/GRU</strong></td>
<td>Deep Learning</td>
<td>Time series prediction, phenology</td>
<td>Large</td>
<td>Temporal pattern learning</td>
</tr>
<tr class="odd">
<td><strong>YOLO/Faster R-CNN</strong></td>
<td>Deep Learning</td>
<td>Object detection (buildings, ships)</td>
<td>Large</td>
<td>Real-time detection, bounding boxes</td>
</tr>
</tbody>
</table>
<p><strong>Training process:</strong></p>
<ol type="1">
<li><strong>Split data:</strong> 70-80% training, 10-15% validation, 10-15% testing
<ul>
<li><strong>Spatial cross-validation:</strong> Essential for EO to avoid spatial leakage</li>
<li>Spatial k-fold or buffered leave-one-out</li>
</ul></li>
<li><strong>Feed training data:</strong> Algorithm adjusts parameters to minimize error</li>
<li><strong>Monitor validation:</strong> Track performance on held-out validation set</li>
<li><strong>Hyperparameter tuning:</strong> Optimize learning rate, batch size, architecture parameters</li>
<li><strong>Early stopping:</strong> Stop when validation performance plateaus</li>
<li><strong>Final evaluation:</strong> Test on completely independent test set</li>
</ol>
<p><strong>Transfer Learning:</strong> - Pre-train on large dataset (ImageNet, SatViT, Prithvi) - Fine-tune on task-specific data - Reduces training data requirements by 10-100× - Faster convergence and better generalization</p>
<div class="philippine-context">
<p><strong>Philippine Example: Poverty Mapping with Transfer Learning</strong></p>
<p>Study using satellite imagery, nighttime lights, and OpenStreetMap data: - Transfer learning from ImageNet improved performance by 14.1% for tropical cyclone impact areas - Requires careful hyperparameter tuning for generalization across regions - Cost-effective approach for limited labeled data scenarios</p>
</div>
</section>
<section id="step-6-validation-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="step-6-validation-and-evaluation">Step 6: Validation and Evaluation</h3>
<p><strong>Rigorous testing on independent data</strong></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Never Test on Training Data!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Testing on data the model has seen gives falsely optimistic results. Always use held-out test data. For EO, random train-test splits can overestimate performance by up to 28% due to spatial autocorrelation.</p>
</div>
</div>
<p><strong>Classification metrics:</strong></p>
<ul>
<li><strong>Overall Accuracy (OA):</strong> Percentage of correctly classified pixels</li>
<li><strong>Confusion Matrix:</strong> Shows which classes are confused with each other</li>
<li><strong>Per-Class Metrics:</strong>
<ul>
<li>Producer’s Accuracy (Recall): How many ground truth samples were correctly classified</li>
<li>User’s Accuracy (Precision): How many predicted samples are actually correct</li>
</ul></li>
<li><strong>F1-Score:</strong> Harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall))</li>
<li><strong>Kappa Coefficient:</strong> Agreement accounting for chance</li>
<li><strong>Matthews Correlation Coefficient (MCC):</strong> Balanced measure even for imbalanced classes</li>
</ul>
<p><strong>Semantic Segmentation metrics:</strong></p>
<ul>
<li><strong>IoU (Intersection over Union):</strong> Area of overlap / Area of union
<ul>
<li>IoU &gt; 0.5: Generally acceptable</li>
<li>IoU &gt; 0.7: High-quality segmentation</li>
<li>IoU &gt; 0.9: Excellent agreement</li>
</ul></li>
<li><strong>Mean IoU (mIoU):</strong> Average IoU across all classes</li>
<li><strong>Dice Coefficient:</strong> 2 × IoU / (1 + IoU), less harsh penalty than IoU</li>
<li><strong>Pixel Accuracy:</strong> Simple but biased toward majority class</li>
<li><strong>Boundary F1 Score:</strong> Precision/recall on boundary pixels</li>
</ul>
<p><strong>Regression metrics:</strong></p>
<ul>
<li><strong>RMSE (Root Mean Squared Error):</strong> Average prediction error, penalizes large errors</li>
<li><strong>MAE (Mean Absolute Error):</strong> Average absolute deviation, less sensitive to outliers</li>
<li><strong>R² (Coefficient of Determination):</strong> Proportion of variance explained (0.88 = 88% explained)</li>
</ul>
<p><strong>Object Detection metrics:</strong></p>
<ul>
<li><strong>Precision/Recall:</strong> At various IoU thresholds (0.5, 0.75)</li>
<li><strong>Average Precision (AP):</strong> Area under precision-recall curve</li>
<li><strong>Mean Average Precision (mAP):</strong> Mean AP across classes</li>
</ul>
<p><strong>Philippine Example: Flood mapping evaluation</strong></p>
<pre><code>Confusion Matrix (DOST-ASTI DATOS flood detection):
                Predicted
              | Flood | No Flood |
Actual Flood  |  450  |   50     |  Producer's Acc (Recall): 90%
Actual No Flood|  30   |  1470    |  Producer's Acc: 98%

User's Accuracy (Precision): 93.8%   96.7%
Overall Accuracy: 96%
F1-Score (Flood class): 91.8%</code></pre>
<p><strong>Spatial Validation Best Practices:</strong></p>
<ol type="1">
<li><strong>Spatial k-fold cross-validation:</strong> Divide data into spatially homogeneous clusters</li>
<li><strong>Buffered leave-one-out:</strong> Create buffer zones around test samples</li>
<li><strong>Independent geographic testing:</strong> Test on completely different regions</li>
<li><strong>Temporal validation:</strong> Train on one time period, test on another</li>
</ol>
</section>
<section id="step-7-deployment-and-operationalization" class="level3">
<h3 class="anchored" data-anchor-id="step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</h3>
<p><strong>Making the model operational:</strong></p>
<p><strong>Deployment strategies:</strong></p>
<ol type="1">
<li><strong>Batch processing:</strong> Apply model to large archives (entire countries, multi-year time series)</li>
<li><strong>Near real-time:</strong> Process new satellite acquisitions automatically (disaster response)</li>
<li><strong>On-demand:</strong> User-triggered analysis (web portals, APIs)</li>
<li><strong>Edge processing:</strong> On-board satellite AI (ESA Φsat-2, launched 2024)</li>
</ol>
<p><strong>On-Board AI Processing (2024 Breakthrough):</strong> - ESA Φsat-2: 22cm CubeSat with on-board AI - Processes imagery directly in orbit - Cloud filtering: Only clear, usable images sent to Earth - Reduces data transmission costs, enables real-time event detection - <strong>Rationale:</strong> With 1,052 active EO satellites generating thousands of terabytes daily, traditional communication cannot relay this volume</p>
<p><strong>Operational considerations:</strong></p>
<ul>
<li><strong>Scalability:</strong> Can it handle regional/national scale?</li>
<li><strong>Automation:</strong> Minimize manual intervention</li>
<li><strong>Performance monitoring:</strong> Track accuracy over time, detect distribution shifts</li>
<li><strong>Model retraining:</strong> Update as conditions change or new data becomes available</li>
<li><strong>Model versioning:</strong> Maintain reproducibility and track improvements</li>
<li><strong>Integration:</strong> Connect to decision support systems, early warning platforms</li>
<li><strong>API development:</strong> Create accessible interfaces for inference</li>
<li><strong>Cloud deployment:</strong> Google Earth Engine, AWS SageMaker, Azure ML, Vertex AI</li>
</ul>
<p><strong>Philippine context:</strong></p>
<ul>
<li><strong>DOST-ASTI AIPI platform:</strong> For model deployment and user-facing AI interfaces</li>
<li><strong>DIMER repository:</strong> For model sharing and democratizing access to trained models</li>
<li><strong>ALaM (Automated Labeling Machine):</strong> Combining automated labeling with crowdsourcing for continuous data quality improvement</li>
<li><strong>Integration with LGU disaster response protocols:</strong> DATOS outputs delivered to local government units</li>
<li><strong>Delivery via PhilSA Digital Space Campus:</strong> Training and capacity building</li>
<li><strong>CoPhil Data Centre (2025):</strong> Cloud-native distribution with API-driven access</li>
</ul>
<hr>
</section>
</section>
<section id="part-3-types-of-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="part-3-types-of-machine-learning">Part 3: Types of Machine Learning</h2>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph Supervised["SUPERVISED LEARNING&lt;br/&gt;Learning from labeled examples"]
        S1[Classification&lt;br/&gt;Discrete categories]
        S2[Regression&lt;br/&gt;Continuous values]
        S3[Object Detection&lt;br/&gt;Locate + classify]
        S4[Semantic Segmentation&lt;br/&gt;Pixel-wise classification]

        S1 --&gt; S1A[Land Cover&lt;br/&gt;Forest, Water, Urban]
        S2 --&gt; S2A[Biomass Estimation&lt;br/&gt;Predict AGB in tons/ha]
        S3 --&gt; S3A[Building Detection&lt;br/&gt;YOLO, Faster R-CNN]
        S4 --&gt; S4A[Flood Mapping&lt;br/&gt;U-Net segmentation]
    end

    subgraph Unsupervised["UNSUPERVISED LEARNING&lt;br/&gt;Finding patterns without labels"]
        U1[Clustering&lt;br/&gt;Group similar pixels]
        U2[Dimensionality&lt;br/&gt;Reduction]
        U3[Anomaly&lt;br/&gt;Detection]

        U1 --&gt; U1A[K-Means&lt;br/&gt;ISODATA&lt;br/&gt;Spectral clusters]
        U2 --&gt; U2A[PCA&lt;br/&gt;MNF Transform&lt;br/&gt;Band reduction]
        U3 --&gt; U3A[Outlier Detection&lt;br/&gt;Change hotspots]
    end

    subgraph SemiSupervised["SEMI-SUPERVISED&lt;br/&gt;Combine labeled + unlabeled"]
        SS1[Self-training&lt;br/&gt;Pseudo-labeling]
        SS2[Co-training&lt;br/&gt;Multiple views]
        SS1 --&gt; SS1A[Bootstrap from&lt;br/&gt;limited labels]
    end

    subgraph Reinforcement["REINFORCEMENT LEARNING&lt;br/&gt;Learn from interaction"]
        R1[Agent-Environment&lt;br/&gt;Interaction]
        R1 --&gt; R1A[Drone Path&lt;br/&gt;Planning]
    end

    style Supervised fill:#e6ffe6,stroke:#00aa44,stroke-width:3px
    style Unsupervised fill:#ffe6e6,stroke:#cc0044,stroke-width:3px
    style SemiSupervised fill:#fff4e6,stroke:#ff8800,stroke-width:3px
    style Reinforcement fill:#e6e6ff,stroke:#6666cc,stroke-width:3px
</pre>
</div>
<p></p><figcaption> Machine Learning Paradigms for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<p><strong>Learning from labeled data</strong></p>
<p>The algorithm is given: - <strong>Input:</strong> Satellite image or features - <strong>Output:</strong> Known label (class or value) - <strong>Goal:</strong> Learn mapping from input to output</p>
<p>Supervised learning is the predominant approach in Earth Observation, where models learn from labeled training data to make predictions on new, unseen data.</p>
<section id="classification-tasks" class="level4">
<h4 class="anchored" data-anchor-id="classification-tasks">Classification Tasks</h4>
<p><strong>Predicting categorical labels</strong></p>
<p><strong>Common Algorithms:</strong> - <strong>Random Forest (RF):</strong> Ensemble method, best performance in object-based classification; robust to noise - <strong>Support Vector Machines (SVM):</strong> Effective for high-dimensional spaces; performs well with limited training samples - <strong>CNNs (Convolutional Neural Networks):</strong> Deep learning for automatic feature learning and complex patterns - <strong>Vision Transformers (ViT):</strong> Attention mechanisms for global context and long-range dependencies</p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Land Cover Classification</strong>
<ul>
<li>Input: Sentinel-2 pixel values (13 bands)</li>
<li>Output: Forest, Water, Urban, Agriculture, Bare soil, Wetlands</li>
<li>Algorithm: Random Forest, CNN, Vision Transformer</li>
<li>Datasets: EuroSAT (27,000 images, 10 classes, 98.57% accuracy)</li>
</ul></li>
<li><strong>Cloud Detection</strong>
<ul>
<li>Input: Multi-band imagery (blue, cirrus bands effective)</li>
<li>Output: Cloud vs.&nbsp;Clear, Cloud shadow, Cirrus</li>
<li>Algorithm: Threshold, Random Forest, U-Net for pixel-wise segmentation</li>
<li>Sentinel-2 SCL: Scene Classification Layer with 11 classes</li>
</ul></li>
<li><strong>Crop Type Mapping</strong>
<ul>
<li>Input: Multi-temporal NDVI, SAR backscatter</li>
<li>Output: Rice, Corn, Sugarcane, Coconut, Vegetables</li>
<li>Algorithm: Random Forest, LSTM (for temporal patterns)</li>
<li>Multi-temporal data &gt; single-date imagery for capturing phenology</li>
</ul></li>
</ol>
<div class="philippine-context">
<p><strong>Philippine Case Study: PhilSA-DENR Mangrove Mapping</strong></p>
<p><strong>Task:</strong> AI-based mangrove forest identification nationwide</p>
<p><strong>Technology:</strong> - Google Earth Engine platform - Mangrove Vegetation Index (MVI) - Sentinel-1 and Sentinel-2 fusion - Multi-temporal Landsat 8 and Sentinel-2 data</p>
<p><strong>Approach:</strong> - U-Net deep learning architecture - Binary classification: mangrove vs.&nbsp;non-mangrove - SAR for cloud-penetrating capability during monsoon</p>
<p><strong>Performance:</strong> - Accuracy: 99.73% (Myanmar study using similar approach) - Random Forest classifier also effective with high temporal resolution (5-day Sentinel-2) - Support Vector Machine shows high accuracy for mangrove discrimination</p>
<p><strong>Applications:</strong> - Blue carbon programs and carbon stock monitoring - Ecosystem service valuation - Conservation planning and restoration monitoring - Palawan multi-spatiotemporal analysis with Markov chain future trend prediction</p>
<p><strong>Result:</strong> Operational nationwide mangrove extent maps with regular updates</p>
</div>
<p><strong>Object Detection:</strong></p>
<p>Unlike pixel-level classification, object detection identifies and localizes specific objects of interest with bounding boxes.</p>
<p><strong>Popular Architectures:</strong> - <strong>YOLO (You Only Look Once):</strong> Real-time detection, single-stage architecture, fast inference - <strong>Faster R-CNN:</strong> Two-stage detector with region proposals, high accuracy - <strong>RetinaNet:</strong> Single-stage with focal loss for handling class imbalance - <strong>EfficientDet:</strong> Scalable architecture balancing accuracy and efficiency</p>
<p><strong>Key Considerations:</strong> - Scale variation: Objects appear at different sizes depending on altitude and resolution - Dense object detection: Multiple overlapping objects in urban or agricultural scenes - Small object detection: Challenging for standard architectures (e.g., individual trees, vehicles)</p>
<p><strong>Applications:</strong> - Building footprint extraction (SpaceNet, xView datasets) - Ship detection in maritime surveillance - Vehicle counting in traffic monitoring - Individual tree crown delineation</p>
<p><strong>Benchmark Dataset: xView</strong> - &gt;1 million objects - 60 classes - &gt;1,400 km² coverage - 0.3m resolution (WorldView-3) - Purpose: Disaster response, overhead imagery analysis</p>
</section>
<section id="regression-tasks" class="level4">
<h4 class="anchored" data-anchor-id="regression-tasks">Regression Tasks</h4>
<p><strong>Predicting continuous values</strong></p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Biomass Estimation</strong>
<ul>
<li>Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices, LiDAR (GEDI)</li>
<li>Output: Forest biomass (tons per hectare), Above-ground biomass (AGB)</li>
<li>Algorithm: Random Forest Regression (most popular: R² = 0.70, RMSE = 25.38 Mg C ha⁻¹)</li>
<li>Multi-sensor integration: LiDAR + SAR + Optical improves accuracy</li>
<li><strong>Note:</strong> SAR saturates at high biomass levels; LiDAR methods achieve ~90% agreement with field data</li>
</ul></li>
<li><strong>Soil Moisture Prediction</strong>
<ul>
<li>Input: Sentinel-1 VV/VH polarization, temperature, NDVI</li>
<li>Output: Volumetric soil moisture (%)</li>
<li>Algorithm: Neural network regression, Random Forest</li>
</ul></li>
<li><strong>Crop Yield Forecasting</strong>
<ul>
<li>Input: NDVI time series, EVI, NDMI, weather data (temperature, precipitation), soil properties</li>
<li>Output: Expected yield (tons per hectare)</li>
<li>Algorithm: LSTM regression (preferred in &gt;40% of studies), Random Forest</li>
<li>Performance: R² &gt; 0.93 for corn and soybean, RMSE &lt; 0.075 for NDVI estimation</li>
<li>Temporal considerations: Early-season (higher uncertainty) vs.&nbsp;end-of-season (more accurate, less actionable)</li>
</ul></li>
</ol>
<div class="philippine-context">
<p><strong>Philippine Operational System: PRiSM Rice Yield Prediction</strong></p>
<p><strong>Overview:</strong> Philippine Rice Information System, operational since 2014</p>
<p><strong>Technology:</strong> - Synthetic Aperture Radar (SAR): Day and night, cloud-penetrating - Crop modeling and cloud computing - UAV imagery and smartphone field surveys - Statistical data integration</p>
<p><strong>Data Sources:</strong> - Remote sensing satellites (Sentinel-1, RADARSAT) - Vegetation indices (NDVI, EVI) - Weather data from PAGASA - Historical yield records</p>
<p><strong>Modeling:</strong> - LSTM networks for temporal modeling of SAR backscatter and vegetation indices - Random Forest for integrating multi-source data - Phenology-based models</p>
<p><strong>Partners:</strong> - International Rice Research Institute (IRRI) - technology development - Philippine Rice Research Institute (PhilRice) - operations since 2018 - Department of Agriculture (DA) - policy formulation and planning</p>
<p><strong>Applications:</strong> - Food security planning and policy formulation - Disaster response (typhoon impact assessment) - Crop insurance (PCIC integration) - Per-season mapping: Wet season (June-Nov), Dry season (Dec-May)</p>
<p><strong>Significance:</strong> First satellite-based rice monitoring system in Southeast Asia, model for regional applications</p>
</div>
<p><strong>Key difference from classification:</strong> - Output is a number on a continuous scale rather than discrete classes - Loss functions measure distance from true value (MSE, MAE, RMSE) - Evaluation uses regression metrics (R², RMSE, MAE) - Predictions can be interpolated and extrapolated</p>
</section>
</section>
<section id="unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised Learning</h3>
<p><strong>Finding patterns in unlabeled data</strong></p>
<p>The algorithm receives: - <strong>Input:</strong> Satellite imagery or features - <strong>No labels provided</strong> - <strong>Goal:</strong> Discover inherent structure or groupings</p>
<p>Unsupervised learning techniques do not require labeled training data, making them valuable for exploratory analysis, data reduction, and scenarios where ground-truth is unavailable or expensive to obtain.</p>
<section id="clustering" class="level4">
<h4 class="anchored" data-anchor-id="clustering">Clustering</h4>
<p><strong>Grouping similar pixels/regions together</strong></p>
<p><strong>Common algorithm: k-means</strong></p>
<ol type="1">
<li>Specify number of clusters (k)</li>
<li>Algorithm iteratively groups pixels with similar spectral characteristics</li>
<li>Result: Image segmented into k clusters</li>
<li><strong>Human interpretation needed:</strong> “Cluster 1 looks like water, Cluster 2 like forest…”</li>
</ol>
<p><strong>Other Clustering Methods:</strong></p>
<p><strong>Hierarchical Clustering:</strong> - Builds tree-like structure (dendrogram) of nested clusters - Agglomerative (bottom-up) or Divisive (top-down) - No need to specify number of clusters a priori - Applications: Multi-scale land cover analysis, ecological zone identification</p>
<p><strong>DBSCAN (Density-Based Spatial Clustering):</strong> - Groups points based on density - Identifies clusters of arbitrary shape - Robust to outliers - Applications: Urban area detection, anomaly detection in satellite imagery</p>
<p><strong>EO Applications:</strong></p>
<ul>
<li><strong>Exploratory analysis:</strong> “How many distinct spectral classes in this region?”</li>
<li><strong>Change detection:</strong> Cluster before/after images to find anomalies</li>
<li><strong>Image segmentation:</strong> Group similar pixels for object-based analysis</li>
<li><strong>InSAR time series:</strong> K-means with PCA for identifying spatially and temporally coherent displacement phenomena</li>
</ul>
</section>
<section id="dimensionality-reduction" class="level4">
<h4 class="anchored" data-anchor-id="dimensionality-reduction">Dimensionality Reduction</h4>
<p><strong>Principal Component Analysis (PCA):</strong> - Linear transformation projecting high-dimensional data onto orthogonal axes of maximum variance - Applications: Hyperspectral data compression, feature extraction, noise reduction, change detection - <strong>Workflow:</strong> Center data → Compute covariance → Calculate eigenvalues/eigenvectors → Select top K components - <strong>Benefits:</strong> Reduces computational requirements, removes redundancy, enhances signal-to-noise ratio - First few PCs capture most variance</p>
<p><strong>Independent Component Analysis (ICA):</strong> - Separates multivariate signal into independent, non-Gaussian components - Applications: Mixed pixel decomposition, blind source separation, endmember extraction in hyperspectral imagery</p>
<p><strong>t-SNE and UMAP:</strong> - Non-linear dimensionality reduction for visualization and exploratory analysis - Preserves local structure, reveals clusters and patterns in 2D/3D - Applications: Visualization of high-dimensional feature spaces, exploration of spectral diversity - <strong>Limitations:</strong> Computationally expensive, hyperparameter sensitive, not suitable for new data projection (t-SNE)</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>When to Use Unsupervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Advantages:</strong> - No need for expensive labeled data - Can discover unexpected patterns - Good for initial data exploration - Data reduction for preprocessing</p>
<p><strong>Limitations:</strong> - Results need interpretation - No guarantee clusters match desired classes - Often less accurate than supervised methods for specific tasks - Difficult to evaluate objectively without labels - Determining optimal number of clusters can be challenging</p>
<p><strong>Best Practice:</strong> Use unsupervised methods for exploration, then refine with supervised learning for operational applications</p>
</div>
</div>
<p><strong>Comparison Example:</strong></p>
<p><strong>Supervised (Land Cover Classification):</strong> - Provide 1000 labeled samples: forest, water, urban - Train Random Forest - Result: Every pixel assigned forest/water/urban - Evaluation: 90% accuracy against test labels</p>
<p><strong>Unsupervised (k-means Clustering):</strong> - No labels provided - Run k-means with k=3 - Result: Three clusters emerge - Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare - Evaluation: Subjective or requires labels anyway</p>
<p><strong>Integration Strategy:</strong> 1. Use clustering to create initial training samples 2. Apply dimensionality reduction (PCA) before classification 3. Combine unsupervised pre-training with supervised fine-tuning 4. Use clustering for quality control of labeled data</p>
<hr>
</section>
</section>
</section>
<section id="part-4-deep-learning-architectures-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-4-deep-learning-architectures-for-earth-observation">Part 4: Deep Learning Architectures for Earth Observation</h2>
<section id="what-is-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-deep-learning">What is Deep Learning?</h3>
<p><strong>Deep Learning = Neural Networks with Many Layers</strong></p>
<ul>
<li>Subset of machine learning</li>
<li>Inspired by biological neurons</li>
<li>Multiple processing layers extract progressively abstract features</li>
<li>Dominant approach for image analysis since ~2012</li>
<li>Revolutionized Earth Observation, enabling automated feature learning and state-of-the-art performance</li>
</ul>
<p><strong>Why “deep”?</strong> - Refers to depth: many hidden layers - Modern networks: 10s to 100s of layers (ResNet-152 has 152 layers) - Enables learning complex, hierarchical representations - Lower layers: Edges, textures - Middle layers: Patterns, shapes - Higher layers: Semantic concepts, objects</p>
<p><strong>Key Advantages for EO:</strong> - Automatic feature extraction from raw pixels - Spatial awareness through convolutional operations - Multi-scale analysis capabilities - Handles large, complex datasets - Transfer learning reduces data requirements</p>
</section>
<section id="neural-network-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-fundamentals">Neural Network Fundamentals</h3>
<section id="the-artificial-neuron" class="level4">
<h4 class="anchored" data-anchor-id="the-artificial-neuron">The Artificial Neuron</h4>
<p><strong>Building block of neural networks:</strong></p>
<pre><code>Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output</code></pre>
<p><strong>Mathematical operation:</strong></p>
<ol type="1">
<li><strong>Weighted sum:</strong> <code>z = w1*x1 + w2*x2 + w3*x3 + b</code></li>
<li><strong>Activation function:</strong> <code>output = activation(z)</code></li>
</ol>
<p><strong>Example: Detecting bright pixels</strong></p>
<pre><code>Inputs: [Red=0.8, Green=0.7, NIR=0.9]
Weights: [w1=1.0, w2=1.0, w3=1.0]
Bias: b = -2.0

z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4
output = ReLU(0.4) = 0.4  (indicates moderately bright)</code></pre>
</section>
<section id="network-architecture" class="level4">
<h4 class="anchored" data-anchor-id="network-architecture">Network Architecture</h4>
<p><strong>Layers of neurons:</strong></p>
<ol type="1">
<li><strong>Input Layer:</strong> Receives raw data (e.g., pixel values from all spectral bands)</li>
<li><strong>Hidden Layers:</strong> Process and transform data through learned representations</li>
<li><strong>Output Layer:</strong> Produces final prediction (class probabilities or continuous values)</li>
</ol>
<p><strong>For a simple image classification:</strong></p>
<pre><code>Input Layer (256 neurons = 16×16 image)
   ↓
Hidden Layer 1 (128 neurons with ReLU)
   ↓
Hidden Layer 2 (64 neurons with ReLU)
   ↓
Output Layer (5 neurons = 5 classes, softmax activation)</code></pre>
<p>Each connection has a <strong>weight</strong> - the network learns optimal weights through training via backpropagation.</p>
</section>
<section id="activation-functions" class="level4">
<h4 class="anchored" data-anchor-id="activation-functions">Activation Functions</h4>
<p><strong>Introduce non-linearity - crucial for learning complex patterns</strong></p>
<p><strong>Common activation functions:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Equation</th>
<th>Range</th>
<th>Use Case</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td><code>max(0, x)</code></td>
<td>[0, ∞)</td>
<td>Hidden layers (most common)</td>
<td>Simple, efficient, avoids vanishing gradient</td>
</tr>
<tr class="even">
<td><strong>Sigmoid</strong></td>
<td><code>1 / (1 + e^-x)</code></td>
<td>(0, 1)</td>
<td>Binary classification output</td>
<td>Smooth, probabilistic interpretation</td>
</tr>
<tr class="odd">
<td><strong>Softmax</strong></td>
<td><code>e^xi / Σe^xj</code></td>
<td>(0, 1), sum=1</td>
<td>Multi-class classification output</td>
<td>Converts logits to probabilities</td>
</tr>
<tr class="even">
<td><strong>Tanh</strong></td>
<td><code>(e^x - e^-x) / (e^x + e^-x)</code></td>
<td>(-1, 1)</td>
<td>Hidden layers (older networks)</td>
<td>Zero-centered, smooth</td>
</tr>
<tr class="odd">
<td><strong>LeakyReLU</strong></td>
<td><code>max(αx, x), α=0.01</code></td>
<td>(-∞, ∞)</td>
<td>Hidden layers</td>
<td>Allows small negative gradient</td>
</tr>
<tr class="even">
<td><strong>GELU</strong></td>
<td><code>x * Φ(x)</code></td>
<td>(-∞, ∞)</td>
<td>Transformers</td>
<td>Smooth, stochastic regularization</td>
</tr>
</tbody>
</table>
<p><strong>Why activation functions matter:</strong></p>
<p>Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth! Networks would only learn linear decision boundaries.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ReLU: The Default Choice
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ReLU (Rectified Linear Unit)</strong> has become standard for hidden layers because:</p>
<ul>
<li>Simple: <code>f(x) = max(0, x)</code></li>
<li>Computationally efficient</li>
<li>Avoids vanishing gradient problem that plagued sigmoid/tanh</li>
<li>Empirically performs very well across diverse problems</li>
<li>Sparsity: ~50% of neurons set to zero, acting as automatic feature selection</li>
</ul>
</div>
</div>
</section>
<section id="loss-functions" class="level4">
<h4 class="anchored" data-anchor-id="loss-functions">Loss Functions</h4>
<p><strong>Measure how wrong the model’s predictions are</strong></p>
<p>The model’s objective: <strong>minimize the loss function</strong> through gradient descent optimization.</p>
<p><strong>For classification:</strong></p>
<p><strong>Categorical Cross-Entropy (Log Loss):</strong></p>
<pre><code>Loss = -Σ(y_true * log(y_pred))</code></pre>
<ul>
<li>Penalizes confident wrong predictions heavily</li>
<li>Encourages high probability for correct class</li>
<li>Standard for multi-class classification</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>True class: Forest (encoded as [1, 0, 0, 0, 0])
Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest
Loss = -1*log(0.7) = 0.36

Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest
Loss = -1*log(0.2) = 1.61  (much higher penalty)</code></pre>
<p><strong>Binary Cross-Entropy:</strong> - For binary classification (e.g., flood vs.&nbsp;no-flood) - Similar principle, optimized for two classes</p>
<p><strong>Focal Loss:</strong> - Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Used in RetinaNet object detection</p>
<p><strong>For semantic segmentation:</strong></p>
<p><strong>Dice Loss:</strong> - Based on Dice coefficient: 2×IoU / (1 + IoU) - Differentiable, suitable for optimization - More balanced for small objects - Often used in medical imaging and EO segmentation</p>
<p><strong>IoU Loss:</strong> - Directly optimizes intersection over union - Less strict than Dice for small discrepancies</p>
<p><strong>For regression:</strong></p>
<p><strong>Mean Squared Error (MSE):</strong></p>
<pre><code>Loss = (1/n) * Σ(y_true - y_pred)²</code></pre>
<p><strong>Example: Biomass prediction:</strong></p>
<pre><code>True: 150 tons/ha
Prediction: 140 tons/ha
Error: 10 tons/ha
Squared Error: 100
MSE = 100 (if single sample)</code></pre>
<p><strong>Mean Absolute Error (MAE):</strong> - Less sensitive to outliers than MSE - More robust when errors follow non-Gaussian distribution</p>
<p><strong>Huber Loss:</strong> - Combination of MSE (small errors) and MAE (large errors) - Robust to outliers while maintaining smooth gradient</p>
</section>
<section id="optimizers" class="level4">
<h4 class="anchored" data-anchor-id="optimizers">Optimizers</h4>
<p><strong>Algorithms that adjust weights to minimize loss</strong></p>
<p><strong>The process:</strong></p>
<ol type="1">
<li>Calculate loss on current batch of data</li>
<li>Compute gradients (via backpropagation): how should each weight change?</li>
<li>Update weights in direction that reduces loss</li>
<li>Repeat thousands/millions of times across epochs</li>
</ol>
<p><strong>Common optimizers:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 28%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Optimizer</th>
<th>Description</th>
<th>Learning Rate</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD</strong></td>
<td>Stochastic Gradient Descent</td>
<td>Constant or scheduled</td>
<td>Simple, well-understood, good for fine-tuning</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>Adaptive Moment Estimation</td>
<td>Adaptive per parameter</td>
<td>Default choice, usually works well</td>
</tr>
<tr class="odd">
<td><strong>AdamW</strong></td>
<td>Adam with Weight Decay</td>
<td>Adaptive</td>
<td>Improved generalization, Transformers</td>
</tr>
<tr class="even">
<td><strong>RMSprop</strong></td>
<td>Root Mean Square Propagation</td>
<td>Adaptive</td>
<td>Good for RNNs/LSTMs</td>
</tr>
<tr class="odd">
<td><strong>AdaGrad</strong></td>
<td>Adaptive Gradient</td>
<td>Adaptive</td>
<td>Features vary in frequency</td>
</tr>
</tbody>
</table>
<p><strong>Adam is most popular</strong> because: - Adapts learning rate per parameter - Combines benefits of momentum (accelerates convergence) and adaptive learning - Requires minimal tuning - Works well across diverse problems - Default hyperparameters (lr=0.001, β1=0.9, β2=0.999) often sufficient</p>
<p><strong>Learning Rate Scheduling:</strong> - <strong>Step Decay:</strong> Reduce learning rate at fixed intervals - <strong>Cosine Annealing:</strong> Smooth decay following cosine function - <strong>Warm-up:</strong> Gradually increase learning rate at beginning - <strong>ReduceLROnPlateau:</strong> Reduce when validation loss plateaus</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Training Terminology
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Epoch:</strong> One complete pass through the entire training dataset</p>
<p><strong>Batch:</strong> Subset of training data processed together before updating weights</p>
<p><strong>Iteration:</strong> One weight update (one batch processed)</p>
<p><strong>Example:</strong> - Training data: 10,000 samples - Batch size: 100 - 1 epoch = 100 iterations (10,000 / 100) - Training for 50 epochs = 5,000 iterations</p>
<p><strong>Typical batch sizes for EO:</strong> - Images: 16-64 (limited by GPU memory) - Patches: 32-128 - Time series samples: 64-256</p>
</div>
</div>
</section>
<section id="the-training-process" class="level4">
<h4 class="anchored" data-anchor-id="the-training-process">The Training Process</h4>
<p><strong>Iterative improvement:</strong></p>
<pre><code>1. Initialize weights (random or pre-trained)
2. For each epoch:
    For each batch:
        a. Forward pass: Compute predictions
        b. Calculate loss
        c. Backward pass: Compute gradients (backpropagation)
        d. Update weights using optimizer
    e. Evaluate on validation set
    f. Save checkpoint if best performance
3. Stop when validation performance plateaus (early stopping)</code></pre>
<p><strong>Monitoring training:</strong></p>
<ul>
<li><strong>Training loss should decrease</strong> - model learning patterns in training data</li>
<li><strong>Validation loss should decrease</strong> - model generalizing to new data</li>
<li><strong>If validation loss increases while training loss decreases:</strong> Overfitting! Apply regularization.</li>
<li><strong>If both losses remain high:</strong> Underfitting. Need more capacity or better features.</li>
</ul>
<p><strong>Regularization techniques:</strong> - <strong>Dropout:</strong> Randomly deactivate neurons during training - <strong>Weight Decay (L2):</strong> Penalize large weights - <strong>Data Augmentation:</strong> Artificially expand training data - <strong>Early Stopping:</strong> Stop training when validation loss stops improving - <strong>Batch Normalization:</strong> Normalize activations, improves stability</p>
</section>
</section>
<section id="convolutional-neural-networks-cnns" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3>
<p><strong>CNNs are the foundation of modern computer vision and EO analysis</strong></p>
<section id="why-cnns-excel-at-eo" class="level4">
<h4 class="anchored" data-anchor-id="why-cnns-excel-at-eo">Why CNNs Excel at EO</h4>
<p><strong>Traditional ML:</strong> - Manual feature engineering needed (NDVI, GLCM textures) - Limited ability to capture spatial patterns - Each pixel treated somewhat independently - Features fixed before training</p>
<p><strong>CNNs:</strong> - <strong>Automatic feature extraction</strong> from raw pixels - <strong>Spatial awareness</strong> through convolutional filters - <strong>Hierarchical learning:</strong> edges → textures → objects → scenes - <strong>Translation invariance:</strong> Detects patterns anywhere in image - <strong>Parameter sharing:</strong> Same filters applied across entire image (efficiency) - <strong>Multi-scale analysis:</strong> Through pooling and different kernel sizes</p>
</section>
<section id="cnn-architecture-components" class="level4">
<h4 class="anchored" data-anchor-id="cnn-architecture-components">CNN Architecture Components</h4>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[Input Image&lt;br/&gt;Sentinel-2&lt;br/&gt;64x64x13 bands] --&gt; B[Conv Layer 1&lt;br/&gt;32 filters 3x3&lt;br/&gt;ReLU activation]

    B --&gt; C[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;32x32x32]

    C --&gt; D[Conv Layer 2&lt;br/&gt;64 filters 3x3&lt;br/&gt;ReLU activation]

    D --&gt; E[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;16x16x64]

    E --&gt; F[Conv Layer 3&lt;br/&gt;128 filters 3x3&lt;br/&gt;ReLU activation]

    F --&gt; G[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;8x8x128]

    G --&gt; H[Flatten&lt;br/&gt;8192 neurons]

    H --&gt; I[Dense Layer&lt;br/&gt;256 neurons&lt;br/&gt;ReLU + Dropout]

    I --&gt; J[Output Layer&lt;br/&gt;Softmax&lt;br/&gt;6 classes]

    J --&gt; K[Predictions&lt;br/&gt;Forest: 0.85&lt;br/&gt;Water: 0.05&lt;br/&gt;Urban: 0.03&lt;br/&gt;Agriculture: 0.04&lt;br/&gt;Bare: 0.02&lt;br/&gt;Wetlands: 0.01]

    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style B fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style C fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style D fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style E fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style F fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style G fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style I fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style J fill:#e6e6ff,stroke:#6666cc,stroke-width:2px
    style K fill:#ccffcc,stroke:#00aa44,stroke-width:2px
</pre>
</div>
<p></p><figcaption> Convolutional Neural Network Architecture for Land Cover Classification</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Convolutional Layers:</strong> - Apply learnable filters (kernels) across image - Each filter detects specific patterns (edges, textures, shapes) - Example: 3×3 kernel slides across image, computing dot product - Multiple filters per layer (e.g., 64, 128, 256 filters) - Stride controls movement (stride=1: every pixel, stride=2: every other pixel) - Padding preserves spatial dimensions</p>
<p><strong>Pooling Layers:</strong> - Reduce spatial dimensions - Max pooling: Take maximum value in window (common) - Average pooling: Take average value - Increases receptive field - Provides translation invariance - Reduces computational cost</p>
<p><strong>Fully Connected Layers:</strong> - Traditional neural network layers at end - Flatten spatial features - Perform final classification - Often replaced by Global Average Pooling in modern architectures</p>
</section>
<section id="popular-cnn-architectures-for-eo" class="level4">
<h4 class="anchored" data-anchor-id="popular-cnn-architectures-for-eo">Popular CNN Architectures for EO</h4>
<p><strong>VGG Networks (VGG16, VGG19):</strong> - Deep architecture with small (3×3) convolutional filters - Simple, uniform design - 16 or 19 layers - Large memory footprint - <strong>EO Application:</strong> VGG16 with instance normalization applied for LULC classification - Good for transfer learning from ImageNet</p>
<p><strong>ResNet (Residual Networks):</strong> - <strong>Innovation:</strong> Skip connections address vanishing gradient problem - Enables very deep networks (50, 101, 152 layers) - Residual blocks: <code>output = F(x) + x</code> - <strong>Performance:</strong> ResNet-18 and ResNet-50 widely used as encoders in semantic segmentation - <strong>EO Success:</strong> U-Net with ResNet encoder achieved precision 0.943 and recall 0.954 for building extraction - Winner architecture in many EO competitions</p>
<p><strong>Inception/GoogLeNet:</strong> - Multi-scale feature extraction - Parallel convolutions with different kernel sizes (1×1, 3×3, 5×5) - Computationally efficient through 1×1 bottleneck layers - <strong>EO Application:</strong> Multi-scale land cover classification</p>
<p><strong>EfficientNet:</strong> - <strong>Innovation:</strong> Compound scaling of depth, width, and resolution - Optimal balance between accuracy and computational efficiency - EfficientNet-B0 to B7 variants - <strong>EO Application:</strong> Increasingly popular for resource-constrained applications - Mobile deployment, edge computing</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Transfer Learning for EO
</div>
</div>
<div class="callout-body-container callout-body">
<p>Pre-trained CNNs (trained on ImageNet) are widely used in EO:</p>
<p><strong>Advantages:</strong> - Reduce training time - Improve performance with limited data - Lower layers learn generic features (edges, textures) applicable across domains</p>
<p><strong>Considerations:</strong> - ImageNet uses RGB images; EO often has more bands - Solutions: Use only RGB bands, or initialize additional channels with pre-trained weights - Fine-tune all layers or freeze early layers</p>
<p><strong>Recent Research (2024):</strong> Self-supervised pre-training on RS data (SatMAE, SatViT) offers modest improvements over ImageNet in few-shot settings, especially when pre-trained on domain-specific EO data.</p>
</div>
</div>
</section>
</section>
<section id="u-net-and-semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="u-net-and-semantic-segmentation">U-Net and Semantic Segmentation</h3>
<section id="u-net-architecture" class="level4">
<h4 class="anchored" data-anchor-id="u-net-architecture">U-Net Architecture</h4>
<p><strong>Description:</strong> Encoder-decoder architecture with skip connections, originally designed for biomedical image segmentation but widely adopted for Earth Observation.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    subgraph Encoder["ENCODER (Contracting Path)"]
        A[Input&lt;br/&gt;SAR Image&lt;br/&gt;256x256x2&lt;br/&gt;VV, VH] --&gt; B[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]
        B --&gt; C[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]
        C --&gt; D[Max Pool 2x2&lt;br/&gt;128x128x64]

        D --&gt; E[Conv 3x3&lt;br/&gt;128 filters]
        E --&gt; F[Conv 3x3&lt;br/&gt;128 filters]
        F --&gt; G[Max Pool 2x2&lt;br/&gt;64x64x128]

        G --&gt; H[Conv 3x3&lt;br/&gt;256 filters]
        H --&gt; I[Conv 3x3&lt;br/&gt;256 filters]
        I --&gt; J[Max Pool 2x2&lt;br/&gt;32x32x256]
    end

    subgraph Bottleneck["BOTTLENECK"]
        J --&gt; K[Conv 3x3&lt;br/&gt;512 filters]
        K --&gt; L[Conv 3x3&lt;br/&gt;512 filters]
    end

    subgraph Decoder["DECODER (Expanding Path)"]
        L --&gt; M[Up-Conv 2x2&lt;br/&gt;256 filters&lt;br/&gt;64x64x256]
        M --&gt; N[Concatenate&lt;br/&gt;with I]
        N --&gt; O[Conv 3x3&lt;br/&gt;256 filters]
        O --&gt; P[Conv 3x3&lt;br/&gt;256 filters]

        P --&gt; Q[Up-Conv 2x2&lt;br/&gt;128 filters&lt;br/&gt;128x128x128]
        Q --&gt; R[Concatenate&lt;br/&gt;with F]
        R --&gt; S[Conv 3x3&lt;br/&gt;128 filters]
        S --&gt; T[Conv 3x3&lt;br/&gt;128 filters]

        T --&gt; U[Up-Conv 2x2&lt;br/&gt;64 filters&lt;br/&gt;256x256x64]
        U --&gt; V[Concatenate&lt;br/&gt;with C]
        V --&gt; W[Conv 3x3&lt;br/&gt;64 filters]
        W --&gt; X[Conv 3x3&lt;br/&gt;64 filters]
    end

    X --&gt; Y[Conv 1x1&lt;br/&gt;2 classes&lt;br/&gt;Sigmoid]
    Y --&gt; Z[Output&lt;br/&gt;Flood Mask&lt;br/&gt;256x256x2&lt;br/&gt;Water/No-Water]

    I -.-&gt;|Skip Connection| N
    F -.-&gt;|Skip Connection| R
    C -.-&gt;|Skip Connection| V

    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style Encoder fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style Bottleneck fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style Decoder fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style Z fill:#ccffcc,stroke:#00aa44,stroke-width:3px
</pre>
</div>
<p></p><figcaption> U-Net Architecture for Semantic Segmentation (Flood Mapping Example)</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Architecture:</strong> - <strong>Encoder (Contracting Path):</strong> Progressively downsamples input (convolutional + pooling), capturing context - <strong>Decoder (Expanding Path):</strong> Upsamples features (transpose convolution), enabling precise localization - <strong>Skip Connections:</strong> Concatenate encoder features with decoder at same resolution, preserving spatial information - Fully symmetric structure</p>
<p><strong>Why U-Net Works Well:</strong> - Skip connections preserve fine-grained spatial information lost during downsampling - Works with relatively small training datasets (important for EO where labels are expensive) - End-to-end pixel-wise predictions - Multi-scale feature fusion</p>
<p><strong>Applications in EO:</strong> - Land cover semantic segmentation - Building footprint extraction - Road network mapping - Crop field delineation - Water body detection - Flood extent mapping</p>
<div class="philippine-context">
<p><strong>Philippine Case Study: Benguet Province Deforestation</strong></p>
<p><strong>Study Details:</strong> - Location: Benguet Province tropical montane forest - Time period: 2015 to early 2022 - Total deforestation detected: 417.93 km² - <strong>Significance:</strong> First deep learning application in Southeast Asian montane forests</p>
<p><strong>Methods:</strong> - Sentinel-1 SAR and Sentinel-2 optical fusion - U-Net deep learning architecture - Comparison with Random Forest and K-Nearest Neighbors</p>
<p><strong>Performance:</strong> - <strong>Accuracy: 99.73%</strong> for binary forest/non-forest classification - Outperformed traditional ML methods - Validated effectiveness of multi-sensor data fusion - Demonstrated U-Net suitability for tropical conditions</p>
<p><strong>Technology Advantages:</strong> - SAR cloud-penetrating capability fills observational gap in tropics - Multi-temporal analysis detects gradual and abrupt changes - Automated processing enables continuous monitoring - Scalable to national level</p>
</div>
<p><strong>U-Net Variants and Improvements:</strong></p>
<p><strong>UNet++:</strong> - Nested skip connections for improved gradient flow - Dense skip pathways - Better feature aggregation</p>
<p><strong>Attention U-Net:</strong> - Incorporates attention mechanisms to focus on relevant features - Attention gates highlight salient features - Improved performance on complex scenes</p>
<p><strong>3D U-Net:</strong> - Extends to volumetric data or multi-temporal stacks - Temporal convolutions for time series - Applications: Crop monitoring, change detection</p>
<p><strong>U-Net with Advanced Encoders:</strong> - <strong>U-Net with ResNet encoder:</strong> Combines U-Net decoder with ResNet encoder - <strong>U-Net with SK-ResNeXt encoder:</strong> Integrates selective kernel and ResNeXt for enhanced feature extraction - <strong>UNetFormer:</strong> Hybrid CNN encoder + Transformer decoder (discussed below)</p>
<p><strong>Performance Metrics:</strong> - Mean IoU (mIoU): Average IoU across all classes - F1-Score per class - Boundary accuracy for precise delineation</p>
</section>
</section>
<section id="deeplab-family" class="level3">
<h3 class="anchored" data-anchor-id="deeplab-family">DeepLab Family</h3>
<p><strong>DeepLab:</strong> State-of-the-art semantic segmentation architecture family</p>
<p><strong>Key Innovations:</strong></p>
<p><strong>Atrous (Dilated) Convolutions:</strong> - Enlarge receptive field without losing resolution - Insert “holes” in convolution kernel - Capture multi-scale context efficiently</p>
<p><strong>Atrous Spatial Pyramid Pooling (ASPP):</strong> - Parallel atrous convolutions with different rates - Captures features at multiple scales - Aggregates information from different receptive fields</p>
<p><strong>Encoder-Decoder Structure (DeepLabv3+):</strong> - Similar to U-Net philosophy - Combines ASPP with decoder for refined boundaries</p>
<p><strong>Performance:</strong> - DeepLabv3+ shows superior performance compared to standard U-Net on many benchmarks - Improved Mean IoU - Better boundary delineation</p>
<p><strong>EO Applications:</strong> - Large-scale land cover mapping - Urban scene segmentation - Agricultural field boundaries</p>
<p><strong>Comparison: U-Net vs.&nbsp;DeepLab</strong> - <strong>U-Net:</strong> Better with limited data, simpler architecture, faster training - <strong>DeepLab:</strong> Better overall performance, more complex, requires more data - <strong>Both:</strong> Widely used in EO, choice depends on data availability and computational resources</p>
</section>
<section id="vision-transformers-vits" class="level3">
<h3 class="anchored" data-anchor-id="vision-transformers-vits">Vision Transformers (ViTs)</h3>
<p><strong>Paradigm shift from convolutions to self-attention mechanisms</strong></p>
<section id="fundamentals" class="level4">
<h4 class="anchored" data-anchor-id="fundamentals">Fundamentals</h4>
<p><strong>Architecture:</strong></p>
<ol type="1">
<li><strong>Patch Embedding:</strong> Divide image into fixed-size patches (e.g., 16×16 pixels)</li>
<li><strong>Linear Projection:</strong> Flatten patches and project to embedding dimension (e.g., 768-D)</li>
<li><strong>Positional Encoding:</strong> Add learnable position information to preserve spatial relationships</li>
<li><strong>Transformer Encoder:</strong> Stack of multi-head self-attention and feed-forward layers</li>
<li><strong>Classification Head:</strong> MLP (Multi-Layer Perceptron) for final prediction</li>
</ol>
<p><strong>Self-Attention Mechanism:</strong> - Models relationships between all image patches - Each patch “attends to” all other patches - Learns which patches are relevant for prediction - Captures long-range dependencies - Adaptively focuses on informative regions</p>
<p><strong>Mathematical Formulation:</strong></p>
<pre><code>Attention(Q, K, V) = softmax(QK^T / √d_k) V

Q = Query (what am I looking for?)
K = Key (what do I contain?)
V = Value (what information do I pass?)</code></pre>
<p><strong>Multi-Head Attention:</strong> - Multiple attention mechanisms in parallel - Each head learns different relationships - Aggregate outputs for richer representation</p>
</section>
<section id="advantages-for-eo" class="level4">
<h4 class="anchored" data-anchor-id="advantages-for-eo">Advantages for EO</h4>
<p><strong>Global Context Modeling:</strong> - Attention mechanism captures relationships across entire image from early layers - CNNs build up receptive field gradually through layers - Particularly valuable for EO where context matters (e.g., urban vs.&nbsp;rural forest)</p>
<p><strong>Long-Range Dependencies:</strong> - Can relate distant image regions - Example: Recognizing rice paddy requires context of surrounding infrastructure, water bodies</p>
<p><strong>Effective for Large-Scale Imagery:</strong> - Scales well to high-resolution satellite images - Efficient self-attention variants reduce computational cost</p>
<p><strong>Strong Transfer Learning:</strong> - Pre-trained ViTs transfer well across tasks - SatViT: Pre-trained on 1.3 million satellite-derived RS images</p>
<p><strong>Handles Variable Input Sizes:</strong> - Flexible patch-based approach - Can adapt to different image resolutions</p>
</section>
<section id="variants-for-remote-sensing" class="level4">
<h4 class="anchored" data-anchor-id="variants-for-remote-sensing">Variants for Remote Sensing</h4>
<p><strong>Swin Transformer:</strong> - <strong>Innovation:</strong> Hierarchical architecture with shifted windows - Local attention within windows (efficient computation) - Shifted window scheme enables cross-window connections - Multi-scale feature representation (like CNN feature pyramids) - State-of-the-art performance on many EO benchmarks</p>
<p><strong>ViT with Spectral Adaptation:</strong> - Modified patch embedding for multi-spectral inputs - Handles variable number of spectral bands (not just RGB) - Pre-training on large satellite image datasets - Applications: Hyperspectral classification, multi-sensor fusion</p>
<p><strong>SatViT:</strong> - Pre-trained on 1.3 million satellite-derived RS images - Domain-specific Vision Transformer for remote sensing - Improved transfer learning performance over ImageNet pre-training - Publicly available for EO community</p>
<p><strong>MS-CLIP (IBM, 2024):</strong> - First vision-language model for multi-spectral Sentinel-2 data - Adapts CLIP dual-encoder architecture for 10+ spectral bands - Enables zero-shot classification and image-text retrieval - <strong>Example:</strong> “Show me images with dense vegetation” without explicit classification</p>
</section>
<section id="challenges" class="level4">
<h4 class="anchored" data-anchor-id="challenges">Challenges</h4>
<p><strong>Data Requirements:</strong> - ViTs require large training datasets (millions of samples) - Less effective with small datasets compared to CNNs - Solution: Transfer learning from pre-trained models (SatViT, ImageNet)</p>
<p><strong>Computational Cost:</strong> - Self-attention quadratic complexity in number of patches - Memory intensive for high-resolution images - Solutions: Swin Transformer (local attention), efficient attention mechanisms</p>
<p><strong>Interpretability:</strong> - Attention maps provide some interpretability - Can visualize which patches model focuses on - Still less intuitive than CNN filter visualizations</p>
</section>
</section>
<section id="hybrid-architectures-unetformer" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-architectures-unetformer">Hybrid Architectures: UNetFormer</h3>
<p><strong>UNetFormer:</strong> Combines CNN encoders with Transformer decoders</p>
<p><strong>Description:</strong> Hybrid architecture leveraging strengths of both CNNs and Transformers</p>
<p><strong>Key Features:</strong> - <strong>CNN Encoder:</strong> ResNet18 captures local spatial features efficiently - <strong>Transformer Decoder:</strong> Models global context and long-range dependencies - <strong>Hybrid Design:</strong> Balances computational efficiency with modeling capacity - Skip connections from encoder to decoder (like U-Net)</p>
<p><strong>Performance:</strong> - State-of-the-art on remote sensing semantic segmentation benchmarks - Particularly effective for urban scene imagery - Outperforms pure CNN and pure Transformer approaches on many tasks</p>
<p><strong>Related Architectures:</strong> - <strong>UNeXt:</strong> Efficient network optimizing depth, width, and resolution - <strong>UNetFormer with boundary enhancement:</strong> Multi-scale approach for improved edge detection - <strong>Segformer:</strong> Transformer encoder + lightweight decoder</p>
<p><strong>When to Use:</strong> - Complex EO scenes requiring both local detail and global context - Semantic segmentation tasks with diverse object scales - When computational resources allow (more expensive than standard U-Net)</p>
</section>
<section id="temporal-models-for-time-series" class="level3">
<h3 class="anchored" data-anchor-id="temporal-models-for-time-series">Temporal Models for Time Series</h3>
<p><strong>Multi-temporal satellite data captures dynamic processes - temporal models extract these patterns</strong></p>
<section id="lstm-and-gru" class="level4">
<h4 class="anchored" data-anchor-id="lstm-and-gru">LSTM and GRU</h4>
<p><strong>LSTM (Long Short-Term Memory):</strong> - <strong>Purpose:</strong> Temporal pattern learning in sequential data - <strong>Architecture:</strong> Recurrent neural network with gating mechanisms - <strong>Gates:</strong> Input gate, forget gate, output gate, cell state - <strong>Advantage:</strong> Learns long-term dependencies, avoids vanishing gradient problem of vanilla RNNs</p>
<p><strong>Applications in EO:</strong> - <strong>Time series classification:</strong> Crop type mapping from multi-temporal NDVI - <strong>Phenology monitoring:</strong> Extracting growing season characteristics - <strong>Yield prediction:</strong> Forecasting crop yields from vegetation index time series - <strong>Change detection:</strong> Detecting disturbances in forest time series - <strong>Weather forecasting:</strong> Climate variables prediction</p>
<p><strong>GRU (Gated Recurrent Unit):</strong> - Simplified version of LSTM - Fewer parameters (faster training) - Often comparable performance to LSTM - Good choice when computational resources are limited</p>
<p><strong>Performance:</strong> - <strong>Most Used:</strong> RNNs applied in &gt;22% of EO time series studies - <strong>LSTMs Preferred:</strong> Used in &gt;40% of crop yield prediction studies - <strong>Accuracy:</strong> R² &gt; 0.93 for corn and soybean yield prediction</p>
<div class="philippine-context">
<p><strong>Philippine Application: Crop Yield Forecasting with LSTM</strong></p>
<p><strong>PRiSM Enhanced with Deep Learning:</strong></p>
<p><strong>Data:</strong> - Multi-temporal Sentinel-1 SAR backscatter (VV, VH polarizations) - Sentinel-2 NDVI time series - PAGASA weather data (rainfall, temperature) - Historical yield records from PhilRice</p>
<p><strong>Approach:</strong> - LSTM network processes time series sequentially - Captures phenological patterns (planting, vegetative growth, reproductive phase, maturity) - Integrates weather variables as auxiliary inputs - Trained on multi-year data across provinces</p>
<p><strong>Performance:</strong> - Earlier and more accurate yield forecasts than statistical models - Mid-season prediction (2-3 months before harvest) with acceptable accuracy - Integration with PRiSM for operational deployment</p>
<p><strong>Applications:</strong> - Food security early warning - Crop insurance (PCIC) - Agricultural planning and market stabilization - Disaster impact assessment</p>
</div>
</section>
<section id="convlstm" class="level4">
<h4 class="anchored" data-anchor-id="convlstm">ConvLSTM</h4>
<p><strong>ConvLSTM:</strong> Combines spatial convolutions with temporal LSTM</p>
<p><strong>Architecture:</strong> - Replaces matrix multiplications in LSTM with convolutional operations - Preserves spatial structure throughout temporal modeling - Input: 3D tensor (time, height, width) - Output: Spatial predictions over time</p>
<p><strong>Advantages:</strong> - Captures both spatial and temporal patterns simultaneously - More parameter efficient than separate spatial and temporal models - End-to-end learning</p>
<p><strong>Applications:</strong> - Weather forecasting and precipitation nowcasting - Flood prediction from time series of meteorological variables - Crop monitoring with spatial context - Spatiotemporal land cover change</p>
</section>
<section id="temporal-attention" class="level4">
<h4 class="anchored" data-anchor-id="temporal-attention">Temporal Attention</h4>
<p><strong>Lightweight Temporal Attention Encoder (L-TAE):</strong> - <strong>Innovation:</strong> Distributes channels among compact attention heads operating in parallel - Outperforms RNNs with fewer parameters and reduced computational complexity - Particularly effective for satellite image time series (SITS) classification</p>
<p><strong>Multi-Head Temporal Attention:</strong> - Each head attends to different temporal patterns - Learn complementary temporal representations - Aggregate outputs for final prediction</p>
<p><strong>Advantages over LSTMs:</strong> - Parallelizable (faster training) - Direct access to all time steps (no sequential bottleneck) - Attention weights provide interpretability (which dates are important?)</p>
<p><strong>Applications:</strong> - Crop type classification from Sentinel-2 time series - Land cover change detection - Phenology extraction</p>
</section>
<section id="temporal-transformers" class="level4">
<h4 class="anchored" data-anchor-id="temporal-transformers">Temporal Transformers</h4>
<p><strong>Transformer for Time Series:</strong> - Self-attention over temporal sequence - Positional encoding preserves temporal order - Can model arbitrarily long sequences</p>
<p><strong>TiMo (2025):</strong> - <strong>Description:</strong> Spatiotemporal vision transformer foundation model - <strong>Innovation:</strong> Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large satellite image time series datasets</p>
<p><strong>Advantages:</strong> - Global temporal context from first layer - Handles variable-length sequences - State-of-the-art performance on temporal EO tasks</p>
<p><strong>Challenges:</strong> - Requires large amounts of training data - Computationally expensive - Best suited for long time series (many observations)</p>
</section>
</section>
<section id="object-detection-architectures" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-architectures">Object Detection Architectures</h3>
<p><strong>Object detection identifies and localizes specific objects with bounding boxes</strong></p>
<section id="yolo-you-only-look-once" class="level4">
<h4 class="anchored" data-anchor-id="yolo-you-only-look-once">YOLO (You Only Look Once)</h4>
<p><strong>Characteristics:</strong> - <strong>Real-time detection:</strong> Single-pass architecture, processes entire image once - <strong>Fast inference:</strong> 30-60+ FPS depending on variant - <strong>Good accuracy-speed trade-off:</strong> Suitable for operational systems - <strong>Single-stage detector:</strong> Predicts bounding boxes and class probabilities directly</p>
<p><strong>Versions:</strong> - <strong>YOLOv3:</strong> Introduced multi-scale predictions - <strong>YOLOv4:</strong> Enhanced training techniques, better accuracy - <strong>YOLOv5:</strong> Popular, well-documented, easy to use (Ultralytics implementation) - <strong>YOLOv6-v8:</strong> Latest, best performance, improved small object detection - <strong>YOLO-NAS:</strong> Neural Architecture Search, state-of-the-art accuracy</p>
<p><strong>Applications in EO:</strong> - <strong>Building detection:</strong> Rapid mapping of structures for disaster damage assessment - <strong>Vehicle detection:</strong> Traffic monitoring, parking lot analysis - <strong>Ship detection:</strong> Maritime surveillance, illegal fishing monitoring - <strong>Small object detection:</strong> Improved in recent versions (important for vehicles, individual trees)</p>
<p><strong>Advantages:</strong> - Fast training and inference - Good generalization - Easy to deploy - Active community and pre-trained models</p>
</section>
<section id="r-cnn-family" class="level4">
<h4 class="anchored" data-anchor-id="r-cnn-family">R-CNN Family</h4>
<p><strong>Faster R-CNN:</strong> - <strong>Architecture:</strong> Two-stage detector - <strong>Stage 1:</strong> Region Proposal Network (RPN) generates candidate object locations - <strong>Stage 2:</strong> Classifies proposals and refines bounding boxes - <strong>Advantage:</strong> High accuracy, especially for diverse object sizes - <strong>Disadvantage:</strong> Slower than single-stage detectors like YOLO</p>
<p><strong>Mask R-CNN:</strong> - <strong>Extension of Faster R-CNN:</strong> Adds instance segmentation branch - <strong>Output:</strong> Bounding box + pixel-level mask for each object - <strong>Applications:</strong> - Building footprints with precise boundaries - Individual tree crown delineation - Object-level change detection - Counting objects (vehicles, animals) with high precision</p>
<p><strong>Performance:</strong> - Generally higher accuracy than YOLO - Better for complex scenes with occlusions - Preferred when accuracy is more important than speed</p>
</section>
<section id="retinanet" class="level4">
<h4 class="anchored" data-anchor-id="retinanet">RetinaNet</h4>
<p><strong>Key Innovation:</strong> - <strong>Focal Loss:</strong> Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Single-stage detector</p>
<p><strong>Advantages:</strong> - Excellent for imbalanced datasets (common in EO: rare objects like ships, rare land cover classes) - Competitive accuracy with two-stage detectors - Faster than R-CNN family</p>
<p><strong>Applications:</strong> - Rare object detection (e.g., informal settlements, landslides) - Multi-class detection with imbalanced classes</p>
</section>
<section id="efficientdet" class="level4">
<h4 class="anchored" data-anchor-id="efficientdet">EfficientDet</h4>
<p><strong>Key Innovation:</strong> - <strong>Compound scaling:</strong> Jointly scales resolution, depth, and width - <strong>BiFPN (Bi-directional Feature Pyramid Network):</strong> Efficient multi-scale feature fusion</p>
<p><strong>Advantages:</strong> - Optimal balance between accuracy and efficiency - Scalable (EfficientDet-D0 to D7) - Suitable for deployment on resource-constrained devices</p>
<p><strong>Applications:</strong> - Edge computing and mobile deployment - Operational systems requiring fast inference</p>
<p><strong>Comparison Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 17%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>YOLO</strong></td>
<td>Very Fast</td>
<td>Good</td>
<td>Real-time applications, rapid mapping</td>
</tr>
<tr class="even">
<td><strong>Faster R-CNN</strong></td>
<td>Slow</td>
<td>High</td>
<td>High-accuracy requirements, diverse object sizes</td>
</tr>
<tr class="odd">
<td><strong>Mask R-CNN</strong></td>
<td>Slow</td>
<td>High</td>
<td>Instance segmentation, precise boundaries</td>
</tr>
<tr class="even">
<td><strong>RetinaNet</strong></td>
<td>Moderate</td>
<td>High</td>
<td>Imbalanced datasets, rare objects</td>
</tr>
<tr class="odd">
<td><strong>EfficientDet</strong></td>
<td>Fast-Moderate</td>
<td>High</td>
<td>Balanced accuracy/speed, deployment</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="multi-modal-architectures" class="level3">
<h3 class="anchored" data-anchor-id="multi-modal-architectures">Multi-Modal Architectures</h3>
<p><strong>Integrating data from multiple sensors for robust monitoring</strong></p>
<section id="optical-sar-fusion" class="level4">
<h4 class="anchored" data-anchor-id="optical-sar-fusion">Optical-SAR Fusion</h4>
<p><strong>Complementary Information:</strong> - <strong>Optical:</strong> Rich spectral information (13 bands for Sentinel-2), sensitive to biochemical properties - <strong>SAR:</strong> Structural information (backscatter), penetrates clouds, sensitive to moisture and geometry</p>
<p><strong>Fusion Strategies:</strong></p>
<p><strong>Early Fusion (Input-level):</strong> - Concatenate inputs at beginning - Example: Stack Sentinel-2 bands with Sentinel-1 VV/VH as additional channels - Simple, but assumes features align semantically - <strong>Advantage:</strong> Single model processes all modalities - <strong>Disadvantage:</strong> May not capture modality-specific patterns optimally</p>
<p><strong>Late Fusion (Decision-level):</strong> - Separate models for each modality - Combine predictions (average, weighted average, voting) - <strong>Advantage:</strong> Each modality processed optimally - <strong>Disadvantage:</strong> Doesn’t exploit inter-modality relationships</p>
<p><strong>Intermediate Fusion (Feature-level):</strong> - Merge features at middle layers - Learn joint representations - <strong>Advantage:</strong> Balances early and late fusion benefits - <strong>Disadvantage:</strong> More complex architecture design</p>
<p><strong>Recent Approaches:</strong></p>
<p><strong>Progressive Fusion Learning:</strong> - Gradually integrates multimodal information - Addresses semantic misalignment between modalities - Applications: Building extraction with optical + SAR</p>
<p><strong>M2Caps (Multi-modal Capsule Networks, 2024):</strong> - Capsule networks for optical-SAR fusion - Applications: Land cover classification - Handles appearance disparities between modalities</p>
<p><strong>Bi-modal Contrastive Learning:</strong> - Self-supervised approach for joint representation - Pre-training on unlabeled optical-SAR pairs - Fine-tune for specific tasks (crop classification, change detection)</p>
<p><strong>Transformer Temporal-Spatial Model (TTSM):</strong> - Synergizes SAR and optical time-series for vegetation monitoring - <strong>Performance:</strong> R² &gt; 0.88 for vegetation reconstruction - Handles missing data in one modality</p>
<div class="philippine-context">
<p><strong>Philippine Application: All-Weather Rice Monitoring</strong></p>
<p><strong>PRiSM Multi-Sensor Approach:</strong></p>
<p><strong>Challenge:</strong> Philippines has &gt;60% cloud cover during monsoon season (June-November)</p>
<p><strong>Solution:</strong> Sentinel-1 SAR + Sentinel-2 optical fusion</p>
<p><strong>Methodology:</strong> - <strong>Sentinel-1 SAR:</strong> Primary data source during wet season (cloud-penetrating) - <strong>Sentinel-2 optical:</strong> Complementary data during dry season and cloud-free periods - <strong>Feature-level fusion:</strong> Combine SAR backscatter (VV, VH) with optical indices (NDVI, EVI) - <strong>Random Forest classifier:</strong> Trained on fused features</p>
<p><strong>Benefits:</strong> - Year-round monitoring regardless of weather - Higher accuracy than single-sensor approach - Reduced data gaps - Continuous rice area tracking</p>
<p><strong>Operational Impact:</strong> - Reliable per-season mapping even during typhoons - Supports disaster damage assessment - Improved yield prediction with temporal SAR backscatter patterns</p>
</div>
<p><strong>Challenges:</strong> - Modality alignment: Different imaging mechanisms (reflectance vs.&nbsp;backscatter) - Semantic misalignment: Features may not correspond across modalities - Optimal fusion level depends on task and data availability - Increased computational cost</p>
<p><strong>Applications:</strong> - All-weather land cover classification - Crop monitoring during cloudy seasons - Building extraction (optical for spectral, SAR for structure) - Flood mapping (SAR for water extent, optical for pre-event land cover) - Forest biomass estimation (optical for species, SAR for structure)</p>
</section>
</section>
<section id="foundation-models-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models-for-earth-observation">Foundation Models for Earth Observation</h3>
<p><strong>Foundation models are large, pre-trained models adaptable to various downstream tasks</strong></p>
<p>Emerged as transformative trend in EO 2023-2025, dramatically reducing resources required for environmental monitoring.</p>
<section id="prithvi-family-ibm-nasa" class="level4">
<h4 class="anchored" data-anchor-id="prithvi-family-ibm-nasa">Prithvi Family (IBM-NASA)</h4>
<p><strong>Prithvi-EO-1.0 (August 2023):</strong> - <strong>Scale:</strong> 100 million parameters - <strong>Training Data:</strong> NASA’s Harmonized Landsat Sentinel-2 (HLS) dataset - <strong>Pre-training Strategy:</strong> Masked autoencoder (MAE) - self-supervised learning on unlabeled imagery - <strong>Significance:</strong> World’s largest geospatial AI model at release - <strong>Availability:</strong> Open-source on Hugging Face</p>
<p><strong>Prithvi-EO-2.0 (December 2024):</strong> - <strong>Scale:</strong> 600 million parameters (6× larger than predecessor) - <strong>Training Data:</strong> 4.2 million global time series samples from HLS at 30m resolution - <strong>Architecture:</strong> Temporal transformer with location and temporal embeddings - <strong>Performance:</strong> 75.6% average score on GEO-bench framework (8% improvement over 1.0) - <strong>Availability:</strong> Hugging Face and IBM’s TerraTorch toolkit</p>
<p><strong>Applications Demonstrated:</strong> - <strong>Flood Mapping:</strong> Valencia, Spain floods (October 2024) using Sentinel-1 + Sentinel-2 - <strong>Burn Scar Detection:</strong> Wildfire impact assessment - <strong>Cloud Gap Reconstruction:</strong> Filling missing data in cloudy imagery - <strong>Multi-Temporal Crop Segmentation:</strong> Mapping crop types across United States</p>
<p><strong>Fine-Tuning Workflow:</strong> 1. Load pre-trained Prithvi model 2. Replace classification head for specific task 3. Fine-tune on small labeled dataset (hundreds to thousands of samples) 4. Deploy for inference</p>
<p><strong>Impact:</strong> - Enables users with limited ML expertise to deploy state-of-the-art models - Reduces labeled data requirements by 10-100× - Democratizes access to advanced AI for EO - Foundation for operational systems in resource-constrained settings</p>
<p><strong>Deployment:</strong> - Integrated into IBM’s TerraTorch toolkit for easy fine-tuning - Model zoo with pre-trained variants - Tutorials and example notebooks</p>
</section>
<section id="other-foundation-models" class="level4">
<h4 class="anchored" data-anchor-id="other-foundation-models">Other Foundation Models</h4>
<p><strong>SatMAE:</strong> - Masked autoencoding for satellite imagery - Self-supervised pre-training on unlabeled data - Transfer learning for downstream tasks - Competitive with ImageNet pre-training in few-shot scenarios</p>
<p><strong>SatViT:</strong> - Pre-trained Vision Transformer on 1.3 million satellite-derived RS images - Domain-specific for remote sensing - Improved transfer learning over ImageNet pre-training - Publicly available for EO community</p>
<p><strong>MS-CLIP (IBM, 2024):</strong> - First vision-language model for multi-spectral Sentinel-2 data - Dual encoder architecture adapted from CLIP - Handles 10+ spectral bands (not just RGB) - <strong>Capabilities:</strong> - Zero-shot classification: Classify without task-specific training - Image-text retrieval: “Find images with rice paddies” - Semantic search: Natural language queries over satellite archives</p>
<p><strong>TiMo (2025):</strong> - Spatiotemporal vision transformer foundation model for satellite image time series - Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large temporal satellite datasets</p>
<p><strong>Why Foundation Models Matter:</strong></p>
<ol type="1">
<li><strong>Data Efficiency:</strong> Pre-training on massive unlabeled data, fine-tune with small labeled sets</li>
<li><strong>Generalization:</strong> Learn robust representations applicable across tasks and regions</li>
<li><strong>Democratization:</strong> Lower barrier to entry for EO AI applications</li>
<li><strong>Rapid Deployment:</strong> Quickly adapt to new applications without training from scratch</li>
<li><strong>Transfer Across Domains:</strong> Models pre-trained globally applicable to local Philippine contexts</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Self-Supervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Foundation models typically use <strong>self-supervised learning</strong> for pre-training:</p>
<p><strong>Masked Autoencoding (MAE):</strong> - Randomly mask patches of input image - Model learns to reconstruct masked patches - Forces model to learn semantic representations - No labels needed - learns from structure of data itself</p>
<p><strong>Contrastive Learning (MoCo, SimCLR):</strong> - Learn representations by contrasting positive and negative pairs - Augmented views of same image are positive pairs - Different images are negative pairs - Model learns invariance to augmentations</p>
<p><strong>SSL4EO-S12 Dataset:</strong> - Large-scale, global, multimodal corpus from Sentinel-1 and Sentinel-2 - Supports self-supervised pre-training research - Multi-seasonal coverage - Enables research on contrastive learning for remote sensing</p>
</div>
</div>
</section>
</section>
<section id="training-strategies" class="level3">
<h3 class="anchored" data-anchor-id="training-strategies">Training Strategies</h3>
<p><strong>Transfer Learning:</strong> - <strong>Approach:</strong> Pre-train on large dataset (ImageNet, SatViT, Prithvi), fine-tune on task-specific data - <strong>Benefits:</strong> Reduces training time, improves performance with limited data - <strong>Best Practice:</strong> Freeze early layers (generic features), fine-tune later layers (task-specific features) - <strong>Recent Research (2024):</strong> Self-supervised pre-training on RS data offers modest improvements over ImageNet in few-shot settings</p>
<p><strong>Data Augmentation:</strong> - <strong>Rotation and flipping:</strong> Particularly suitable for satellite imagery (no canonical orientation) - <strong>Color jittering:</strong> Simulate atmospheric variations - <strong>Random crops:</strong> Increase spatial diversity - <strong>Mixup and CutMix:</strong> Regularization techniques for classification - <strong>Caution:</strong> Ensure augmentations are realistic for EO (e.g., don’t vertically flip landscapes with clear sky/ground distinction)</p>
<p><strong>Self-Supervised Learning:</strong> - <strong>Contrastive learning:</strong> MoCo, SimCLR for learning representations - <strong>Masked image modeling:</strong> MAE for learning to reconstruct images - <strong>Multi-modal alignment:</strong> CLIP-style vision-language pre-training - <strong>SSL4EO-2024 Summer School:</strong> First summer school on self-supervised learning for EO (July 2024, Copenhagen)</p>
<p><strong>Few-Shot Learning:</strong> - <strong>Motivation:</strong> Limited labeled data, expensive annotation - <strong>Methods:</strong> Metric learning, meta-learning, prototypical networks - <strong>Applications:</strong> Novel land cover classes, rare object detection, new geographic regions - <strong>Example:</strong> Gerry Roxas Foundation deforestation classification achieved 43% accuracy with only 8% training data</p>
<p><strong>Active Learning:</strong> - <strong>Strategy:</strong> Iteratively select most informative samples for labeling - <strong>Process:</strong> Train model → Find uncertain predictions → Label those → Retrain - <strong>Benefits:</strong> Reduced annotation cost (27% improvement in mIoU with only 2% labeled data) - <strong>WeakAL Framework:</strong> Combines active learning and weak supervision, computing &gt;90% of labels automatically while maintaining competitive performance</p>
<p><strong>Best Practices:</strong> - Start with pre-trained weights when available (Prithvi, SatViT, ImageNet) - Use appropriate learning rate schedules (cosine annealing with warm-up) - Apply batch normalization or layer normalization for training stability - Monitor overfitting through validation metrics (gap between train and validation loss) - Implement early stopping and model checkpointing - Use mixed-precision training (FP16) for faster training on modern GPUs</p>
<hr>
</section>
</section>
<section id="part-5-benchmark-datasets-for-training-and-validation" class="level2">
<h2 class="anchored" data-anchor-id="part-5-benchmark-datasets-for-training-and-validation">Part 5: Benchmark Datasets for Training and Validation</h2>
<p><strong>Benchmark datasets enable standardized comparison of algorithms and serve as training resources</strong></p>
<section id="patch-level-classification-datasets" class="level3">
<h3 class="anchored" data-anchor-id="patch-level-classification-datasets">Patch-Level Classification Datasets</h3>
<section id="eurosat" class="level4">
<h4 class="anchored" data-anchor-id="eurosat">EuroSAT</h4>
<p><strong>Specifications:</strong> - <strong>Images:</strong> 27,000 labeled images - <strong>Classes:</strong> 10 land cover types - <strong>Size:</strong> 64×64 pixel patches - <strong>Bands:</strong> 13 (Sentinel-2 multispectral) - <strong>Coverage:</strong> Europe - <strong>Classification Accuracy:</strong> 98.57% achieved with CNNs</p>
<p><strong>Classes:</strong> Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial Buildings, Pasture, Permanent Crop, Residential Buildings, River, Sea/Lake</p>
<p><strong>Access:</strong> - GitHub: https://github.com/phelber/EuroSAT - TensorFlow Datasets - PyTorch datasets - Commonly used for benchmarking deep learning architectures</p>
</section>
<section id="bigearthnet-v2.0" class="level4">
<h4 class="anchored" data-anchor-id="bigearthnet-v2.0">BigEarthNet v2.0</h4>
<p><strong>Specifications:</strong> - <strong>Patches:</strong> 549,488 paired Sentinel-1 and Sentinel-2 patches - <strong>Size:</strong> 1.2×1.2 km on ground - <strong>Classes:</strong> 19 (CORINE Land Cover nomenclature) - <strong>Type:</strong> Multi-label classification (multiple classes per patch) - <strong>Coverage:</strong> 10 European countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland)</p>
<p><strong>Key Features:</strong> - Multi-modal (optical + SAR) - Multi-label annotations (real-world complexity) - Large-scale (largest Sentinel dataset)</p>
<p><strong>Access:</strong> - Website: https://bigearth.net/ - TensorFlow Datasets - Papers With Code</p>
<p><strong>Applications:</strong> - Multi-label land cover classification - Multi-modal fusion research - Benchmark for semantic segmentation</p>
</section>
<section id="landcovernet" class="level4">
<h4 class="anchored" data-anchor-id="landcovernet">LandCoverNet</h4>
<p><strong>Specifications:</strong> - Global coverage - Sentinel-2 based - Multi-temporal (annual) - Multiple continents</p>
<p><strong>Applications:</strong> - Global land cover mapping benchmark - Multi-temporal classification - Seasonal analysis and phenology</p>
</section>
</section>
<section id="object-detection-datasets" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-datasets">Object Detection Datasets</h3>
<section id="xview" class="level4">
<h4 class="anchored" data-anchor-id="xview">xView</h4>
<p><strong>Specifications:</strong> - <strong>Objects:</strong> &gt;1 million annotated objects - <strong>Classes:</strong> 60 - <strong>Area:</strong> &gt;1,400 km² - <strong>Resolution:</strong> 0.3m (WorldView-3 satellite) - <strong>Format:</strong> Bounding boxes</p>
<p><strong>Purpose:</strong> - Disaster response applications - Overhead imagery analysis - Object detection benchmarking - Small object detection</p>
<p><strong>Access:</strong> - Website: http://xviewdataset.org/ - Papers With Code - Challenge competitions</p>
</section>
<section id="dota-dataset-for-object-detection-in-aerial-images" class="level4">
<h4 class="anchored" data-anchor-id="dota-dataset-for-object-detection-in-aerial-images">DOTA (Dataset for Object Detection in Aerial Images)</h4>
<p><strong>Specifications:</strong> - <strong>Instances:</strong> 1,793,658 annotated objects - <strong>Categories:</strong> 18 object types - <strong>Images:</strong> 11,268 - <strong>Annotation:</strong> Oriented bounding boxes (OBB) - <strong>Sources:</strong> Google Earth, GF-2 Satellite, aerial platforms</p>
<p><strong>Key Feature:</strong> - <strong>Oriented annotations:</strong> Captures object rotation (important for buildings, ships, aircraft) - Various object orientations and aspect ratios - Multiple sensors and resolutions</p>
<p><strong>Access:</strong> - Website: https://captain-whu.github.io/DOTA/ - Papers With Code - GitHub repositories</p>
</section>
</section>
<section id="semantic-segmentation-datasets" class="level3">
<h3 class="anchored" data-anchor-id="semantic-segmentation-datasets">Semantic Segmentation Datasets</h3>
<section id="openearthmap" class="level4">
<h4 class="anchored" data-anchor-id="openearthmap">OpenEarthMap</h4>
<p><strong>Specifications:</strong> - Global high-resolution land cover mapping benchmark - Multiple continents represented - Semantic segmentation annotations - High-resolution imagery</p>
<p><strong>Purpose:</strong> - Global mapping challenges - Multi-region training and generalization testing - Standardized semantic segmentation evaluation</p>
</section>
<section id="spacenet" class="level4">
<h4 class="anchored" data-anchor-id="spacenet">SpaceNet</h4>
<p><strong>Overview:</strong> Foundation dataset for building footprints and road networks</p>
<p><strong>Versions:</strong> - SpaceNet 1-7: Multiple cities, different tasks - Building footprint extraction - Road network mapping - Flood impact assessment (SpaceNet 8) - Open competition with benchmark results</p>
<p><strong>Applications:</strong> - Building extraction algorithms - Road network detection - Multi-sensor fusion (optical + SAR for SpaceNet 6)</p>
</section>
</section>
<section id="scene-classification-datasets" class="level3">
<h3 class="anchored" data-anchor-id="scene-classification-datasets">Scene Classification Datasets</h3>
<section id="aid-aerial-image-dataset" class="level4">
<h4 class="anchored" data-anchor-id="aid-aerial-image-dataset">AID (Aerial Image Dataset)</h4>
<p><strong>Specifications:</strong> - <strong>Images:</strong> 10,000 - <strong>Categories:</strong> 30 scene categories - <strong>Size:</strong> 600×600 pixels - <strong>Resolution:</strong> 0.5-8m spatial resolution - <strong>Source:</strong> Google Earth imagery</p>
<p><strong>Purpose:</strong> - Scene classification benchmarking - Transfer learning evaluation - Feature extraction research</p>
</section>
<section id="nwpu-resisc45" class="level4">
<h4 class="anchored" data-anchor-id="nwpu-resisc45">NWPU-RESISC45</h4>
<p><strong>Specifications:</strong> - <strong>Categories:</strong> 45 scene types - <strong>Images:</strong> 31,500 (700 per class) - <strong>Size:</strong> 256×256 pixels - <strong>Source:</strong> High-resolution aerial images</p>
<p><strong>Applications:</strong> - Scene recognition - Transfer learning source - Benchmark comparisons</p>
</section>
</section>
<section id="time-series-datasets" class="level3">
<h3 class="anchored" data-anchor-id="time-series-datasets">Time Series Datasets</h3>
<section id="tiselac-time-series-land-cover" class="level4">
<h4 class="anchored" data-anchor-id="tiselac-time-series-land-cover">TiSeLaC (Time Series Land Cover)</h4>
<p><strong>Purpose:</strong> - Multi-temporal classification - Phenology analysis - Temporal pattern learning</p>
<p><strong>Applications:</strong> - Crop type mapping from time series - Vegetation dynamics - Seasonal change detection</p>
</section>
<section id="satellite-image-time-series-sits-datasets" class="level4">
<h4 class="anchored" data-anchor-id="satellite-image-time-series-sits-datasets">Satellite Image Time Series (SITS) Datasets</h4>
<p><strong>Various Sources:</strong> - MODIS time series (daily, 250m-1km) - Sentinel-2 time series (5-day, 10-20m) - Landsat time series (16-day, 30m)</p>
<p><strong>Applications:</strong> - LSTM and temporal attention training - Phenology extraction - Land cover trajectory analysis</p>
</section>
</section>
<section id="philippine-specific-data-resources" class="level3">
<h3 class="anchored" data-anchor-id="philippine-specific-data-resources">Philippine-Specific Data Resources</h3>
<p><strong>Available Operational Data:</strong></p>
<p><strong>PRiSM Products:</strong> - Rice area maps (per season: wet and dry) - Seasonality information (planting dates, growth stages) - Yield estimates - Historical archive since 2014 - Website: https://prism.philrice.gov.ph/</p>
<p><strong>PhilSA Products:</strong> - Flood extent maps from DATOS system - Mangrove extent maps (PhilSA-DENR collaboration) - Land cover maps - Disaster damage assessment outputs - Website: https://philsa.gov.ph/</p>
<p><strong>DOST-ASTI:</strong> - DATOS disaster response maps - Hazard maps (flood, landslide susceptibility) - AI-powered rapid assessments - Website: https://hazardhunter.georisk.gov.ph/map</p>
<p><strong>NAMRIA Geoportal:</strong> - Topographic maps - Land cover basemaps - Administrative boundaries - Digital Elevation Models</p>
<p><strong>Importance of Benchmark Datasets:</strong> 1. <strong>Standardized Evaluation:</strong> Compare algorithms objectively 2. <strong>Training Resources:</strong> Pre-labeled data for model training 3. <strong>Transfer Learning:</strong> Pre-train on large datasets, fine-tune for specific applications 4. <strong>Research Reproducibility:</strong> Enable comparison across studies 5. <strong>Community Building:</strong> Shared resources accelerate progress</p>
<hr>
</section>
</section>
<section id="part-6-data-centric-ai-in-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-6-data-centric-ai-in-earth-observation">Part 6: Data-Centric AI in Earth Observation</h2>
<section id="the-paradigm-shift-2025" class="level3">
<h3 class="anchored" data-anchor-id="the-paradigm-shift-2025">The Paradigm Shift (2025)</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Data &gt; Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Old paradigm (Model-Centric AI):</strong> - Focus on developing better algorithms - Keep data fixed, iterate on model architecture - “Our new model achieves 92% accuracy!” - Endless hyperparameter tuning</p>
<p><strong>New paradigm (Data-Centric AI):</strong> - Focus on improving data quality and curation - Keep model fixed (use proven architectures), iterate on data - “Better data improved our model from 85% to 95% accuracy!” - Systematic data improvement</p>
<p><strong>Van der Schaar Lab’s DC-Check Framework:</strong> Argues that reliable ML hinges on characterizing, evaluating, and monitoring training data across the pipeline - not just model complexity.</p>
</div>
</div>
<p><strong>Why the shift?</strong></p>
<ol type="1">
<li><strong>Model architectures have matured:</strong> ResNet, U-Net, LSTM, Transformers are well-established and publicly available</li>
<li><strong>Biggest gains come from data:</strong> Research shows most underperforming models suffer from data issues, not algorithm deficiencies</li>
<li><strong>Real-world deployment:</strong> Data quality determines operational success and trustworthiness</li>
<li><strong>Diminishing returns:</strong> Incremental model improvements yield smaller gains than data improvements</li>
<li><strong>Foundation models:</strong> Pre-trained models (Prithvi, SatViT) reduce need for architecture innovation</li>
</ol>
<p><strong>Data-Centric Principles:</strong></p>
<p>From van der Schaar Lab’s DC-Check framework: - <strong>Characterizing:</strong> Understand training data distribution, coverage, biases - <strong>Evaluating:</strong> Assess data quality, label accuracy, representation - <strong>Monitoring:</strong> Track data drift, performance on subgroups, uncertainty - <strong>Stratification:</strong> Easy/Ambiguous/Hard samples require different treatment - <strong>Data-SUITE:</strong> Suitability, Usefulness, Insufficiency, Thoroughness, Expressiveness checks</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph ModelCentric["MODEL-CENTRIC AI (Old Paradigm)"]
        MC1[Fixed Data] --&gt; MC2[Iterate Models]
        MC2 --&gt; MC3[Tune Hyperparameters]
        MC3 --&gt; MC4[Try New Architectures]
        MC4 --&gt; MC5[85% → 87% → 88%&lt;br/&gt;Diminishing Returns]
    end

    subgraph DataCentric["DATA-CENTRIC AI (2025 Paradigm)"]
        DC1[Proven Architecture&lt;br/&gt;ResNet, U-Net, ViT] --&gt; DC2[Improve Data Quality]
        DC2 --&gt; DC3[Increase Data Quantity]
        DC3 --&gt; DC4[Enhance Data Diversity]
        DC4 --&gt; DC5[Refine Annotations]
        DC5 --&gt; DC6[85% → 92% → 95%&lt;br/&gt;Significant Gains]
    end

    subgraph Pillars["FOUR PILLARS OF DATA-CENTRIC AI"]
        P1[1. Quality&lt;br/&gt;Accurate, consistent&lt;br/&gt;Cloud-free&lt;br/&gt;Atmospherically corrected]
        P2[2. Quantity&lt;br/&gt;Sufficient samples&lt;br/&gt;Per class balance&lt;br/&gt;Training data scale]
        P3[3. Diversity&lt;br/&gt;Geographic coverage&lt;br/&gt;Temporal variation&lt;br/&gt;Seasonal representation]
        P4[4. Annotation&lt;br/&gt;Label accuracy&lt;br/&gt;Boundary precision&lt;br/&gt;Class consistency]
    end

    DC2 --&gt; P1
    DC3 --&gt; P2
    DC4 --&gt; P3
    DC5 --&gt; P4

    subgraph DCCheck["DC-CHECK FRAMEWORK"]
        DCC1[Characterize&lt;br/&gt;Data distribution&lt;br/&gt;Coverage, biases]
        DCC2[Evaluate&lt;br/&gt;Label quality&lt;br/&gt;Representation]
        DCC3[Monitor&lt;br/&gt;Data drift&lt;br/&gt;Performance tracking]
    end

    P1 --&gt; DCC1
    P2 --&gt; DCC1
    P3 --&gt; DCC2
    P4 --&gt; DCC2
    DCC1 --&gt; DCC3
    DCC2 --&gt; DCC3

    DCC3 --&gt; Result[Robust,&lt;br/&gt;Operational&lt;br/&gt;Models]

    style ModelCentric fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style DataCentric fill:#e6ffe6,stroke:#00aa44,stroke-width:3px
    style Pillars fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style DCCheck fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style Result fill:#ccffcc,stroke:#00aa44,stroke-width:3px,color:#000
</pre>
</div>
<p></p><figcaption> Data-Centric AI Framework for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="pillar-1-data-quality" class="level3">
<h3 class="anchored" data-anchor-id="pillar-1-data-quality">Pillar 1: Data Quality</h3>
<p><strong>High-quality data is accurate, consistent, and properly processed</strong></p>
<p><strong>For satellite imagery:</strong></p>
<p><strong>Quality issues to address:</strong></p>
<ul>
<li><strong>Cloud contamination:</strong> Use Level-2A with SCL cloud masks, aggressive filtering</li>
<li><strong>Atmospheric effects:</strong> Always use atmospherically corrected data (surface reflectance, not TOA)</li>
<li><strong>Sensor artifacts:</strong> Check for striping, banding, saturation, dead pixels</li>
<li><strong>Geometric accuracy:</strong> Ensure sub-pixel registration across time and sensors</li>
<li><strong>Radiometric consistency:</strong> Calibrate across sensors and acquisition times</li>
<li><strong>Temporal alignment:</strong> Match acquisition dates to ground conditions (phenology, seasonal changes)</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Challenge: Cloud Cover</strong></p>
<p>Philippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon season).</p>
<p><strong>Data quality solutions:</strong> - <strong>Multi-temporal compositing:</strong> Median over 3-6 months to reduce cloud impact - <strong>Multi-sensor fusion:</strong> Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds - <strong>Aggressive cloud masking:</strong> Accept fewer images for higher quality (quality &gt; quantity) - <strong>Leverage dry season:</strong> December-May for optical data acquisition - <strong>Deep learning reconstruction:</strong> Prithvi-EO-2.0 demonstrated cloud gap reconstruction - <strong>Temporal interpolation:</strong> Fill gaps using adjacent clear observations</p>
<p><strong>DATOS System Approach:</strong> - Prioritize Sentinel-1 SAR during typhoon season (cloud-independent) - Rapid processing (10-20 minutes) for disaster response - Multi-temporal composites for flood extent mapping - Integration with pre-event optical data for context</p>
</div>
<p><strong>For training labels:</strong></p>
<p><strong>Quality issues:</strong></p>
<ul>
<li><strong>Positional error:</strong> GPS drift (±5-10m common), georeferencing mismatch</li>
<li><strong>Temporal mismatch:</strong> 2018 labels with 2020 imagery (land cover changes)</li>
<li><strong>Class ambiguity:</strong> Unclear definitions (shrub vs.&nbsp;sparse forest? informal settlement vs.&nbsp;slum?)</li>
<li><strong>Mixed pixels:</strong> Polygon boundaries include multiple classes (especially at coarse resolutions)</li>
<li><strong>Labeling inconsistency:</strong> Different interpreters apply different criteria</li>
<li><strong>Edge effects:</strong> Boundaries between classes often have high uncertainty</li>
<li><strong>Scale mismatch:</strong> Labels created at different resolution than imagery</li>
</ul>
<p><strong>Best practices:</strong></p>
<ol type="1">
<li><strong>Clear class definitions:</strong> Document what each class includes/excludes with examples</li>
<li><strong>Consistent methodology:</strong> Same interpreter(s), same time of year, same reference imagery</li>
<li><strong>Quality control:</strong> Multiple reviewers, consensus protocols, inter-annotator agreement metrics</li>
<li><strong>Temporal alignment:</strong> Labels contemporary with imagery (within months for dynamic classes)</li>
<li><strong>Positional accuracy:</strong> Use high-resolution reference imagery (VHR, Google Earth)</li>
<li><strong>Buffer boundaries:</strong> Consider excluding mixed pixels at class boundaries from training</li>
<li><strong>Metadata:</strong> Record labeling conditions, interpreter, date, confidence level</li>
<li><strong>Iterative refinement:</strong> Use model predictions to identify and correct label errors</li>
</ol>
<p><strong>Training Data Errors Impact:</strong></p>
<p>Research shows training data errors cause substantial errors in final predictions. Example scenarios: - Mislabeled rice paddies → Model confuses rice with other crops - Temporal mismatch → Model learns outdated patterns - Positional errors → Model learns from wrong pixels - Inconsistent labels → Model learns noise rather than signal</p>
</section>
<section id="pillar-2-data-quantity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-2-data-quantity">Pillar 2: Data Quantity</h3>
<p><strong>More data (usually) improves performance, but quality matters more!</strong></p>
<p><strong>How much data do you need?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 37%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Typical Requirements</th>
<th>With Transfer Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Forest</td>
<td>100s - 1000s samples per class</td>
<td>Same</td>
</tr>
<tr class="even">
<td>SVM</td>
<td>100s - 1000s samples</td>
<td>Same</td>
</tr>
<tr class="odd">
<td>Simple CNN</td>
<td>1000s - 10,000s samples</td>
<td>100s - 1000s</td>
</tr>
<tr class="even">
<td>Deep CNN (ResNet, U-Net)</td>
<td>10,000s - 100,000s samples</td>
<td>1000s - 10,000s</td>
</tr>
<tr class="odd">
<td>Vision Transformer</td>
<td>100,000s - millions</td>
<td>10,000s - 100,000s</td>
</tr>
<tr class="even">
<td>Foundation Models (pre-training)</td>
<td>Millions - billions</td>
<td>N/A (already pre-trained)</td>
</tr>
<tr class="odd">
<td>Foundation Models (fine-tuning)</td>
<td>N/A</td>
<td>100s - 1000s</td>
</tr>
</tbody>
</table>
<p><strong>Strategies when labeled data is limited:</strong></p>
<p><strong>1. Data Augmentation</strong> - <strong>Geometric:</strong> Rotation, flipping, cropping, scaling, translation - <strong>Photometric:</strong> Brightness, contrast, saturation adjustments - <strong>Noise addition:</strong> Gaussian noise, salt-and-pepper - <strong>Spectral:</strong> Band dropout, mixup between spectral signatures - <strong>Caution:</strong> Ensure augmentations are realistic for EO (e.g., don’t flip images with clear up/down orientation)</p>
<p><strong>2. Transfer Learning</strong> - Use model pre-trained on large dataset (ImageNet, SatMAE, Prithvi) - Fine-tune on your small dataset - Leverages learned features from similar tasks - <strong>Reduces data requirements by 10-100×</strong> - Philippine poverty mapping example: 14.1% improvement using transfer learning</p>
<p><strong>3. Active Learning</strong> - <strong>Process:</strong> Iteratively train model → find uncertain predictions → label those → retrain - Efficiently focuses labeling effort where it matters most - Research shows 27% improvement in mIoU with only 2% labeled data - Prioritize samples near decision boundaries</p>
<p><strong>4. Few-Shot Learning</strong> - <strong>Methods:</strong> Metric learning, meta-learning, prototypical networks - Learn from very few examples per class - Gerry Roxas Foundation deforestation: 43% accuracy with only 8% training data - Useful for rare classes or novel geographic regions</p>
<p><strong>5. Weak Supervision</strong> - Leverage noisy or incomplete labels - <strong>WeakAL framework:</strong> Combines active learning and weak supervision - Computes &gt;90% of labels automatically while maintaining competitive performance - Trade-off: Lower individual label quality, but much larger quantity</p>
<p><strong>6. Synthetic Data</strong> - Generate training data via simulation or GANs - Example: Simulated SAR scenes for flood detection - Useful when real data is dangerous/expensive to collect - Caution: Domain gap between synthetic and real data</p>
<p><strong>7. Self-Supervised Pre-training</strong> - Pre-train on unlabeled data (masked autoencoding, contrastive learning) - Fine-tune on small labeled dataset - Foundation models (Prithvi) exemplify this approach - <strong>SSL4EO-S12:</strong> Large-scale dataset for self-supervised learning</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>2024 Research: Data Efficiency
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Findings from “Data-Centric Machine Learning for Earth Observation” (arXiv 2024):</strong></p>
<ul>
<li>Some EO tasks reach optimal accuracy with <strong>&lt;20% of temporal instances</strong></li>
<li><strong>Single band</strong> from single sensor can be sufficient for specific tasks</li>
<li><strong>Implication:</strong> Smart data selection &gt; brute force data collection</li>
<li>Feature selection and dimensionality reduction crucial</li>
<li>Use PCA, tree-based feature importance, or domain knowledge to identify essential features</li>
</ul>
<p><strong>Takeaway:</strong> Focus on acquiring diverse, high-quality samples rather than maximizing quantity indiscriminately.</p>
</div>
</div>
<div class="philippine-context">
<p><strong>Philippine Solution: ALaM Project (DOST-ASTI)</strong></p>
<p><strong>Automated Labeling Machine (ALaM)</strong> addresses annotation bottleneck:</p>
<p><strong>Approach:</strong> - <strong>Automated labeling:</strong> ML models generate initial labels - <strong>Crowdsourcing:</strong> Distributed verification and correction - <strong>Human-in-the-loop quality control:</strong> Expert review of uncertain labels - <strong>Active learning integration:</strong> Prioritize samples for human review</p>
<p><strong>Benefits:</strong> - Significantly reduces labeling time and cost - Scales to national coverage - Integration with DIMER model repository for continuous improvement - Democratizes access to labeled training data</p>
<p><strong>Integration with SkAI-Pinas:</strong> - Part of national AI framework - Addresses gap between abundant remote sensing data and sustainable AI pipelines - Supports operational systems like DATOS and PRiSM</p>
</div>
</section>
<section id="pillar-3-data-diversity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-3-data-diversity">Pillar 3: Data Diversity</h3>
<p><strong>Representative data covers the full range of scenarios the model will encounter</strong></p>
<p>Models trained on narrow data distributions fail when deployed in diverse real-world conditions. Diversity ensures robustness and generalization.</p>
<p><strong>Dimensions of diversity:</strong></p>
<p><strong>1. Geographic diversity</strong> - Different regions (Luzon, Visayas, Mindanao) - Different ecosystems (lowland rainforest, montane cloud forest, mangrove, coral reef) - Different climate zones (Type I-IV Philippine climate classification) - Urban, peri-urban, rural contexts - Different topography (flat, hilly, mountainous)</p>
<p><strong>2. Temporal diversity</strong> - Different seasons (wet season: June-Nov, dry season: Dec-May) - Different years (inter-annual variability, El Niño vs.&nbsp;La Niña) - Different phenological stages (rice: planting, vegetative, reproductive, maturity) - Different times of day (for SAR: morning vs.&nbsp;evening passes) - Historical baselines and recent conditions</p>
<p><strong>3. Class diversity</strong> - Multiple examples per class capturing intra-class variability - Edge cases and rare types (e.g., burned forest, flooded agriculture) - Transitional zones (forest-agriculture boundary, urban-rural fringe) - Different sub-types (e.g., rice varieties, mangrove species, building materials)</p>
<p><strong>4. Sensor diversity</strong> - Different satellites (Sentinel-2A, 2B, 2C) - Different atmospheric conditions (clear, hazy, dusty) - Different viewing angles (SAR: ascending vs.&nbsp;descending) - Different processing baselines (if applicable) - Multi-sensor when relevant (optical + SAR)</p>
<p><strong>5. Socioeconomic diversity</strong> - Different development contexts (high-density urban, informal settlements, rural villages) - Different agricultural practices (mechanized, traditional, mixed) - Different infrastructure quality (paved roads, dirt tracks)</p>
<p><strong>Example: Urban classification</strong></p>
<p><strong>Poor diversity:</strong> All training samples from Metro Manila CBD (Central Business District)</p>
<p><strong>Result:</strong> Model fails on: - Small provincial towns (different building density, height, materials) - Informal settlements (different patterns, materials, roof types) - Peri-urban areas (mixed land cover, agriculture near buildings) - Historical centers (older building styles)</p>
<p><strong>Good diversity:</strong> Samples from: - <strong>Large cities:</strong> Manila, Cebu, Davao (high-density, modern buildings) - <strong>Medium towns:</strong> Baguio, Iloilo, Cagayan de Oro (mixed density) - <strong>Small municipalities:</strong> Various provinces - <strong>Different building materials:</strong> Concrete, metal roofing, nipa huts, wood - <strong>Different periods:</strong> Capture urban growth and change - <strong>Informal settlements:</strong> Slums, squatter areas - <strong>Peri-urban:</strong> Transition zones</p>
<p><strong>Result:</strong> Model generalizes well across Philippines</p>
<p><strong>Validation of Diversity:</strong></p>
<p>Test model performance on stratified subsets: - Per-region accuracy (does it work in all islands?) - Per-season accuracy (dry vs.&nbsp;wet season) - Per-class accuracy (all classes represented equally well?) - Cross-region generalization (train on Luzon, test on Mindanao)</p>
</section>
<section id="pillar-4-annotation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</h3>
<p><strong>How you label data profoundly impacts model performance</strong></p>
<p>Annotation is often the most expensive and time-consuming part of ML workflow. Strategic annotation maximizes value.</p>
<p><strong>Annotation approaches:</strong></p>
<ol type="1">
<li><strong>Point sampling:</strong> Fast, but limited context, suitable for classification</li>
<li><strong>Polygon delineation:</strong> More information, more time-consuming, required for semantic segmentation</li>
<li><strong>Pixel-level labeling:</strong> Maximum detail, most expensive, essential for precise segmentation</li>
<li><strong>Image-level labels:</strong> Easiest, suitable for scene classification, limited spatial information</li>
<li><strong>Bounding boxes:</strong> For object detection, faster than pixel-level masks</li>
</ol>
<p><strong>Best practices:</strong></p>
<p><strong>1. Expert involvement</strong> - Use domain experts for complex classes (forest types, crop stages, mangrove species) - Train labelers thoroughly on class definitions with examples - Regular calibration sessions to maintain consistency - Document difficult cases and edge cases</p>
<p><strong>2. Quality over quantity</strong> - <strong>500 high-quality labels &gt; 5000 noisy labels</strong> - Invest in review and correction processes - Document difficult cases and ambiguous examples - Use confidence scores to flag uncertain labels</p>
<p><strong>3. Class balance</strong> - Ensure adequate representation of minority classes - Stratified sampling by class (not just random) - Consider class weights in training if imbalanced - Oversampling rare classes or undersampling common classes - <strong>Imbalanced classes:</strong> Major challenge in EO (e.g., rare disasters, rare land cover types)</p>
<p><strong>4. Consensus protocols</strong> - Multiple labelers per sample (especially for ambiguous cases) - Majority vote or adjudication for disagreements - Measure inter-annotator agreement (Cohen’s Kappa, Krippendorff’s Alpha) - Establish minimum agreement threshold (e.g., 80%)</p>
<p><strong>5. Iterative refinement</strong> - Use model predictions to find label errors (disagreement between model and label) - Retrain after improving labels (data-centric iteration) - Focus effort on low-confidence predictions - <strong>Model-in-the-loop labeling:</strong> Model suggests labels, humans verify</p>
<p><strong>6. Annotation tools and platforms</strong> - Use efficient labeling tools (LabelMe, CVAT, Label Studio, Labelbox) - For EO: Tools supporting geospatial formats (GeoTIFF, shapefiles) - Integration with cloud platforms (Google Earth Engine, QGIS) - Export to ML-ready formats</p>
<p><strong>7. Crowdsourcing considerations</strong> - Clear instructions and examples - Quality control through redundancy and expert review - Gamification to maintain engagement - Examples: Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping</p>
<p><strong>EO-Specific Annotation Challenges:</strong></p>
<p>From Kili Technology’s Earth Observation Data Labeling Guide: - <strong>Sensor diversity:</strong> Different spectral bands, resolutions, formats - <strong>Massive data volumes:</strong> Petabyte-scale archives (“four Vs”: Volume, Velocity, Variety, Veracity) - <strong>Domain expertise requirements:</strong> Complex classes require specialized knowledge - <strong>Weak labeling approaches:</strong> Leverage noisy labels, distant supervision - <strong>Active learning integration:</strong> Prioritize informative samples - <strong>Stakeholder-friendly tooling:</strong> Tools accessible to non-ML experts</p>
<div class="philippine-context">
<p><strong>Philippine Annotation Ecosystem:</strong></p>
<p><strong>ALaM (Automated Labeling Machine - DOST-ASTI):</strong> - Combines automated labeling with crowdsourcing - Human-in-the-loop quality control - Integration with DIMER model repository - Reduces labeling time and cost significantly - <strong>Workflow:</strong> Automated labels → Crowdsourced verification → Expert review → Training data</p>
<p><strong>DATOS (DOST-ASTI):</strong> - Rapid disaster mapping (10-20 minute response) - On-the-fly labeling during disaster response - Iterative refinement based on ground validation - Integration with LGU feedback</p>
<p><strong>Academic Partnerships:</strong> - University of the Philippines - remote sensing courses with labeling components - PhilRice - rice field delineation and crop stage labeling - DENR - forest and mangrove mapping with expert foresters</p>
<p><strong>International Support:</strong> - Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping - CoPhil training programs on labeling best practices - European Copernicus expertise transfer</p>
</div>
</section>
<section id="examples-data-centric-success-stories" class="level3">
<h3 class="anchored" data-anchor-id="examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</h3>
<section id="nasa-ibm-geospatial-foundation-model-prithvi" class="level4">
<h4 class="anchored" data-anchor-id="nasa-ibm-geospatial-foundation-model-prithvi">NASA-IBM Geospatial Foundation Model (Prithvi)</h4>
<p><strong>Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)</strong></p>
<p><strong>Data-centric approach:</strong> - <strong>Scale:</strong> Millions of satellite images from HLS (30m resolution, global coverage) - <strong>Self-supervised pre-training:</strong> Masked autoencoding (no labels needed) - <strong>Data quality:</strong> HLS provides analysis-ready data (atmospheric correction, BRDF normalization, co-registration) - <strong>Fine-tuned for specific tasks:</strong> With small labeled datasets (100s-1000s samples)</p>
<p><strong>Result:</strong> - State-of-the-art performance on multiple EO tasks (flood mapping, burn scar detection, crop segmentation) - <strong>Reduces labeled data requirements by 10-100×</strong> - Democratizes access to powerful EO AI - Foundation for operational systems worldwide</p>
<p><strong>Key Insight:</strong> Investment in massive, high-quality pre-training data enables downstream applications with minimal task-specific labels.</p>
</section>
<section id="esa-φsat-2-on-board-ai-launched-2024" class="level4">
<h4 class="anchored" data-anchor-id="esa-φsat-2-on-board-ai-launched-2024">ESA Φsat-2 On-Board AI (Launched 2024)</h4>
<p><strong>22cm CubeSat with on-board AI processing</strong></p>
<p><strong>Data-centric innovation:</strong> - Processes imagery directly on satellite - <strong>Data quality selection happens in space!</strong> - Only transmits actionable information (not raw data) - Cloud filtering: Only clear, usable images sent to Earth - Reduces bandwidth requirements by orders of magnitude - Enables real-time event detection (fires, ships, clouds)</p>
<p><strong>Rationale:</strong> With 1,052 active EO satellites generating thousands of terabytes daily, traditional radio frequency communication cannot relay this volume. On-board AI filters data at source.</p>
<p><strong>Implication:</strong> Data quality and relevance prioritized over quantity. Shift from “collect everything” to “collect intelligently.”</p>
</section>
<section id="earthdaily-constellation" class="level4">
<h4 class="anchored" data-anchor-id="earthdaily-constellation">EarthDaily Constellation</h4>
<p><strong>10-satellite constellation for daily global coverage at 5-10m resolution</strong></p>
<p><strong>Focus on AI-ready data:</strong> - <strong>Scientific-grade calibration:</strong> Rigorous radiometric accuracy - <strong>Consistent, reliable acquisitions:</strong> Predictable revisit times - <strong>Optimized spectral bands for ML:</strong> Bands selected based on ML feature importance - <strong>Emphasis on data quality for algorithm performance:</strong> Analysis-ready data products</p>
<p><strong>Philosophy:</strong> Data quality and consistency are first-class design criteria, not afterthoughts. Build satellites around AI needs.</p>
</section>
<section id="weakal-framework-active-learning-weak-supervision" class="level4">
<h4 class="anchored" data-anchor-id="weakal-framework-active-learning-weak-supervision">WeakAL Framework (Active Learning + Weak Supervision)</h4>
<p><strong>Research from remote sensing ML community</strong></p>
<p><strong>Approach:</strong> - Combines active learning (select informative samples) with weak supervision (leverage noisy labels) - Computes <strong>&gt;90% of labels automatically</strong> while maintaining competitive performance - Human effort focused on most uncertain/informative samples</p>
<p><strong>Results:</strong> - 27% improvement in mIoU with only 2% manually labeled data - Demonstrates data-efficient learning - Practical for large-scale operational mapping</p>
<p><strong>Key Insight:</strong> Strategic data selection and semi-automated labeling can achieve strong performance with minimal human effort.</p>
<hr>
</section>
</section>
</section>
<section id="part-7-explainable-ai-xai-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-7-explainable-ai-xai-for-earth-observation">Part 7: Explainable AI (XAI) for Earth Observation</h2>
<section id="why-xai-matters-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="why-xai-matters-in-eo">Why XAI Matters in EO</h3>
<p><strong>The Problem:</strong></p>
<p>Deep learning models are often <strong>“black boxes”</strong> - they produce accurate predictions, but we don’t understand why. For operational EO systems, this creates challenges:</p>
<ol type="1">
<li><strong>Scientific Insights:</strong> Can’t extract physical understanding from model decisions</li>
<li><strong>Bias Detection:</strong> Can’t identify if model relies on spurious correlations (e.g., cloud shadows, artifacts)</li>
<li><strong>Trust and Adoption:</strong> Stakeholders reluctant to use models they don’t understand</li>
<li><strong>Debugging:</strong> Difficult to diagnose errors and improve models</li>
<li><strong>Regulatory/Policy:</strong> Some applications require explainability (e.g., disaster fund allocation)</li>
</ol>
<p><strong>Recent Efforts (2023-2025):</strong></p>
<p>Despite significant advances in deep learning for remote sensing, <strong>lack of explainability remains a major criticism</strong>. The community is increasingly exploring Explainable AI techniques:</p>
<ul>
<li>Increasingly intensive exploration of XAI methods for EO</li>
<li>Integration of attention visualization in transformer architectures</li>
<li>Saliency maps and feature attribution techniques</li>
<li>Trade-off studies: accuracy vs.&nbsp;interpretability</li>
</ul>
</section>
<section id="xai-methods-for-eo" class="level3">
<h3 class="anchored" data-anchor-id="xai-methods-for-eo">XAI Methods for EO</h3>
<p><strong>Gradient-Based Methods:</strong></p>
<p><strong>Grad-CAM (Gradient-weighted Class Activation Mapping):</strong> - <strong>Process:</strong> Compute gradients of target class with respect to final convolutional layer - <strong>Output:</strong> Heatmap highlighting regions important for prediction - <strong>Advantages:</strong> Most interpretable method, computationally efficient, works with any CNN - <strong>Applications:</strong> Visualize which parts of satellite image model focuses on (e.g., “model detects water by focusing on blue spectral signature”)</p>
<p><strong>Guided Backpropagation:</strong> - Visualizes pixels contributing to prediction - Sharper visualizations than Grad-CAM - Highlights fine-grained features</p>
<p><strong>Integrated Gradients:</strong> - Accumulates gradients along path from baseline to input - More robust attributions than simple gradients - Satisfies desirable axioms (sensitivity, implementation invariance)</p>
<p><strong>Perturbation-Based Methods:</strong></p>
<p><strong>Occlusion:</strong> - <strong>Process:</strong> Block image regions and observe prediction change - <strong>Output:</strong> Sensitivity map showing which regions are critical - <strong>Advantages:</strong> High interpretability, intuitive - <strong>Disadvantages:</strong> Computationally expensive (must test many occlusions)</p>
<p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> - <strong>Process:</strong> Train simple, interpretable model (e.g., linear) to approximate complex model locally - <strong>Output:</strong> Feature importances for specific prediction - <strong>Advantages:</strong> Model-agnostic, interpretable - <strong>Disadvantages:</strong> Expensive computation, local rather than global explanation</p>
<p><strong>Model-Based Methods:</strong></p>
<p><strong>SHAP (SHapley Additive exPlanations):</strong> - <strong>Process:</strong> Game theory approach - compute contribution of each feature - <strong>Output:</strong> Feature importance values for prediction - <strong>Advantages:</strong> Theoretically grounded, consistent - <strong>Applications:</strong> Explain which spectral bands, indices, or temporal features drive predictions</p>
<p><strong>Attention Visualization (for Transformers):</strong> - <strong>Process:</strong> Visualize attention weights from self-attention mechanism - <strong>Output:</strong> Heatmap showing which patches/regions model attends to - <strong>Advantages:</strong> Built into architecture, interpretable - <strong>Applications:</strong> Vision Transformers (ViT), UNetFormer - see which spatial regions model focuses on</p>
<p><strong>Feature Importance (for Tree-Based Models):</strong> - Random Forest, XGBoost provide feature importance scores - <strong>Output:</strong> Ranking of features by contribution to predictions - <strong>Advantages:</strong> Simple, intuitive, built-in - <strong>Applications:</strong> Understand which spectral bands, indices, temporal features are most informative</p>
</section>
<section id="applications-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-eo">Applications in EO</h3>
<p><strong>1. Understanding Model Decisions:</strong> - Visualize which spectral bands contribute most (e.g., does model rely on SWIR for burn detection?) - Identify spatial patterns model focuses on (e.g., texture vs.&nbsp;spectral signature) - Discover unexpected correlations (e.g., model using cloud shadows instead of actual land cover)</p>
<p><strong>2. Discovering Scientific Insights:</strong> - Identify which vegetation indices are most predictive for crop types - Understand temporal patterns in multi-date imagery (which dates are critical for classification?) - Extract biophysical relationships learned by model</p>
<p><strong>3. Detecting and Mitigating Biases:</strong> - Identify if model relies on artifacts (e.g., sensor striping, JPEG compression) - Detect geographic biases (model works in training region, fails elsewhere due to spurious features) - Ensure model uses physically meaningful features</p>
<p><strong>4. Building Trust with Stakeholders:</strong> - Demonstrate to policymakers that model decisions are reasonable - Show LGUs which features drive disaster risk predictions - Explain to farmers why certain fields are flagged for attention</p>
<p><strong>5. Debugging and Improving Models:</strong> - Identify when model makes errors (e.g., confuses rice with water due to flooding) - Guide data collection (which features need more training samples?) - Inform feature engineering (which derived features would help?)</p>
</section>
<section id="challenges-and-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-trade-offs">Challenges and Trade-Offs</h3>
<p><strong>Accuracy vs.&nbsp;Interpretability:</strong> - Simple models (decision trees, linear regression) are interpretable but less accurate - Complex models (deep CNNs, transformers) are more accurate but less interpretable - <strong>Trade-off:</strong> Choose based on application criticality and stakeholder needs</p>
<p><strong>Computational Cost:</strong> - Post-hoc explanation methods (LIME, occlusion) can be expensive - Gradient-based methods (Grad-CAM) are fast - Consider explanation cost for operational systems</p>
<p><strong>Faithfulness:</strong> - Do explanations truly reflect model’s reasoning, or are they misleading? - Saliency maps can be noisy or highlight irrelevant features - Validation: Compare explanations against domain knowledge</p>
<p><strong>Global vs.&nbsp;Local:</strong> - Local explanations (single prediction) may not generalize - Global explanations (entire model behavior) are harder to compute and interpret - Need both perspectives for complete understanding</p>
</section>
<section id="best-practices-for-xai-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-xai-in-eo">Best Practices for XAI in EO</h3>
<ol type="1">
<li><strong>Use Multiple Methods:</strong> Different XAI methods can reveal complementary insights</li>
<li><strong>Validate Explanations:</strong> Check against domain knowledge, physical understanding</li>
<li><strong>Integrate into Workflow:</strong> Make XAI routine part of model development, not afterthought</li>
<li><strong>Communicate Effectively:</strong> Visualize explanations clearly for stakeholders (heatmaps, feature importance plots)</li>
<li><strong>Document Limitations:</strong> Be transparent about what explanations can and cannot tell us</li>
<li><strong>Balance Complexity:</strong> For operational systems, consider interpretable models when accuracy difference is small</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>XAI Resources for EO
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Tools:</strong> - <strong>Captum (PyTorch):</strong> Library for model interpretability (Grad-CAM, Integrated Gradients, SHAP) - <strong>SHAP Library:</strong> SHapley Additive exPlanations for Python - <strong>Grad-CAM Implementations:</strong> Available for TensorFlow/Keras and PyTorch - <strong>Attention Visualization:</strong> Built into transformer implementations (HuggingFace Transformers)</p>
<p><strong>Research:</strong> - “Explainable AI for Earth Observation: A Review” (ongoing research area) - SSL4EO-2024 Summer School included XAI sessions - Growing number of papers combining EO and XAI at IGARSS, ISPRS, ML4Earth conferences</p>
</div>
</div>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Session 2 Summary
</div>
</div>
<div class="callout-body-container callout-body">
<section id="core-concepts" class="level3">
<h3 class="anchored" data-anchor-id="core-concepts">Core Concepts</h3>
<ol type="1">
<li><strong>AI/ML learns patterns from data</strong> rather than explicit programming - enables automated analysis of massive satellite archives</li>
<li><strong>The EO workflow</strong> spans problem definition → data acquisition → preprocessing → features → training → validation → deployment</li>
<li><strong>Supervised learning</strong> (classification &amp; regression) is dominant for EO because we need specific outputs; unsupervised (clustering) useful for exploration</li>
</ol>
</section>
<section id="deep-learning-architectures" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-architectures">Deep Learning Architectures</h3>
<ol start="4" type="1">
<li><strong>CNNs</strong> are foundation of EO image analysis - automatic feature extraction, spatial awareness, hierarchical learning</li>
<li><strong>U-Net</strong> excels at semantic segmentation with encoder-decoder + skip connections (e.g., Benguet deforestation: 99.73% accuracy)</li>
<li><strong>Vision Transformers</strong> capture global context and long-range dependencies via self-attention (SatViT, MS-CLIP for multi-spectral data)</li>
<li><strong>LSTMs/RNNs</strong> model temporal patterns in time series (PRiSM rice monitoring, crop yield prediction: R² &gt; 0.93)</li>
<li><strong>Object Detection</strong> (YOLO, Faster R-CNN) localize objects with bounding boxes (buildings, ships, vehicles)</li>
<li><strong>Foundation Models</strong> (Prithvi-EO-2.0: 600M parameters) enable fine-tuning with 10-100× less labeled data</li>
</ol>
</section>
<section id="advanced-techniques" class="level3">
<h3 class="anchored" data-anchor-id="advanced-techniques">Advanced Techniques</h3>
<ol start="10" type="1">
<li><strong>Multi-modal fusion</strong> combines optical + SAR for all-weather monitoring (critical for Philippine monsoon season)</li>
<li><strong>Transfer learning</strong> dramatically reduces data requirements - pre-train on large dataset, fine-tune on small task-specific dataset</li>
<li><strong>Self-supervised learning</strong> pre-trains on unlabeled data via masked autoencoding (Prithvi) or contrastive learning</li>
</ol>
</section>
<section id="benchmark-datasets" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-datasets">Benchmark Datasets</h3>
<ol start="13" type="1">
<li><strong>EuroSAT</strong> (27,000 images, 10 classes, 98.57% accuracy), <strong>BigEarthNet</strong> (549,488 patches, multi-modal), <strong>xView</strong> (&gt;1M objects, 60 classes)</li>
<li>Benchmarks enable standardized evaluation, provide training resources, support transfer learning</li>
</ol>
</section>
<section id="data-centric-ai-2025-paradigm" class="level3">
<h3 class="anchored" data-anchor-id="data-centric-ai-2025-paradigm">Data-Centric AI (2025 Paradigm)</h3>
<ol start="15" type="1">
<li><strong>Data quality &gt; model complexity:</strong> Improving data from 85% → 95% accuracy beats endless model tuning</li>
<li><strong>Four Pillars:</strong> Quality (accurate, consistent, properly processed), Quantity (sufficient samples, augmentation), Diversity (geographic, temporal, class, sensor), Annotation (strategic, high-quality labeling)</li>
<li><strong>Philippine Solutions:</strong> DOST-ASTI ALaM (Automated Labeling Machine), DIMER model repository, active learning</li>
</ol>
</section>
<section id="explainable-ai" class="level3">
<h3 class="anchored" data-anchor-id="explainable-ai">Explainable AI</h3>
<ol start="18" type="1">
<li><strong>XAI crucial for operational systems:</strong> Builds trust, enables debugging, extracts scientific insights, detects biases</li>
<li><strong>Methods:</strong> Grad-CAM (heatmaps), SHAP (feature importance), Attention visualization (transformers)</li>
</ol>
</section>
<section id="philippine-operational-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-operational-context">Philippine Operational Context</h3>
<ol start="20" type="1">
<li><strong>DATOS (DOST-ASTI):</strong> 10-20 minute AI-powered flood mapping from Sentinel-1 SAR</li>
<li><strong>PRiSM (PhilRice-IRRI):</strong> Operational since 2014, all-weather rice monitoring combining SAR + optical</li>
<li><strong>PhilSA-DENR:</strong> Nationwide mangrove mapping with U-Net (99.73% accuracy)</li>
<li><strong>CoPhil Data Centre (2025):</strong> Local, high-bandwidth access to Sentinel data, cloud-native distribution</li>
<li><strong>Leverage existing infrastructure:</strong> DIMER, AIPI, ALaM, CoPhil to operationalize AI/ML workflows</li>
</ol>
<p><strong>Next steps:</strong> Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!</p>
</section>
</div>
</div>
<hr>
</section>
<section id="discussion-questions" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions">Discussion Questions</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Reflect &amp; Discuss
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>What EO problem in your work</strong> could benefit from ML? Is it classification, regression, segmentation, or object detection? Which architecture would you choose?</p></li>
<li><p><strong>Data quality in Philippine context:</strong> How do you address cloud cover, temporal dynamics, and atmospheric effects in your satellite data?</p></li>
<li><p><strong>Foundation models:</strong> How could Prithvi-EO-2.0 or other pre-trained models reduce barriers for your organization? What Philippine-specific fine-tuning would be needed?</p></li>
<li><p><strong>Multi-modal fusion:</strong> When would you combine Sentinel-2 optical with Sentinel-1 SAR? What are practical challenges?</p></li>
<li><p><strong>Data-centric approach:</strong> What are biggest data quality issues you face? How could ALaM or active learning help?</p></li>
<li><p><strong>Benchmark datasets:</strong> Which international datasets could you use for pre-training? How to ensure models generalize to Philippines?</p></li>
<li><p><strong>Explainable AI:</strong> For your application, why would explainability matter? Which XAI method would you use?</p></li>
<li><p><strong>DIMER and AIPI platforms:</strong> How might these reduce barriers to deploying ML in your organization? What models would you contribute or use?</p></li>
<li><p><strong>Temporal modeling:</strong> For what applications would LSTM or temporal attention be valuable? What data would you need?</p></li>
<li><p><strong>CoPhil opportunities:</strong> How can you leverage the upcoming Data Centre and training programs? What collaborations would be valuable?</p></li>
</ol>
</div>
</div>
<hr>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<section id="foundational-concepts" class="level3">
<h3 class="anchored" data-anchor-id="foundational-concepts">Foundational Concepts</h3>
<ul>
<li><a href="https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science">NASA ARSET: Fundamentals of Machine Learning for Earth Science</a></li>
<li><a href="https://arxiv.org/abs/2312.05327">Data-Centric AI: Better, Not Just More</a></li>
<li><a href="https://www.vanderschaar-lab.com/dc-check/what-is-data-centric-ai/">Van der Schaar Lab: What is Data-Centric AI?</a></li>
</ul>
</section>
<section id="deep-learning-architectures-1" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-architectures-1">Deep Learning Architectures</h3>
<ul>
<li><a href="https://www.deeplearningbook.org/">Deep Learning Book (Goodfellow et al.)</a> - Free online</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Nielsen)</a> - Interactive tutorial</li>
<li><a href="https://github.com/satellite-image-deep-learning/techniques">Satellite Image Deep Learning Techniques</a> - Comprehensive GitHub repository</li>
</ul>
</section>
<section id="deep-learning-for-eo" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-for-eo">Deep Learning for EO</h3>
<ul>
<li><a href="https://www.mdpi.com/2072-4292/12/15/2495">Deep Learning for Land Use and Land Cover Classification</a> - 2020 review</li>
<li><a href="https://www.tandfonline.com/doi/full/10.1080/17538947.2024.2328827">Deep Learning for Remote Sensing Image Segmentation</a> - 2024 review</li>
<li><a href="https://www.mdpi.com/2072-4292/12/10/1667">Object Detection and Image Segmentation with Deep Learning on EO Data</a></li>
</ul>
</section>
<section id="foundation-models" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models">Foundation Models</h3>
<ul>
<li><a href="https://huggingface.co/ibm-nasa-geospatial">IBM-NASA Prithvi Models on Hugging Face</a></li>
<li><a href="https://arxiv.org/abs/2412.02732">Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model</a></li>
<li><a href="https://research.ibm.com/blog/prithvi2-geospatial">IBM Research: Prithvi-EO-2.0 Blog</a></li>
</ul>
</section>
<section id="self-supervised-learning-1" class="level3">
<h3 class="anchored" data-anchor-id="self-supervised-learning-1">Self-Supervised Learning</h3>
<ul>
<li><a href="https://langnico.github.io/posts/SSL4EO-2024-review/">SSL4EO-2024 Summer School Review</a></li>
<li><a href="https://arxiv.org/abs/2405.20462">Multi-Label Guided Soft Contrastive Learning for EO</a></li>
</ul>
</section>
<section id="data-centric-ai" class="level3">
<h3 class="anchored" data-anchor-id="data-centric-ai">Data-Centric AI</h3>
<ul>
<li><a href="https://arxiv.org/html/2408.11384v1">Data-Centric Machine Learning for Earth Observation</a></li>
<li><a href="https://kili-technology.com/data-labeling/earth-observation-data-labeling-guide">Kili Technology: Earth Observation Data Labeling Guide</a></li>
</ul>
</section>
<section id="explainable-ai-1" class="level3">
<h3 class="anchored" data-anchor-id="explainable-ai-1">Explainable AI</h3>
<ul>
<li><a href="https://captum.ai/">Captum: Model Interpretability for PyTorch</a></li>
<li><a href="https://shap.readthedocs.io/">SHAP Library Documentation</a></li>
</ul>
</section>
<section id="eo-specific-ml" class="level3">
<h3 class="anchored" data-anchor-id="eo-specific-ml">EO-Specific ML</h3>
<ul>
<li><a href="https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/">EO College: Introduction to Machine Learning for Earth Observation</a></li>
<li><a href="https://ml4earth.de/">ML4Earth Resources</a></li>
<li><a href="https://www.climatechange.ai/subject_areas/earth_observation_monitoring">Climate Change AI: Earth Observation &amp; Monitoring</a></li>
<li><a href="https://www.mdpi.com/2072-4292/15/16/4112">A Review of Practical AI for Remote Sensing in Earth Sciences</a> - 2023</li>
</ul>
</section>
<section id="benchmark-datasets-1" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-datasets-1">Benchmark Datasets</h3>
<ul>
<li><a href="https://github.com/phelber/EuroSAT">EuroSAT GitHub</a></li>
<li><a href="https://bigearth.net/">BigEarthNet Website</a></li>
<li><a href="http://xviewdataset.org/">xView Dataset</a></li>
<li><a href="https://captain-whu.github.io/DOTA/">DOTA: Dataset for Object Detection in Aerial Images</a></li>
</ul>
</section>
<section id="philippine-ai-initiatives" class="level3">
<h3 class="anchored" data-anchor-id="philippine-ai-initiatives">Philippine AI Initiatives</h3>
<ul>
<li><a href="https://asti.dost.gov.ph/projects/datos">DOST-ASTI: Remote Sensing and Data Science (DATOS) Help Desk</a></li>
<li><a href="https://www.pna.gov.ph/articles/1136226">Philippine News Agency: DOST AI R&amp;D Projects</a> - SkAI-Pinas, DIMER, AIPI</li>
<li><a href="https://prism.philrice.gov.ph/">PRiSM: Philippine Rice Information System</a></li>
<li><a href="https://philsa.gov.ph/">PhilSA: Philippine Space Agency</a></li>
<li><a href="https://copphil.philsa.gov.ph/">CoPhil Centre</a></li>
</ul>
</section>
<section id="recent-advances" class="level3">
<h3 class="anchored" data-anchor-id="recent-advances">Recent Advances</h3>
<ul>
<li><a href="https://arxiv.org/abs/2305.08413">Artificial Intelligence to Advance Earth Observation: A Review</a> - 2023</li>
<li><a href="https://arxiv.org/html/2501.12030v1">Advancing Earth Observation with AI</a> - 2025</li>
<li><a href="https://ai4eo.eu/">ESA AI for Earth Observation</a></li>
<li><a href="https://github.com/acgeospatial/awesome-earthobservation-code">Awesome Earth Observation Code</a></li>
</ul>
<hr>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/DimitrisKasabalis\.github\.io\/cophil-training-v1\.0");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day1/sessions/session1.html" class="pagination-link" aria-label="Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day1/sessions/session3.html" class="pagination-link" aria-label="Session 3: Hands-on Python for Geospatial Data">
        <span class="nav-page-text">Session 3: Hands-on Python for Geospatial Data</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Session 2: Core Concepts of AI/ML for Earth Observation"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Understanding the fundamentals of machine learning for satellite data analysis"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">nav</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb"</span><span class="ot"> aria-label</span><span class="op">=</span><span class="st">"Breadcrumb"</span><span class="dt">&gt;</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../../index.html"</span><span class="dt">&gt;</span>Home<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../index.html"</span><span class="dt">&gt;</span>Day 1<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-current"</span><span class="dt">&gt;</span>Session 2<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">nav</span><span class="dt">&gt;</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>::: {.session-info}</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>**Duration:** 2 hours | **Format:** Lecture + Conceptual Exercises | **Platform:** Presentation</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session Overview</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You'll learn the complete AI/ML workflow, understand different learning paradigms, explore deep learning architectures including CNNs, Vision Transformers, and temporal models, discover benchmark datasets, and understand why data quality matters more than model complexity in 2025's data-centric AI paradigm.</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>The session integrates cutting-edge methodologies with concrete Philippine case studies, from DOST-ASTI's 10-20 minute flood detection using AI to PhilSA's mangrove mapping efforts, demonstrating operational AI/ML systems already serving disaster risk reduction and natural resource management needs.</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>::: {.learning-objectives}</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Objectives</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>By the end of this session, you will be able to:</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define** AI and ML in the context of Earth Observation</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Describe** the complete AI/ML workflow from problem definition to deployment</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Distinguish** between supervised and unsupervised learning with EO examples</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain** classification vs. regression tasks in satellite data analysis</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Identify** major deep learning architectures: CNNs, U-Net, Vision Transformers, RNNs/LSTMs, object detection networks</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understand** key components: neurons, layers, activation functions, loss functions, optimizers</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Compare** different model architectures and when to apply each</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recognize** benchmark datasets used for training and evaluation</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Articulate** the data-centric AI paradigm and its importance for EO</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply** best practices for data quality, quantity, diversity, and annotation</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain** Explainable AI (XAI) and why it matters for operational systems</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## Presentation Slides</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> src</span><span class="op">=</span><span class="st">"../presentations/02_session2_ai_ml_fundamentals.html"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"100%"</span><span class="ot"> height</span><span class="op">=</span><span class="st">"600"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"border: 1px solid #ccc; border-radius: 4px;"</span><span class="dt">&gt;&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 1: What is AI/ML?</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="fu">### Defining the Terms</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>**Artificial Intelligence (AI):**</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Broad field focused on creating intelligent machines</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Systems that can perceive, reason, learn, and act</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Includes everything from rule-based systems to machine learning</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In EO: Enables automated interpretation of petabytes of satellite data</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>**Machine Learning (ML):**</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Subset of AI focused on learning from data</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Algorithms that improve performance through experience</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key distinction:** No explicit programming of rules</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep Learning:** Subset of ML using multi-layered neural networks</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">AI, Machine Learning, and Deep Learning Relationship</span><span class="ot">"</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>graph TB</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>    subgraph AI[<span class="ot">"</span><span class="st">ARTIFICIAL INTELLIGENCE&lt;br/&gt;Creating intelligent machines</span><span class="ot">"</span>]</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>        subgraph ML[<span class="ot">"</span><span class="st">MACHINE LEARNING&lt;br/&gt;Learning from data without explicit programming</span><span class="ot">"</span>]</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>            subgraph DL[<span class="ot">"</span><span class="st">DEEP LEARNING&lt;br/&gt;Multi-layered neural networks</span><span class="ot">"</span>]</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>                DL1[Convolutional&lt;br/&gt;Neural Networks&lt;br/&gt;CNNs]</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>                DL2[Recurrent&lt;br/&gt;Neural Networks&lt;br/&gt;RNNs/LSTMs]</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>                DL3[Transformers&lt;br/&gt;Vision Transformers]</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>                DL4[GANs &amp; VAEs&lt;br/&gt;Generative Models]</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>            end</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>            ML1[Random Forest]</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>            ML2[Support Vector&lt;br/&gt;Machines]</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>            ML3[Decision Trees]</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>            ML4[K-Means&lt;br/&gt;Clustering]</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>        end</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>        AI1[Expert Systems&lt;br/&gt;Rule-based]</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>        AI2[Fuzzy Logic]</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>        AI3[Genetic&lt;br/&gt;Algorithms]</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>    DL1 -.-&gt;|EO Apps| EO1[Land Cover&lt;br/&gt;Classification]</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>    DL1 -.-&gt;|EO Apps| EO2[Semantic&lt;br/&gt;Segmentation]</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>    DL2 -.-&gt;|EO Apps| EO3[Time Series&lt;br/&gt;Crop Monitoring]</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>    DL3 -.-&gt;|EO Apps| EO4[Change&lt;br/&gt;Detection]</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>    style AI fill:<span class="co">#e6f3ff,stroke:#0066cc,stroke-width:3px</span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>    style ML fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>    style DL fill:<span class="co">#e6ffe6,stroke:#00aa44,stroke-width:2px</span></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>    style DL1 fill:<span class="co">#00cc66,stroke:#008844,stroke-width:1px,color:#fff</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>    style DL2 fill:<span class="co">#00cc66,stroke:#008844,stroke-width:1px,color:#fff</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>    style DL3 fill:<span class="co">#00cc66,stroke:#008844,stroke-width:1px,color:#fff</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## The ML Difference</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a>**Traditional Programming:**</span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a><span class="in">Rules + Data → Output</span></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>**Machine Learning:**</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a><span class="in">Data + Desired Output → Rules (Model)</span></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>In EO: Instead of coding "if NIR &gt; 0.6 and Red &lt; 0.3, then forest", ML learns the pattern from labeled examples.</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why ML for Earth Observation?</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a>**Challenges that ML addresses:**</span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Scale:** NASA's Earth Science Data Systems exceeded 148 PB in 2023, projected 250 PB in 2025 - impossible to manually analyze</span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Complexity:** Multispectral, multi-temporal, spatial patterns humans can't easily detect</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Consistency:** Automated processing ensures reproducible results across time and space</span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Speed:** Real-time disaster mapping requires immediate analysis (DOST-ASTI DATOS: 10-20 minute flood response)</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Multi-modal fusion:** Integrating optical, SAR, LiDAR data for robust monitoring</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>**Traditional vs. ML approaches:**</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Task <span class="pp">|</span> Traditional <span class="pp">|</span> ML Approach <span class="pp">|</span></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------|-------------|</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Water detection** <span class="pp">|</span> Manual NDWI threshold <span class="pp">|</span> Learn optimal threshold + texture from examples <span class="pp">|</span></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Land cover** <span class="pp">|</span> Rule-based classification <span class="pp">|</span> Random Forest or CNN with training samples <span class="pp">|</span></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Flood mapping** <span class="pp">|</span> Expert visual interpretation <span class="pp">|</span> U-Net segmentation trained on labeled floods <span class="pp">|</span></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Crop monitoring** <span class="pp">|</span> Fixed vegetation index thresholds <span class="pp">|</span> LSTM time series model learning phenology <span class="pp">|</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Building detection** <span class="pp">|</span> Manual digitization <span class="pp">|</span> YOLO or Faster R-CNN object detection <span class="pp">|</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Deforestation** <span class="pp">|</span> Visual comparison of dates <span class="pp">|</span> Siamese networks for change detection <span class="pp">|</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>**Philippine Operational Systems:**</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>The Philippines demonstrates successful AI/ML deployment:</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DATOS (DOST-ASTI):** AI-powered flood mapping from Sentinel-1 SAR achieves 10-20 minute response time during typhoons</span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PRiSM (PhilRice-IRRI):** Operational since 2014, first satellite-based rice monitoring in Southeast Asia</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SkAI-Pinas (DOST):** National AI framework addressing the gap between abundant remote sensing data and sustainable AI pipelines</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DIMER Model Repository (DOST-ASTI):** Democratizing access to trained models for Philippine contexts</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 2: The AI/ML Workflow for Earth Observation</span></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>Understanding the complete workflow is essential for successful EO projects. Each step matters, and according to 2024 research, most underperforming models suffer from data issues rather than algorithm deficiencies.</span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Complete AI/ML Workflow for Earth Observation</span><span class="ot">"</span></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>flowchart TD</span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">1</span>. Problem Definition&lt;br/&gt;What question?&lt;br/&gt;What output?] --&gt; B[<span class="dv">2</span>. Data Acquisition&lt;br/&gt;Satellite imagery&lt;br/&gt;Ground truth&lt;br/&gt;Ancillary data]</span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a>    B --&gt; C[<span class="dv">3</span>. Data Preprocessing&lt;br/&gt;Atmospheric correction&lt;br/&gt;Cloud masking&lt;br/&gt;Normalization]</span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a>    C --&gt; D[<span class="dv">4</span>. Data Annotation&lt;br/&gt;Label training samples&lt;br/&gt;Quality control&lt;br/&gt;Class balancing]</span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a>    D --&gt; E[<span class="dv">5</span>. Feature Engineering&lt;br/&gt;Spectral indices&lt;br/&gt;Texture features&lt;br/&gt;Temporal metrics]</span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>    E --&gt; F{<span class="dv">6</span>. Train/Val/Test&lt;br/&gt;Split}</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>    F --&gt;|<span class="dv">70</span>%| G[Training Set]</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>    F --&gt;|<span class="dv">15</span>%| H[Validation Set]</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>    F --&gt;|<span class="dv">15</span>%| I[Test Set]</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>    G --&gt; J[<span class="dv">7</span>. Model Training&lt;br/&gt;Select architecture&lt;br/&gt;Set hyperparameters&lt;br/&gt;Train on GPU]</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>    H --&gt; K[<span class="dv">8</span>. Model Validation&lt;br/&gt;Tune hyperparameters&lt;br/&gt;Monitor overfitting&lt;br/&gt;Early stopping]</span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>    J --&gt; K</span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>    K --&gt;|Iterate| J</span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>    K --&gt; L{Performance&lt;br/&gt;Acceptable?}</span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>    L --&gt;|No| M[Improve Data&lt;br/&gt;More samples&lt;br/&gt;Better labels&lt;br/&gt;Data augmentation]</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>    M --&gt; D</span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>    L --&gt;|Yes| N[<span class="dv">9</span>. Model Testing&lt;br/&gt;Final evaluation&lt;br/&gt;Unseen test set&lt;br/&gt;Confusion matrix]</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>    I --&gt; N</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>    N --&gt; O[<span class="dv">10</span>. Deployment&lt;br/&gt;Production <span class="fu">system</span>&lt;br/&gt;Monitoring&lt;br/&gt;Maintenance]</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>    O --&gt; P{Model Drift?&lt;br/&gt;Performance&lt;br/&gt;degraded?}</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>    P --&gt;|Yes| Q[Retrain with&lt;br/&gt;new data]</span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>    Q --&gt; D</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>    P --&gt;|No| O</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>    style B fill:<span class="co">#00aa44,stroke:#006622,stroke-width:2px,color:#fff</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>    style C fill:<span class="co">#00aa44,stroke:#006622,stroke-width:2px,color:#fff</span></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>    style D fill:<span class="co">#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>    style J fill:<span class="co">#cc00cc,stroke:#880088,stroke-width:2px,color:#fff</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a>    style K fill:<span class="co">#cc00cc,stroke:#880088,stroke-width:2px,color:#fff</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>    style O fill:<span class="co">#009999,stroke:#006666,stroke-width:2px,color:#fff</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 1: Problem Definition</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>**Define clearly what you want to achieve:**</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What question are you answering? (e.g., "Where are mangroves declining?")</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What output do you need? (map, time series, alert system?)</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What accuracy is acceptable?</span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What constraints exist? (time, computational resources, data availability)</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What is the operational context and who will use the outputs?</span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>**Philippine Example: PRiSM Rice Monitoring**</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>**Problem:** Provide timely rice area and production estimates for food security planning and disaster response</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>**Clear definition:**</span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-class: rice wet season, rice dry season, non-rice agriculture, non-agriculture</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>10m spatial resolution (Sentinel-1 SAR + Sentinel-2 optical)</span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal: Per-season mapping (wet: June-Nov, dry: Dec-May)</span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accuracy target: &gt;90% for policy-level decisions</span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Operational: Automated processing, quarterly updates to DA/PCIC</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cloud-penetrating capability essential for monsoon season</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 2: Data Acquisition</span></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a>**Gather all necessary data:**</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Satellite imagery:** Sentinel-1/2, Landsat, commercial VHR, hyperspectral</span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ground truth:** Field surveys, high-res imagery interpretation, existing maps, crowdsourced data</span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ancillary data:** DEM, climate, administrative boundaries, road networks, socioeconomic data</span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>**Data volume considerations:**</span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>NASA's Earth Science Data Systems: 148 PB (2023) → 205 PB (2024) → 250 PB (2025)</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel constellation generates thousands of terabytes daily</span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1,052 active EO satellites as of 2024</span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>**Data sources for Philippines:**</span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CoPhil Mirror Site (2025):** Local, high-bandwidth access to Sentinel data covering entire archipelago</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Copernicus Data Space Ecosystem:** STAC-compliant catalogues, API-driven access</span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Google Earth Engine:** Harmonized Sentinel-2 surface reflectance, Sentinel-1 GRD collections</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PhilSA SIYASAT:** NovaSAR-1 X-band SAR data</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Diwata-1/2 microsatellites:** Philippine-operated disaster monitoring</span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NAMRIA Geoportal:** Land cover basemaps, topographic data</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PAGASA:** Climate and meteorological data</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 3: Data Pre-processing</span></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>**Critical step - "Garbage in, garbage out"**</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>Data pre-processing is foundational and directly impacts downstream analysis accuracy. Proper preprocessing ensures data quality, consistency, and comparability across time and sensors.</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>**For satellite imagery:**</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>**Atmospheric Correction:**</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Purpose:** Remove atmospheric effects (scattering, absorption)</span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Convert:** Top-of-Atmosphere (TOA) reflectance → Surface reflectance (SR)</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-2:** Use Level-2A products (Sen2Cor algorithm)</span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**HLS Products:** NASA's Harmonized Landsat Sentinel-2 applies LaSRC + BRDF normalization</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Essential for:** Multi-temporal comparisons, quantitative biophysical parameter retrieval</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>**Cloud Masking:**</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-2 SCL:** Scene Classification Layer (clouds, shadows, snow, water, vegetation)</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Machine learning approaches:** U-Net architectures for pixel-wise cloud segmentation</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-temporal approaches:** Leverage temporal patterns to identify clouds</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gap filling:** Temporal interpolation, spatial interpolation, deep learning reconstruction (Prithvi-EO-2.0)</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>**SAR-Specific Preprocessing (Sentinel-1):**</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Orbit file application:** Precise geolocation</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radiometric calibration:** Convert DN to sigma nought (σ⁰), beta nought (β⁰), or gamma nought (γ⁰)</span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**De-bursting:** Remove black boundaries between sub-swaths in TOPS mode</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Speckle filtering:** Lee, Frost, Gamma-MAP filters; CNN-based despecklers preserve edges</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Terrain correction (RTC):** Orthorectification using DEM (SRTM, Copernicus DEM)</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-temporal filtering:** Leverage temporal stack to reduce speckle</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>**Normalization and Scaling:**</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Min-max normalization:** Scale to <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> range</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Z-score standardization:** Center to mean=0, std=1 (common for deep learning)</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Percentile clipping:** Reduce impact of outliers (e.g., 2nd and 98th percentiles)</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per-band normalization:** Account for different dynamic ranges across spectral bands</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a>**For training labels:**</span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Quality control:** Verify label accuracy through multiple reviewers</span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coordinate alignment:** Ensure labels match imagery timing and location</span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class balancing:** Ensure adequate samples per class</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Format standardization:** Convert to ML-ready format (GeoTIFF, TFRecord, COG)</span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-processing Pitfalls</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>**Common errors that degrade model performance:**</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using Top-of-Atmosphere instead of surface reflectance</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal mismatch: 2020 imagery with 2018 labels</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Incomplete cloud masking leaving cloud shadows</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mixed pixels at boundaries (especially for validation)</span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inconsistent band ordering across scenes</span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ignoring spatial autocorrelation (random train-test splits can lead to 28% overoptimistic performance)</span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Not applying same preprocessing to training and deployment data</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 4: Feature Engineering</span></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a>**Deriving informative variables from raw data**</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>Feature engineering transforms raw satellite data into informative representations that enhance model performance. This step is crucial for traditional ML algorithms and beneficial even for deep learning.</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>**For traditional ML (Random Forest, SVM):**</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>**Spectral Indices:**</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vegetation:** NDVI, EVI, SAVI, NDRE, GNDVI, LAI</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Water:** NDWI, MNDWI, NDMI</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Built-up:** NDBI, Bare Soil Index (BSI)</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Burn:** NBR, dNBR</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>**Textural Features (GLCM):**</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Contrast, Correlation, Energy, Homogeneity, Entropy, Dissimilarity, Variance</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Window sizes: 3×3, 5×5, 7×7</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple directions: 0°, 45°, 90°, 135°</span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>**Temporal Features:**</span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Statistical: Mean, median, std dev, min, max, percentiles (10th, 25th, 75th, 90th)</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficient of Variation (CV): Normalized variability measure</span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Amplitude: Difference between peak and minimum</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Phenological: Start of season (SOS), Peak of season (POS), End of season (EOS)</span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trends: Linear regression slopes, breakpoint detection</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>**Multi-Modal Features:**</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optical-SAR fusion:** Concatenate optical indices with SAR backscatter</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Derived ratios:** VV/VH polarization ratio, optical/SAR combinations</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SAR texture:** GLCM features from backscatter</span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interferometric:** Coherence from InSAR</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>**Example: Forest classification features**</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="co"># Spectral indices</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>NDVI <span class="op">=</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> Red)</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>NDWI <span class="op">=</span> (Green <span class="op">-</span> NIR) <span class="op">/</span> (Green <span class="op">+</span> NIR)</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>EVI <span class="op">=</span> <span class="fl">2.5</span> <span class="op">*</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> <span class="dv">6</span><span class="op">*</span>Red <span class="op">-</span> <span class="fl">7.5</span><span class="op">*</span>Blue <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a><span class="co"># SAR features</span></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>VV_VH_ratio <span class="op">=</span> VV_backscatter <span class="op">/</span> VH_backscatter</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>SAR_texture <span class="op">=</span> GLCM_contrast(VH_backscatter)</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a><span class="co"># Temporal</span></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a>NDVI_mean <span class="op">=</span> mean(NDVI_time_series)</span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>NDVI_cv <span class="op">=</span> std(NDVI_time_series) <span class="op">/</span> NDVI_mean</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a><span class="co"># Topographic</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>Elevation, Slope, Aspect</span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Input feature vector per pixel</span></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, EVI, NDWI,</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>     VV, VH, VV_VH_ratio, SAR_texture, NDVI_mean, NDVI_cv,</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>     Elevation, Slope, Aspect]</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>**For deep learning (CNNs, U-Net, Vision Transformers):**</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less manual feature engineering needed</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Networks automatically learn features from raw pixels</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Still benefit from good input data (cloud-free, calibrated, normalized)</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-spectral bands as input channels</span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider temporal stacking for multi-date analysis</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2024 Research Insight: Feature Selection</span></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a>Seven unsupervised dimensionality reduction algorithms tested on hyperspectral data from HYPSO-1 satellite showed that:</span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Careful feature selection can achieve optimal accuracy with &lt;20% of temporal instances</span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Single band from single sensor can be sufficient for specific tasks</span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Implication:** Smart data selection &gt; brute force data collection</span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use PCA, MNF, or tree-based feature importance for efficient selection</span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 5: Model Selection and Training</span></span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>**Choose appropriate algorithm:**</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>**Consider:**</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Task type (classification, regression, segmentation, object detection)</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data size (deep learning needs more data; transfer learning reduces requirements)</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretability requirements (operational systems often need explainability)</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computational resources (edge devices vs. cloud platforms)</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deployment constraints (inference speed, model size)</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>**Common EO algorithms:**</span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Algorithm <span class="pp">|</span> Type <span class="pp">|</span> Best For <span class="pp">|</span> Data Needs <span class="pp">|</span> Key Strengths <span class="pp">|</span></span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|------|----------|------------|---------------|</span></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Random Forest** <span class="pp">|</span> Ensemble <span class="pp">|</span> Classification, feature importance, baseline <span class="pp">|</span> Medium (100s-1000s) <span class="pp">|</span> Robust, interpretable, handles high dimensions <span class="pp">|</span></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SVM** <span class="pp">|</span> Kernel <span class="pp">|</span> Binary classification, small data <span class="pp">|</span> Small-Medium <span class="pp">|</span> Effective high-dimensional spaces <span class="pp">|</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **XGBoost/LightGBM** <span class="pp">|</span> Gradient Boosting <span class="pp">|</span> Tabular features, yield prediction <span class="pp">|</span> Medium <span class="pp">|</span> High performance on structured data <span class="pp">|</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **CNN** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Image classification, automatic features <span class="pp">|</span> Large (10,000s+) <span class="pp">|</span> Spatial awareness, hierarchical learning <span class="pp">|</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **U-Net** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Semantic segmentation (pixel-wise) <span class="pp">|</span> Large <span class="pp">|</span> Skip connections, works with limited data <span class="pp">|</span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **ResNet** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Very deep networks, complex classification <span class="pp">|</span> Large <span class="pp">|</span> Residual connections avoid vanishing gradients <span class="pp">|</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Vision Transformer** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Global context, spatial relationships <span class="pp">|</span> Very Large <span class="pp">|</span> Attention mechanisms, long-range dependencies <span class="pp">|</span></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **LSTM/GRU** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Time series prediction, phenology <span class="pp">|</span> Large <span class="pp">|</span> Temporal pattern learning <span class="pp">|</span></span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **YOLO/Faster R-CNN** <span class="pp">|</span> Deep Learning <span class="pp">|</span> Object detection (buildings, ships) <span class="pp">|</span> Large <span class="pp">|</span> Real-time detection, bounding boxes <span class="pp">|</span></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a>**Training process:**</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Split data:** 70-80% training, 10-15% validation, 10-15% testing</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Spatial cross-validation:** Essential for EO to avoid spatial leakage</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Spatial k-fold or buffered leave-one-out</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Feed training data:** Algorithm adjusts parameters to minimize error</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Monitor validation:** Track performance on held-out validation set</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Hyperparameter tuning:** Optimize learning rate, batch size, architecture parameters</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Early stopping:** Stop when validation performance plateaus</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Final evaluation:** Test on completely independent test set</span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>**Transfer Learning:**</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-train on large dataset (ImageNet, SatViT, Prithvi)</span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fine-tune on task-specific data</span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces training data requirements by 10-100×</span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Faster convergence and better generalization</span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a>**Philippine Example: Poverty Mapping with Transfer Learning**</span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a>Study using satellite imagery, nighttime lights, and OpenStreetMap data:</span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning from ImageNet improved performance by 14.1% for tropical cyclone impact areas</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires careful hyperparameter tuning for generalization across regions</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cost-effective approach for limited labeled data scenarios</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 6: Validation and Evaluation</span></span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a>**Rigorous testing on independent data**</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a><span class="fu">## Never Test on Training Data!</span></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a>Testing on data the model has seen gives falsely optimistic results. Always use held-out test data. For EO, random train-test splits can overestimate performance by up to 28% due to spatial autocorrelation.</span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>**Classification metrics:**</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overall Accuracy (OA):** Percentage of correctly classified pixels</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confusion Matrix:** Shows which classes are confused with each other</span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per-Class Metrics:**</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Producer's Accuracy (Recall): How many ground truth samples were correctly classified</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>User's Accuracy (Precision): How many predicted samples are actually correct</span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F1-Score:** Harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall))</span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kappa Coefficient:** Agreement accounting for chance</span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Matthews Correlation Coefficient (MCC):** Balanced measure even for imbalanced classes</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>**Semantic Segmentation metrics:**</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**IoU (Intersection over Union):** Area of overlap / Area of union</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>IoU &gt; 0.5: Generally acceptable</span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>IoU &gt; 0.7: High-quality segmentation</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>IoU &gt; 0.9: Excellent agreement</span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mean IoU (mIoU):** Average IoU across all classes</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dice Coefficient:** 2 × IoU / (1 + IoU), less harsh penalty than IoU</span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pixel Accuracy:** Simple but biased toward majority class</span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Boundary F1 Score:** Precision/recall on boundary pixels</span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>**Regression metrics:**</span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSE (Root Mean Squared Error):** Average prediction error, penalizes large errors</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MAE (Mean Absolute Error):** Average absolute deviation, less sensitive to outliers</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R² (Coefficient of Determination):** Proportion of variance explained (0.88 = 88% explained)</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>**Object Detection metrics:**</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Precision/Recall:** At various IoU thresholds (0.5, 0.75)</span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Average Precision (AP):** Area under precision-recall curve</span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mean Average Precision (mAP):** Mean AP across classes</span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>**Philippine Example: Flood mapping evaluation**</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a><span class="in">Confusion Matrix (DOST-ASTI DATOS flood detection):</span></span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a><span class="in">                Predicted</span></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a><span class="in">              | Flood | No Flood |</span></span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a><span class="in">Actual Flood  |  450  |   50     |  Producer's Acc (Recall): 90%</span></span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a><span class="in">Actual No Flood|  30   |  1470    |  Producer's Acc: 98%</span></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a><span class="in">User's Accuracy (Precision): 93.8%   96.7%</span></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a><span class="in">Overall Accuracy: 96%</span></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a><span class="in">F1-Score (Flood class): 91.8%</span></span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a>**Spatial Validation Best Practices:**</span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Spatial k-fold cross-validation:** Divide data into spatially homogeneous clusters</span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Buffered leave-one-out:** Create buffer zones around test samples</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Independent geographic testing:** Test on completely different regions</span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Temporal validation:** Train on one time period, test on another</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 7: Deployment and Operationalization</span></span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a>**Making the model operational:**</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a>**Deployment strategies:**</span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Batch processing:** Apply model to large archives (entire countries, multi-year time series)</span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Near real-time:** Process new satellite acquisitions automatically (disaster response)</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**On-demand:** User-triggered analysis (web portals, APIs)</span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Edge processing:** On-board satellite AI (ESA Φsat-2, launched 2024)</span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a>**On-Board AI Processing (2024 Breakthrough):**</span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ESA Φsat-2: 22cm CubeSat with on-board AI</span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Processes imagery directly in orbit</span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cloud filtering: Only clear, usable images sent to Earth</span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces data transmission costs, enables real-time event detection</span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rationale:** With 1,052 active EO satellites generating thousands of terabytes daily, traditional communication cannot relay this volume</span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a>**Operational considerations:**</span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scalability:** Can it handle regional/national scale?</span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automation:** Minimize manual intervention</span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance monitoring:** Track accuracy over time, detect distribution shifts</span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model retraining:** Update as conditions change or new data becomes available</span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model versioning:** Maintain reproducibility and track improvements</span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Integration:** Connect to decision support systems, early warning platforms</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**API development:** Create accessible interfaces for inference</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud deployment:** Google Earth Engine, AWS SageMaker, Azure ML, Vertex AI</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a>**Philippine context:**</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DOST-ASTI AIPI platform:** For model deployment and user-facing AI interfaces</span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DIMER repository:** For model sharing and democratizing access to trained models</span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ALaM (Automated Labeling Machine):** Combining automated labeling with crowdsourcing for continuous data quality improvement</span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Integration with LGU disaster response protocols:** DATOS outputs delivered to local government units</span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Delivery via PhilSA Digital Space Campus:** Training and capacity building</span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CoPhil Data Centre (2025):** Cloud-native distribution with API-driven access</span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 3: Types of Machine Learning</span></span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Machine Learning Paradigms for Earth Observation</span><span class="ot">"</span></span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a>graph TB</span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>    subgraph Supervised[<span class="ot">"</span><span class="st">SUPERVISED LEARNING&lt;br/&gt;Learning from labeled examples</span><span class="ot">"</span>]</span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a>        S1[Classification&lt;br/&gt;Discrete categories]</span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a>        S2[Regression&lt;br/&gt;Continuous <span class="fu">values</span>]</span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>        S3[Object Detection&lt;br/&gt;Locate + classify]</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a>        S4[Semantic Segmentation&lt;br/&gt;Pixel-wise classification]</span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>        S1 --&gt; S1A[Land Cover&lt;br/&gt;Forest, Water, Urban]</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>        S2 --&gt; S2A[Biomass Estimation&lt;br/&gt;Predict AGB in tons/ha]</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a>        S3 --&gt; S3A[Building Detection&lt;br/&gt;YOLO, Faster R-CNN]</span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a>        S4 --&gt; S4A[Flood Mapping&lt;br/&gt;U-Net segmentation]</span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>    subgraph Unsupervised[<span class="ot">"</span><span class="st">UNSUPERVISED LEARNING&lt;br/&gt;Finding patterns without labels</span><span class="ot">"</span>]</span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>        U1[Clustering&lt;br/&gt;Group similar pixels]</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>        U2[Dimensionality&lt;br/&gt;Reduction]</span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>        U3[Anomaly&lt;br/&gt;Detection]</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a>        U1 --&gt; U1A[K-Means&lt;br/&gt;ISODATA&lt;br/&gt;Spectral clusters]</span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a>        U2 --&gt; U2A[PCA&lt;br/&gt;MNF Transform&lt;br/&gt;Band reduction]</span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a>        U3 --&gt; U3A[Outlier Detection&lt;br/&gt;Change hotspots]</span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a>    subgraph SemiSupervised[<span class="ot">"</span><span class="st">SEMI-SUPERVISED&lt;br/&gt;Combine labeled + unlabeled</span><span class="ot">"</span>]</span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a>        SS1[Self-training&lt;br/&gt;Pseudo-labeling]</span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a>        SS2[Co-training&lt;br/&gt;Multiple views]</span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a>        SS1 --&gt; SS1A[Bootstrap from&lt;br/&gt;limited labels]</span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a>    subgraph Reinforcement[<span class="ot">"</span><span class="st">REINFORCEMENT LEARNING&lt;br/&gt;Learn from interaction</span><span class="ot">"</span>]</span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a>        R1[Agent-Environment&lt;br/&gt;Interaction]</span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a>        R1 --&gt; R1A[Drone Path&lt;br/&gt;Planning]</span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a>    style Supervised fill:<span class="co">#e6ffe6,stroke:#00aa44,stroke-width:3px</span></span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a>    style Unsupervised fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:3px</span></span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a>    style SemiSupervised fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:3px</span></span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>    style Reinforcement fill:<span class="co">#e6e6ff,stroke:#6666cc,stroke-width:3px</span></span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a><span class="fu">### Supervised Learning</span></span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a>**Learning from labeled data**</span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a>The algorithm is given:</span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input:** Satellite image or features</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Known label (class or value)</span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goal:** Learn mapping from input to output</span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>Supervised learning is the predominant approach in Earth Observation, where models learn from labeled training data to make predictions on new, unseen data.</span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Classification Tasks</span></span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a>**Predicting categorical labels**</span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>**Common Algorithms:**</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forest (RF):** Ensemble method, best performance in object-based classification; robust to noise</span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Support Vector Machines (SVM):** Effective for high-dimensional spaces; performs well with limited training samples</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CNNs (Convolutional Neural Networks):** Deep learning for automatic feature learning and complex patterns</span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vision Transformers (ViT):** Attention mechanisms for global context and long-range dependencies</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>**EO Examples:**</span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Land Cover Classification**</span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-2 pixel values (13 bands)</span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Forest, Water, Urban, Agriculture, Bare soil, Wetlands</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest, CNN, Vision Transformer</span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Datasets: EuroSAT (27,000 images, 10 classes, 98.57% accuracy)</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Cloud Detection**</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Multi-band imagery (blue, cirrus bands effective)</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Cloud vs. Clear, Cloud shadow, Cirrus</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Threshold, Random Forest, U-Net for pixel-wise segmentation</span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Sentinel-2 SCL: Scene Classification Layer with 11 classes</span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Crop Type Mapping**</span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Multi-temporal NDVI, SAR backscatter</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Rice, Corn, Sugarcane, Coconut, Vegetables</span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest, LSTM (for temporal patterns)</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Multi-temporal data &gt; single-date imagery for capturing phenology</span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a>**Philippine Case Study: PhilSA-DENR Mangrove Mapping**</span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a>**Task:** AI-based mangrove forest identification nationwide</span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a>**Technology:**</span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Google Earth Engine platform</span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mangrove Vegetation Index (MVI)</span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-1 and Sentinel-2 fusion</span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal Landsat 8 and Sentinel-2 data</span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a>**Approach:**</span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net deep learning architecture</span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary classification: mangrove vs. non-mangrove</span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SAR for cloud-penetrating capability during monsoon</span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accuracy: 99.73% (Myanmar study using similar approach)</span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random Forest classifier also effective with high temporal resolution (5-day Sentinel-2)</span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Support Vector Machine shows high accuracy for mangrove discrimination</span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Blue carbon programs and carbon stock monitoring</span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ecosystem service valuation</span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conservation planning and restoration monitoring</span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Palawan multi-spatiotemporal analysis with Markov chain future trend prediction</span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a>**Result:** Operational nationwide mangrove extent maps with regular updates</span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a>**Object Detection:**</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>Unlike pixel-level classification, object detection identifies and localizes specific objects of interest with bounding boxes.</span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a>**Popular Architectures:**</span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLO (You Only Look Once):** Real-time detection, single-stage architecture, fast inference</span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Faster R-CNN:** Two-stage detector with region proposals, high accuracy</span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RetinaNet:** Single-stage with focal loss for handling class imbalance</span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EfficientDet:** Scalable architecture balancing accuracy and efficiency</span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a>**Key Considerations:**</span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scale variation: Objects appear at different sizes depending on altitude and resolution</span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dense object detection: Multiple overlapping objects in urban or agricultural scenes</span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small object detection: Challenging for standard architectures (e.g., individual trees, vehicles)</span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building footprint extraction (SpaceNet, xView datasets)</span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ship detection in maritime surveillance</span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vehicle counting in traffic monitoring</span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Individual tree crown delineation</span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a>**Benchmark Dataset: xView**</span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&gt;1 million objects</span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>60 classes</span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&gt;1,400 km² coverage</span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>0.3m resolution (WorldView-3)</span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Purpose: Disaster response, overhead imagery analysis</span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Regression Tasks</span></span>
<span id="cb14-705"><a href="#cb14-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-706"><a href="#cb14-706" aria-hidden="true" tabindex="-1"></a>**Predicting continuous values**</span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a>**EO Examples:**</span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Biomass Estimation**</span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices, LiDAR (GEDI)</span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Forest biomass (tons per hectare), Above-ground biomass (AGB)</span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Random Forest Regression (most popular: R² = 0.70, RMSE = 25.38 Mg C ha⁻¹)</span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Multi-sensor integration: LiDAR + SAR + Optical improves accuracy</span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Note:** SAR saturates at high biomass levels; LiDAR methods achieve ~90% agreement with field data</span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Soil Moisture Prediction**</span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: Sentinel-1 VV/VH polarization, temperature, NDVI</span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Volumetric soil moisture (%)</span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: Neural network regression, Random Forest</span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Crop Yield Forecasting**</span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input: NDVI time series, EVI, NDMI, weather data (temperature, precipitation), soil properties</span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: Expected yield (tons per hectare)</span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Algorithm: LSTM regression (preferred in &gt;40% of studies), Random Forest</span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Performance: R² &gt; 0.93 for corn and soybean, RMSE &lt; 0.075 for NDVI estimation</span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Temporal considerations: Early-season (higher uncertainty) vs. end-of-season (more accurate, less actionable)</span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a>**Philippine Operational System: PRiSM Rice Yield Prediction**</span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a>**Overview:** Philippine Rice Information System, operational since 2014</span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a>**Technology:**</span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Synthetic Aperture Radar (SAR): Day and night, cloud-penetrating</span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop modeling and cloud computing</span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>UAV imagery and smartphone field surveys</span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Statistical data integration</span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a>**Data Sources:**</span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Remote sensing satellites (Sentinel-1, RADARSAT)</span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vegetation indices (NDVI, EVI)</span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weather data from PAGASA</span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Historical yield records</span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a>**Modeling:**</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LSTM networks for temporal modeling of SAR backscatter and vegetation indices</span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random Forest for integrating multi-source data</span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Phenology-based models</span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a>**Partners:**</span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>International Rice Research Institute (IRRI) - technology development</span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Philippine Rice Research Institute (PhilRice) - operations since 2018</span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Department of Agriculture (DA) - policy formulation and planning</span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Food security planning and policy formulation</span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Disaster response (typhoon impact assessment)</span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop insurance (PCIC integration)</span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Per-season mapping: Wet season (June-Nov), Dry season (Dec-May)</span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a>**Significance:** First satellite-based rice monitoring system in Southeast Asia, model for regional applications</span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a>**Key difference from classification:**</span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output is a number on a continuous scale rather than discrete classes</span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss functions measure distance from true value (MSE, MAE, RMSE)</span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation uses regression metrics (R², RMSE, MAE)</span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predictions can be interpolated and extrapolated</span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unsupervised Learning</span></span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a>**Finding patterns in unlabeled data**</span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a>The algorithm receives:</span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input:** Satellite imagery or features</span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**No labels provided**</span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goal:** Discover inherent structure or groupings</span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a>Unsupervised learning techniques do not require labeled training data, making them valuable for exploratory analysis, data reduction, and scenarios where ground-truth is unavailable or expensive to obtain.</span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Clustering</span></span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a>**Grouping similar pixels/regions together**</span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a>**Common algorithm: k-means**</span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Specify number of clusters (k)</span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Algorithm iteratively groups pixels with similar spectral characteristics</span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Result: Image segmented into k clusters</span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Human interpretation needed:** "Cluster 1 looks like water, Cluster 2 like forest..."</span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a>**Other Clustering Methods:**</span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a>**Hierarchical Clustering:**</span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Builds tree-like structure (dendrogram) of nested clusters</span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Agglomerative (bottom-up) or Divisive (top-down)</span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No need to specify number of clusters a priori</span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Multi-scale land cover analysis, ecological zone identification</span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a>**DBSCAN (Density-Based Spatial Clustering):**</span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Groups points based on density</span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identifies clusters of arbitrary shape</span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Robust to outliers</span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Urban area detection, anomaly detection in satellite imagery</span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a>**EO Applications:**</span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Exploratory analysis:** "How many distinct spectral classes in this region?"</span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Change detection:** Cluster before/after images to find anomalies</span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Image segmentation:** Group similar pixels for object-based analysis</span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**InSAR time series:** K-means with PCA for identifying spatially and temporally coherent displacement phenomena</span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Dimensionality Reduction</span></span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a>**Principal Component Analysis (PCA):**</span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear transformation projecting high-dimensional data onto orthogonal axes of maximum variance</span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Hyperspectral data compression, feature extraction, noise reduction, change detection</span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Workflow:** Center data → Compute covariance → Calculate eigenvalues/eigenvectors → Select top K components</span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Benefits:** Reduces computational requirements, removes redundancy, enhances signal-to-noise ratio</span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First few PCs capture most variance</span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a>**Independent Component Analysis (ICA):**</span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Separates multivariate signal into independent, non-Gaussian components</span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Mixed pixel decomposition, blind source separation, endmember extraction in hyperspectral imagery</span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a>**t-SNE and UMAP:**</span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Non-linear dimensionality reduction for visualization and exploratory analysis</span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Preserves local structure, reveals clusters and patterns in 2D/3D</span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Visualization of high-dimensional feature spaces, exploration of spectral diversity</span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Limitations:** Computationally expensive, hyperparameter sensitive, not suitable for new data projection (t-SNE)</span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a><span class="fu">## When to Use Unsupervised Learning</span></span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No need for expensive labeled data</span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can discover unexpected patterns</span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for initial data exploration</span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data reduction for preprocessing</span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a>**Limitations:**</span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Results need interpretation</span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No guarantee clusters match desired classes</span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often less accurate than supervised methods for specific tasks</span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Difficult to evaluate objectively without labels</span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Determining optimal number of clusters can be challenging</span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a>**Best Practice:** Use unsupervised methods for exploration, then refine with supervised learning for operational applications</span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a>**Comparison Example:**</span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a>**Supervised (Land Cover Classification):**</span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provide 1000 labeled samples: forest, water, urban</span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train Random Forest</span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Result: Every pixel assigned forest/water/urban</span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation: 90% accuracy against test labels</span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a>**Unsupervised (k-means Clustering):**</span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No labels provided</span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run k-means with k=3</span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Result: Three clusters emerge</span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare</span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation: Subjective or requires labels anyway</span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a>**Integration Strategy:**</span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use clustering to create initial training samples</span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Apply dimensionality reduction (PCA) before classification</span>
<span id="cb14-870"><a href="#cb14-870" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Combine unsupervised pre-training with supervised fine-tuning</span>
<span id="cb14-871"><a href="#cb14-871" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Use clustering for quality control of labeled data</span>
<span id="cb14-872"><a href="#cb14-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-873"><a href="#cb14-873" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-874"><a href="#cb14-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-875"><a href="#cb14-875" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 4: Deep Learning Architectures for Earth Observation</span></span>
<span id="cb14-876"><a href="#cb14-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-877"><a href="#cb14-877" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is Deep Learning?</span></span>
<span id="cb14-878"><a href="#cb14-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-879"><a href="#cb14-879" aria-hidden="true" tabindex="-1"></a>**Deep Learning = Neural Networks with Many Layers**</span>
<span id="cb14-880"><a href="#cb14-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-881"><a href="#cb14-881" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Subset of machine learning</span>
<span id="cb14-882"><a href="#cb14-882" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inspired by biological neurons</span>
<span id="cb14-883"><a href="#cb14-883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple processing layers extract progressively abstract features</span>
<span id="cb14-884"><a href="#cb14-884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dominant approach for image analysis since ~2012</span>
<span id="cb14-885"><a href="#cb14-885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Revolutionized Earth Observation, enabling automated feature learning and state-of-the-art performance</span>
<span id="cb14-886"><a href="#cb14-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-887"><a href="#cb14-887" aria-hidden="true" tabindex="-1"></a>**Why "deep"?**</span>
<span id="cb14-888"><a href="#cb14-888" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Refers to depth: many hidden layers</span>
<span id="cb14-889"><a href="#cb14-889" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modern networks: 10s to 100s of layers (ResNet-152 has 152 layers)</span>
<span id="cb14-890"><a href="#cb14-890" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables learning complex, hierarchical representations</span>
<span id="cb14-891"><a href="#cb14-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lower layers: Edges, textures</span>
<span id="cb14-892"><a href="#cb14-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Middle layers: Patterns, shapes</span>
<span id="cb14-893"><a href="#cb14-893" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Higher layers: Semantic concepts, objects</span>
<span id="cb14-894"><a href="#cb14-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-895"><a href="#cb14-895" aria-hidden="true" tabindex="-1"></a>**Key Advantages for EO:**</span>
<span id="cb14-896"><a href="#cb14-896" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic feature extraction from raw pixels</span>
<span id="cb14-897"><a href="#cb14-897" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spatial awareness through convolutional operations</span>
<span id="cb14-898"><a href="#cb14-898" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale analysis capabilities</span>
<span id="cb14-899"><a href="#cb14-899" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles large, complex datasets</span>
<span id="cb14-900"><a href="#cb14-900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning reduces data requirements</span>
<span id="cb14-901"><a href="#cb14-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-902"><a href="#cb14-902" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neural Network Fundamentals</span></span>
<span id="cb14-903"><a href="#cb14-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-904"><a href="#cb14-904" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Artificial Neuron</span></span>
<span id="cb14-905"><a href="#cb14-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-906"><a href="#cb14-906" aria-hidden="true" tabindex="-1"></a>**Building block of neural networks:**</span>
<span id="cb14-907"><a href="#cb14-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-908"><a href="#cb14-908" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-909"><a href="#cb14-909" aria-hidden="true" tabindex="-1"></a><span class="in">Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output</span></span>
<span id="cb14-910"><a href="#cb14-910" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-911"><a href="#cb14-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-912"><a href="#cb14-912" aria-hidden="true" tabindex="-1"></a>**Mathematical operation:**</span>
<span id="cb14-913"><a href="#cb14-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-914"><a href="#cb14-914" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Weighted sum:** <span class="in">`z = w1*x1 + w2*x2 + w3*x3 + b`</span></span>
<span id="cb14-915"><a href="#cb14-915" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Activation function:** <span class="in">`output = activation(z)`</span></span>
<span id="cb14-916"><a href="#cb14-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-917"><a href="#cb14-917" aria-hidden="true" tabindex="-1"></a>**Example: Detecting bright pixels**</span>
<span id="cb14-918"><a href="#cb14-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-919"><a href="#cb14-919" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-920"><a href="#cb14-920" aria-hidden="true" tabindex="-1"></a><span class="in">Inputs: [Red=0.8, Green=0.7, NIR=0.9]</span></span>
<span id="cb14-921"><a href="#cb14-921" aria-hidden="true" tabindex="-1"></a><span class="in">Weights: [w1=1.0, w2=1.0, w3=1.0]</span></span>
<span id="cb14-922"><a href="#cb14-922" aria-hidden="true" tabindex="-1"></a><span class="in">Bias: b = -2.0</span></span>
<span id="cb14-923"><a href="#cb14-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-924"><a href="#cb14-924" aria-hidden="true" tabindex="-1"></a><span class="in">z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4</span></span>
<span id="cb14-925"><a href="#cb14-925" aria-hidden="true" tabindex="-1"></a><span class="in">output = ReLU(0.4) = 0.4  (indicates moderately bright)</span></span>
<span id="cb14-926"><a href="#cb14-926" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-927"><a href="#cb14-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-928"><a href="#cb14-928" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Network Architecture</span></span>
<span id="cb14-929"><a href="#cb14-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-930"><a href="#cb14-930" aria-hidden="true" tabindex="-1"></a>**Layers of neurons:**</span>
<span id="cb14-931"><a href="#cb14-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-932"><a href="#cb14-932" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Input Layer:** Receives raw data (e.g., pixel values from all spectral bands)</span>
<span id="cb14-933"><a href="#cb14-933" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Hidden Layers:** Process and transform data through learned representations</span>
<span id="cb14-934"><a href="#cb14-934" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Output Layer:** Produces final prediction (class probabilities or continuous values)</span>
<span id="cb14-935"><a href="#cb14-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-936"><a href="#cb14-936" aria-hidden="true" tabindex="-1"></a>**For a simple image classification:**</span>
<span id="cb14-937"><a href="#cb14-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-938"><a href="#cb14-938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-939"><a href="#cb14-939" aria-hidden="true" tabindex="-1"></a><span class="in">Input Layer (256 neurons = 16×16 image)</span></span>
<span id="cb14-940"><a href="#cb14-940" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb14-941"><a href="#cb14-941" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 1 (128 neurons with ReLU)</span></span>
<span id="cb14-942"><a href="#cb14-942" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb14-943"><a href="#cb14-943" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 2 (64 neurons with ReLU)</span></span>
<span id="cb14-944"><a href="#cb14-944" aria-hidden="true" tabindex="-1"></a><span class="in">   ↓</span></span>
<span id="cb14-945"><a href="#cb14-945" aria-hidden="true" tabindex="-1"></a><span class="in">Output Layer (5 neurons = 5 classes, softmax activation)</span></span>
<span id="cb14-946"><a href="#cb14-946" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-947"><a href="#cb14-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-948"><a href="#cb14-948" aria-hidden="true" tabindex="-1"></a>Each connection has a **weight** - the network learns optimal weights through training via backpropagation.</span>
<span id="cb14-949"><a href="#cb14-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-950"><a href="#cb14-950" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Activation Functions</span></span>
<span id="cb14-951"><a href="#cb14-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-952"><a href="#cb14-952" aria-hidden="true" tabindex="-1"></a>**Introduce non-linearity - crucial for learning complex patterns**</span>
<span id="cb14-953"><a href="#cb14-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-954"><a href="#cb14-954" aria-hidden="true" tabindex="-1"></a>**Common activation functions:**</span>
<span id="cb14-955"><a href="#cb14-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-956"><a href="#cb14-956" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Function <span class="pp">|</span> Equation <span class="pp">|</span> Range <span class="pp">|</span> Use Case <span class="pp">|</span> Properties <span class="pp">|</span></span>
<span id="cb14-957"><a href="#cb14-957" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|----------|-------|----------|------------|</span></span>
<span id="cb14-958"><a href="#cb14-958" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **ReLU** <span class="pp">|</span> <span class="in">`max(0, x)`</span> <span class="pp">|</span> [0, ∞) <span class="pp">|</span> Hidden layers (most common) <span class="pp">|</span> Simple, efficient, avoids vanishing gradient <span class="pp">|</span></span>
<span id="cb14-959"><a href="#cb14-959" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Sigmoid** <span class="pp">|</span> <span class="in">`1 / (1 + e^-x)`</span> <span class="pp">|</span> (0, 1) <span class="pp">|</span> Binary classification output <span class="pp">|</span> Smooth, probabilistic interpretation <span class="pp">|</span></span>
<span id="cb14-960"><a href="#cb14-960" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Softmax** <span class="pp">|</span> <span class="in">`e^xi / Σe^xj`</span> <span class="pp">|</span> (0, 1), sum=1 <span class="pp">|</span> Multi-class classification output <span class="pp">|</span> Converts logits to probabilities <span class="pp">|</span></span>
<span id="cb14-961"><a href="#cb14-961" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Tanh** <span class="pp">|</span> <span class="in">`(e^x - e^-x) / (e^x + e^-x)`</span> <span class="pp">|</span> (-1, 1) <span class="pp">|</span> Hidden layers (older networks) <span class="pp">|</span> Zero-centered, smooth <span class="pp">|</span></span>
<span id="cb14-962"><a href="#cb14-962" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **LeakyReLU** <span class="pp">|</span> <span class="in">`max(αx, x), α=0.01`</span> <span class="pp">|</span> (-∞, ∞) <span class="pp">|</span> Hidden layers <span class="pp">|</span> Allows small negative gradient <span class="pp">|</span></span>
<span id="cb14-963"><a href="#cb14-963" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **GELU** <span class="pp">|</span> <span class="in">`x * Φ(x)`</span> <span class="pp">|</span> (-∞, ∞) <span class="pp">|</span> Transformers <span class="pp">|</span> Smooth, stochastic regularization <span class="pp">|</span></span>
<span id="cb14-964"><a href="#cb14-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-965"><a href="#cb14-965" aria-hidden="true" tabindex="-1"></a>**Why activation functions matter:**</span>
<span id="cb14-966"><a href="#cb14-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-967"><a href="#cb14-967" aria-hidden="true" tabindex="-1"></a>Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth! Networks would only learn linear decision boundaries.</span>
<span id="cb14-968"><a href="#cb14-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-969"><a href="#cb14-969" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-970"><a href="#cb14-970" aria-hidden="true" tabindex="-1"></a><span class="fu">## ReLU: The Default Choice</span></span>
<span id="cb14-971"><a href="#cb14-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-972"><a href="#cb14-972" aria-hidden="true" tabindex="-1"></a>**ReLU (Rectified Linear Unit)** has become standard for hidden layers because:</span>
<span id="cb14-973"><a href="#cb14-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-974"><a href="#cb14-974" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple: <span class="in">`f(x) = max(0, x)`</span></span>
<span id="cb14-975"><a href="#cb14-975" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally efficient</span>
<span id="cb14-976"><a href="#cb14-976" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Avoids vanishing gradient problem that plagued sigmoid/tanh</span>
<span id="cb14-977"><a href="#cb14-977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Empirically performs very well across diverse problems</span>
<span id="cb14-978"><a href="#cb14-978" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sparsity: ~50% of neurons set to zero, acting as automatic feature selection</span>
<span id="cb14-979"><a href="#cb14-979" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-980"><a href="#cb14-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-981"><a href="#cb14-981" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Loss Functions</span></span>
<span id="cb14-982"><a href="#cb14-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-983"><a href="#cb14-983" aria-hidden="true" tabindex="-1"></a>**Measure how wrong the model's predictions are**</span>
<span id="cb14-984"><a href="#cb14-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-985"><a href="#cb14-985" aria-hidden="true" tabindex="-1"></a>The model's objective: **minimize the loss function** through gradient descent optimization.</span>
<span id="cb14-986"><a href="#cb14-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-987"><a href="#cb14-987" aria-hidden="true" tabindex="-1"></a>**For classification:**</span>
<span id="cb14-988"><a href="#cb14-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-989"><a href="#cb14-989" aria-hidden="true" tabindex="-1"></a>**Categorical Cross-Entropy (Log Loss):**</span>
<span id="cb14-990"><a href="#cb14-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-991"><a href="#cb14-991" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-992"><a href="#cb14-992" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -Σ(y_true * log(y_pred))</span></span>
<span id="cb14-993"><a href="#cb14-993" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-994"><a href="#cb14-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-995"><a href="#cb14-995" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Penalizes confident wrong predictions heavily</span>
<span id="cb14-996"><a href="#cb14-996" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encourages high probability for correct class</span>
<span id="cb14-997"><a href="#cb14-997" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard for multi-class classification</span>
<span id="cb14-998"><a href="#cb14-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-999"><a href="#cb14-999" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb14-1000"><a href="#cb14-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1001"><a href="#cb14-1001" aria-hidden="true" tabindex="-1"></a><span class="in">True class: Forest (encoded as [1, 0, 0, 0, 0])</span></span>
<span id="cb14-1002"><a href="#cb14-1002" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest</span></span>
<span id="cb14-1003"><a href="#cb14-1003" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -1*log(0.7) = 0.36</span></span>
<span id="cb14-1004"><a href="#cb14-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1005"><a href="#cb14-1005" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest</span></span>
<span id="cb14-1006"><a href="#cb14-1006" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = -1*log(0.2) = 1.61  (much higher penalty)</span></span>
<span id="cb14-1007"><a href="#cb14-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1008"><a href="#cb14-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1009"><a href="#cb14-1009" aria-hidden="true" tabindex="-1"></a>**Binary Cross-Entropy:**</span>
<span id="cb14-1010"><a href="#cb14-1010" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For binary classification (e.g., flood vs. no-flood)</span>
<span id="cb14-1011"><a href="#cb14-1011" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Similar principle, optimized for two classes</span>
<span id="cb14-1012"><a href="#cb14-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1013"><a href="#cb14-1013" aria-hidden="true" tabindex="-1"></a>**Focal Loss:**</span>
<span id="cb14-1014"><a href="#cb14-1014" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Addresses class imbalance by down-weighting well-classified examples</span>
<span id="cb14-1015"><a href="#cb14-1015" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focuses training on hard examples</span>
<span id="cb14-1016"><a href="#cb14-1016" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Used in RetinaNet object detection</span>
<span id="cb14-1017"><a href="#cb14-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1018"><a href="#cb14-1018" aria-hidden="true" tabindex="-1"></a>**For semantic segmentation:**</span>
<span id="cb14-1019"><a href="#cb14-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1020"><a href="#cb14-1020" aria-hidden="true" tabindex="-1"></a>**Dice Loss:**</span>
<span id="cb14-1021"><a href="#cb14-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Based on Dice coefficient: 2×IoU / (1 + IoU)</span>
<span id="cb14-1022"><a href="#cb14-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Differentiable, suitable for optimization</span>
<span id="cb14-1023"><a href="#cb14-1023" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More balanced for small objects</span>
<span id="cb14-1024"><a href="#cb14-1024" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often used in medical imaging and EO segmentation</span>
<span id="cb14-1025"><a href="#cb14-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1026"><a href="#cb14-1026" aria-hidden="true" tabindex="-1"></a>**IoU Loss:**</span>
<span id="cb14-1027"><a href="#cb14-1027" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Directly optimizes intersection over union</span>
<span id="cb14-1028"><a href="#cb14-1028" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less strict than Dice for small discrepancies</span>
<span id="cb14-1029"><a href="#cb14-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1030"><a href="#cb14-1030" aria-hidden="true" tabindex="-1"></a>**For regression:**</span>
<span id="cb14-1031"><a href="#cb14-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1032"><a href="#cb14-1032" aria-hidden="true" tabindex="-1"></a>**Mean Squared Error (MSE):**</span>
<span id="cb14-1033"><a href="#cb14-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1034"><a href="#cb14-1034" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1035"><a href="#cb14-1035" aria-hidden="true" tabindex="-1"></a><span class="in">Loss = (1/n) * Σ(y_true - y_pred)²</span></span>
<span id="cb14-1036"><a href="#cb14-1036" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1037"><a href="#cb14-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1038"><a href="#cb14-1038" aria-hidden="true" tabindex="-1"></a>**Example: Biomass prediction:**</span>
<span id="cb14-1039"><a href="#cb14-1039" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1040"><a href="#cb14-1040" aria-hidden="true" tabindex="-1"></a><span class="in">True: 150 tons/ha</span></span>
<span id="cb14-1041"><a href="#cb14-1041" aria-hidden="true" tabindex="-1"></a><span class="in">Prediction: 140 tons/ha</span></span>
<span id="cb14-1042"><a href="#cb14-1042" aria-hidden="true" tabindex="-1"></a><span class="in">Error: 10 tons/ha</span></span>
<span id="cb14-1043"><a href="#cb14-1043" aria-hidden="true" tabindex="-1"></a><span class="in">Squared Error: 100</span></span>
<span id="cb14-1044"><a href="#cb14-1044" aria-hidden="true" tabindex="-1"></a><span class="in">MSE = 100 (if single sample)</span></span>
<span id="cb14-1045"><a href="#cb14-1045" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1046"><a href="#cb14-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1047"><a href="#cb14-1047" aria-hidden="true" tabindex="-1"></a>**Mean Absolute Error (MAE):**</span>
<span id="cb14-1048"><a href="#cb14-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less sensitive to outliers than MSE</span>
<span id="cb14-1049"><a href="#cb14-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More robust when errors follow non-Gaussian distribution</span>
<span id="cb14-1050"><a href="#cb14-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1051"><a href="#cb14-1051" aria-hidden="true" tabindex="-1"></a>**Huber Loss:**</span>
<span id="cb14-1052"><a href="#cb14-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combination of MSE (small errors) and MAE (large errors)</span>
<span id="cb14-1053"><a href="#cb14-1053" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Robust to outliers while maintaining smooth gradient</span>
<span id="cb14-1054"><a href="#cb14-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1055"><a href="#cb14-1055" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Optimizers</span></span>
<span id="cb14-1056"><a href="#cb14-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1057"><a href="#cb14-1057" aria-hidden="true" tabindex="-1"></a>**Algorithms that adjust weights to minimize loss**</span>
<span id="cb14-1058"><a href="#cb14-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1059"><a href="#cb14-1059" aria-hidden="true" tabindex="-1"></a>**The process:**</span>
<span id="cb14-1060"><a href="#cb14-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1061"><a href="#cb14-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate loss on current batch of data</span>
<span id="cb14-1062"><a href="#cb14-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute gradients (via backpropagation): how should each weight change?</span>
<span id="cb14-1063"><a href="#cb14-1063" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Update weights in direction that reduces loss</span>
<span id="cb14-1064"><a href="#cb14-1064" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Repeat thousands/millions of times across epochs</span>
<span id="cb14-1065"><a href="#cb14-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1066"><a href="#cb14-1066" aria-hidden="true" tabindex="-1"></a>**Common optimizers:**</span>
<span id="cb14-1067"><a href="#cb14-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1068"><a href="#cb14-1068" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Optimizer <span class="pp">|</span> Description <span class="pp">|</span> Learning Rate <span class="pp">|</span> When to Use <span class="pp">|</span></span>
<span id="cb14-1069"><a href="#cb14-1069" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-------------|---------------|-------------|</span></span>
<span id="cb14-1070"><a href="#cb14-1070" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SGD** <span class="pp">|</span> Stochastic Gradient Descent <span class="pp">|</span> Constant or scheduled <span class="pp">|</span> Simple, well-understood, good for fine-tuning <span class="pp">|</span></span>
<span id="cb14-1071"><a href="#cb14-1071" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Adam** <span class="pp">|</span> Adaptive Moment Estimation <span class="pp">|</span> Adaptive per parameter <span class="pp">|</span> Default choice, usually works well <span class="pp">|</span></span>
<span id="cb14-1072"><a href="#cb14-1072" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **AdamW** <span class="pp">|</span> Adam with Weight Decay <span class="pp">|</span> Adaptive <span class="pp">|</span> Improved generalization, Transformers <span class="pp">|</span></span>
<span id="cb14-1073"><a href="#cb14-1073" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **RMSprop** <span class="pp">|</span> Root Mean Square Propagation <span class="pp">|</span> Adaptive <span class="pp">|</span> Good for RNNs/LSTMs <span class="pp">|</span></span>
<span id="cb14-1074"><a href="#cb14-1074" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **AdaGrad** <span class="pp">|</span> Adaptive Gradient <span class="pp">|</span> Adaptive <span class="pp">|</span> Features vary in frequency <span class="pp">|</span></span>
<span id="cb14-1075"><a href="#cb14-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1076"><a href="#cb14-1076" aria-hidden="true" tabindex="-1"></a>**Adam is most popular** because:</span>
<span id="cb14-1077"><a href="#cb14-1077" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adapts learning rate per parameter</span>
<span id="cb14-1078"><a href="#cb14-1078" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines benefits of momentum (accelerates convergence) and adaptive learning</span>
<span id="cb14-1079"><a href="#cb14-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires minimal tuning</span>
<span id="cb14-1080"><a href="#cb14-1080" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works well across diverse problems</span>
<span id="cb14-1081"><a href="#cb14-1081" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Default hyperparameters (lr=0.001, β1=0.9, β2=0.999) often sufficient</span>
<span id="cb14-1082"><a href="#cb14-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1083"><a href="#cb14-1083" aria-hidden="true" tabindex="-1"></a>**Learning Rate Scheduling:**</span>
<span id="cb14-1084"><a href="#cb14-1084" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Step Decay:** Reduce learning rate at fixed intervals</span>
<span id="cb14-1085"><a href="#cb14-1085" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cosine Annealing:** Smooth decay following cosine function</span>
<span id="cb14-1086"><a href="#cb14-1086" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Warm-up:** Gradually increase learning rate at beginning</span>
<span id="cb14-1087"><a href="#cb14-1087" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReduceLROnPlateau:** Reduce when validation loss plateaus</span>
<span id="cb14-1088"><a href="#cb14-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1089"><a href="#cb14-1089" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-1090"><a href="#cb14-1090" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Terminology</span></span>
<span id="cb14-1091"><a href="#cb14-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1092"><a href="#cb14-1092" aria-hidden="true" tabindex="-1"></a>**Epoch:** One complete pass through the entire training dataset</span>
<span id="cb14-1093"><a href="#cb14-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1094"><a href="#cb14-1094" aria-hidden="true" tabindex="-1"></a>**Batch:** Subset of training data processed together before updating weights</span>
<span id="cb14-1095"><a href="#cb14-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1096"><a href="#cb14-1096" aria-hidden="true" tabindex="-1"></a>**Iteration:** One weight update (one batch processed)</span>
<span id="cb14-1097"><a href="#cb14-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1098"><a href="#cb14-1098" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb14-1099"><a href="#cb14-1099" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training data: 10,000 samples</span>
<span id="cb14-1100"><a href="#cb14-1100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Batch size: 100</span>
<span id="cb14-1101"><a href="#cb14-1101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1 epoch = 100 iterations (10,000 / 100)</span>
<span id="cb14-1102"><a href="#cb14-1102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training for 50 epochs = 5,000 iterations</span>
<span id="cb14-1103"><a href="#cb14-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1104"><a href="#cb14-1104" aria-hidden="true" tabindex="-1"></a>**Typical batch sizes for EO:**</span>
<span id="cb14-1105"><a href="#cb14-1105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Images: 16-64 (limited by GPU memory)</span>
<span id="cb14-1106"><a href="#cb14-1106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Patches: 32-128</span>
<span id="cb14-1107"><a href="#cb14-1107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time series samples: 64-256</span>
<span id="cb14-1108"><a href="#cb14-1108" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1109"><a href="#cb14-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1110"><a href="#cb14-1110" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Training Process</span></span>
<span id="cb14-1111"><a href="#cb14-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1112"><a href="#cb14-1112" aria-hidden="true" tabindex="-1"></a>**Iterative improvement:**</span>
<span id="cb14-1113"><a href="#cb14-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1114"><a href="#cb14-1114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1115"><a href="#cb14-1115" aria-hidden="true" tabindex="-1"></a><span class="in">1. Initialize weights (random or pre-trained)</span></span>
<span id="cb14-1116"><a href="#cb14-1116" aria-hidden="true" tabindex="-1"></a><span class="in">2. For each epoch:</span></span>
<span id="cb14-1117"><a href="#cb14-1117" aria-hidden="true" tabindex="-1"></a><span class="in">    For each batch:</span></span>
<span id="cb14-1118"><a href="#cb14-1118" aria-hidden="true" tabindex="-1"></a><span class="in">        a. Forward pass: Compute predictions</span></span>
<span id="cb14-1119"><a href="#cb14-1119" aria-hidden="true" tabindex="-1"></a><span class="in">        b. Calculate loss</span></span>
<span id="cb14-1120"><a href="#cb14-1120" aria-hidden="true" tabindex="-1"></a><span class="in">        c. Backward pass: Compute gradients (backpropagation)</span></span>
<span id="cb14-1121"><a href="#cb14-1121" aria-hidden="true" tabindex="-1"></a><span class="in">        d. Update weights using optimizer</span></span>
<span id="cb14-1122"><a href="#cb14-1122" aria-hidden="true" tabindex="-1"></a><span class="in">    e. Evaluate on validation set</span></span>
<span id="cb14-1123"><a href="#cb14-1123" aria-hidden="true" tabindex="-1"></a><span class="in">    f. Save checkpoint if best performance</span></span>
<span id="cb14-1124"><a href="#cb14-1124" aria-hidden="true" tabindex="-1"></a><span class="in">3. Stop when validation performance plateaus (early stopping)</span></span>
<span id="cb14-1125"><a href="#cb14-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1126"><a href="#cb14-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1127"><a href="#cb14-1127" aria-hidden="true" tabindex="-1"></a>**Monitoring training:**</span>
<span id="cb14-1128"><a href="#cb14-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1129"><a href="#cb14-1129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training loss should decrease** - model learning patterns in training data</span>
<span id="cb14-1130"><a href="#cb14-1130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Validation loss should decrease** - model generalizing to new data</span>
<span id="cb14-1131"><a href="#cb14-1131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**If validation loss increases while training loss decreases:** Overfitting! Apply regularization.</span>
<span id="cb14-1132"><a href="#cb14-1132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**If both losses remain high:** Underfitting. Need more capacity or better features.</span>
<span id="cb14-1133"><a href="#cb14-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1134"><a href="#cb14-1134" aria-hidden="true" tabindex="-1"></a>**Regularization techniques:**</span>
<span id="cb14-1135"><a href="#cb14-1135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dropout:** Randomly deactivate neurons during training</span>
<span id="cb14-1136"><a href="#cb14-1136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Weight Decay (L2):** Penalize large weights</span>
<span id="cb14-1137"><a href="#cb14-1137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data Augmentation:** Artificially expand training data</span>
<span id="cb14-1138"><a href="#cb14-1138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Early Stopping:** Stop training when validation loss stops improving</span>
<span id="cb14-1139"><a href="#cb14-1139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Batch Normalization:** Normalize activations, improves stability</span>
<span id="cb14-1140"><a href="#cb14-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1141"><a href="#cb14-1141" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolutional Neural Networks (CNNs)</span></span>
<span id="cb14-1142"><a href="#cb14-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1143"><a href="#cb14-1143" aria-hidden="true" tabindex="-1"></a>**CNNs are the foundation of modern computer vision and EO analysis**</span>
<span id="cb14-1144"><a href="#cb14-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1145"><a href="#cb14-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Why CNNs Excel at EO</span></span>
<span id="cb14-1146"><a href="#cb14-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1147"><a href="#cb14-1147" aria-hidden="true" tabindex="-1"></a>**Traditional ML:**</span>
<span id="cb14-1148"><a href="#cb14-1148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manual feature engineering needed (NDVI, GLCM textures)</span>
<span id="cb14-1149"><a href="#cb14-1149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limited ability to capture spatial patterns</span>
<span id="cb14-1150"><a href="#cb14-1150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each pixel treated somewhat independently</span>
<span id="cb14-1151"><a href="#cb14-1151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Features fixed before training</span>
<span id="cb14-1152"><a href="#cb14-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1153"><a href="#cb14-1153" aria-hidden="true" tabindex="-1"></a>**CNNs:**</span>
<span id="cb14-1154"><a href="#cb14-1154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automatic feature extraction** from raw pixels</span>
<span id="cb14-1155"><a href="#cb14-1155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spatial awareness** through convolutional filters</span>
<span id="cb14-1156"><a href="#cb14-1156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical learning:** edges → textures → objects → scenes</span>
<span id="cb14-1157"><a href="#cb14-1157" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Translation invariance:** Detects patterns anywhere in image</span>
<span id="cb14-1158"><a href="#cb14-1158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter sharing:** Same filters applied across entire image (efficiency)</span>
<span id="cb14-1159"><a href="#cb14-1159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-scale analysis:** Through pooling and different kernel sizes</span>
<span id="cb14-1160"><a href="#cb14-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1161"><a href="#cb14-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">#### CNN Architecture Components</span></span>
<span id="cb14-1162"><a href="#cb14-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1165"><a href="#cb14-1165" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-1166"><a href="#cb14-1166" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Convolutional Neural Network Architecture for Land Cover Classification</span><span class="ot">"</span></span>
<span id="cb14-1167"><a href="#cb14-1167" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-1168"><a href="#cb14-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1169"><a href="#cb14-1169" aria-hidden="true" tabindex="-1"></a>flowchart LR</span>
<span id="cb14-1170"><a href="#cb14-1170" aria-hidden="true" tabindex="-1"></a>    A[Input Image&lt;br/&gt;Sentinel<span class="dv">-2</span>&lt;br/&gt;<span class="dv">64</span>x64x13 bands] --&gt; B[Conv Layer <span class="dv">1</span>&lt;br/&gt;<span class="dv">32</span> filters <span class="dv">3</span>x3&lt;br/&gt;ReLU activation]</span>
<span id="cb14-1171"><a href="#cb14-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1172"><a href="#cb14-1172" aria-hidden="true" tabindex="-1"></a>    B --&gt; C[Max Pooling&lt;br/&gt;<span class="dv">2</span>x2&lt;br/&gt;<span class="dv">32</span>x32x32]</span>
<span id="cb14-1173"><a href="#cb14-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1174"><a href="#cb14-1174" aria-hidden="true" tabindex="-1"></a>    C --&gt; D[Conv Layer <span class="dv">2</span>&lt;br/&gt;<span class="dv">64</span> filters <span class="dv">3</span>x3&lt;br/&gt;ReLU activation]</span>
<span id="cb14-1175"><a href="#cb14-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1176"><a href="#cb14-1176" aria-hidden="true" tabindex="-1"></a>    D --&gt; E[Max Pooling&lt;br/&gt;<span class="dv">2</span>x2&lt;br/&gt;<span class="dv">16</span>x16x64]</span>
<span id="cb14-1177"><a href="#cb14-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1178"><a href="#cb14-1178" aria-hidden="true" tabindex="-1"></a>    E --&gt; F[Conv Layer <span class="dv">3</span>&lt;br/&gt;<span class="dv">128</span> filters <span class="dv">3</span>x3&lt;br/&gt;ReLU activation]</span>
<span id="cb14-1179"><a href="#cb14-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1180"><a href="#cb14-1180" aria-hidden="true" tabindex="-1"></a>    F --&gt; G[Max Pooling&lt;br/&gt;<span class="dv">2</span>x2&lt;br/&gt;<span class="dv">8</span>x8x128]</span>
<span id="cb14-1181"><a href="#cb14-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1182"><a href="#cb14-1182" aria-hidden="true" tabindex="-1"></a>    G --&gt; H[Flatten&lt;br/&gt;<span class="dv">8192</span> neurons]</span>
<span id="cb14-1183"><a href="#cb14-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1184"><a href="#cb14-1184" aria-hidden="true" tabindex="-1"></a>    H --&gt; I[Dense Layer&lt;br/&gt;<span class="dv">256</span> neurons&lt;br/&gt;ReLU + Dropout]</span>
<span id="cb14-1185"><a href="#cb14-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1186"><a href="#cb14-1186" aria-hidden="true" tabindex="-1"></a>    I --&gt; J[Output Layer&lt;br/&gt;Softmax&lt;br/&gt;<span class="dv">6</span> classes]</span>
<span id="cb14-1187"><a href="#cb14-1187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1188"><a href="#cb14-1188" aria-hidden="true" tabindex="-1"></a>    J --&gt; K[Predictions&lt;br/&gt;Forest: <span class="fl">0.85</span>&lt;br/&gt;Water: <span class="fl">0.05</span>&lt;br/&gt;Urban: <span class="fl">0.03</span>&lt;br/&gt;Agriculture: <span class="fl">0.04</span>&lt;br/&gt;Bare: <span class="fl">0.02</span>&lt;br/&gt;Wetlands: <span class="fl">0.01</span>]</span>
<span id="cb14-1189"><a href="#cb14-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1190"><a href="#cb14-1190" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#e6f3ff,stroke:#0066cc,stroke-width:2px</span></span>
<span id="cb14-1191"><a href="#cb14-1191" aria-hidden="true" tabindex="-1"></a>    style B fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:2px</span></span>
<span id="cb14-1192"><a href="#cb14-1192" aria-hidden="true" tabindex="-1"></a>    style C fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-1193"><a href="#cb14-1193" aria-hidden="true" tabindex="-1"></a>    style D fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:2px</span></span>
<span id="cb14-1194"><a href="#cb14-1194" aria-hidden="true" tabindex="-1"></a>    style E fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-1195"><a href="#cb14-1195" aria-hidden="true" tabindex="-1"></a>    style F fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:2px</span></span>
<span id="cb14-1196"><a href="#cb14-1196" aria-hidden="true" tabindex="-1"></a>    style G fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-1197"><a href="#cb14-1197" aria-hidden="true" tabindex="-1"></a>    style I fill:<span class="co">#e6ffe6,stroke:#00aa44,stroke-width:2px</span></span>
<span id="cb14-1198"><a href="#cb14-1198" aria-hidden="true" tabindex="-1"></a>    style J fill:<span class="co">#e6e6ff,stroke:#6666cc,stroke-width:2px</span></span>
<span id="cb14-1199"><a href="#cb14-1199" aria-hidden="true" tabindex="-1"></a>    style K fill:<span class="co">#ccffcc,stroke:#00aa44,stroke-width:2px</span></span>
<span id="cb14-1200"><a href="#cb14-1200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1201"><a href="#cb14-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1202"><a href="#cb14-1202" aria-hidden="true" tabindex="-1"></a>**Convolutional Layers:**</span>
<span id="cb14-1203"><a href="#cb14-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply learnable filters (kernels) across image</span>
<span id="cb14-1204"><a href="#cb14-1204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each filter detects specific patterns (edges, textures, shapes)</span>
<span id="cb14-1205"><a href="#cb14-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: 3×3 kernel slides across image, computing dot product</span>
<span id="cb14-1206"><a href="#cb14-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple filters per layer (e.g., 64, 128, 256 filters)</span>
<span id="cb14-1207"><a href="#cb14-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride controls movement (stride=1: every pixel, stride=2: every other pixel)</span>
<span id="cb14-1208"><a href="#cb14-1208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding preserves spatial dimensions</span>
<span id="cb14-1209"><a href="#cb14-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1210"><a href="#cb14-1210" aria-hidden="true" tabindex="-1"></a>**Pooling Layers:**</span>
<span id="cb14-1211"><a href="#cb14-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduce spatial dimensions</span>
<span id="cb14-1212"><a href="#cb14-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Max pooling: Take maximum value in window (common)</span>
<span id="cb14-1213"><a href="#cb14-1213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Average pooling: Take average value</span>
<span id="cb14-1214"><a href="#cb14-1214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Increases receptive field</span>
<span id="cb14-1215"><a href="#cb14-1215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provides translation invariance</span>
<span id="cb14-1216"><a href="#cb14-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces computational cost</span>
<span id="cb14-1217"><a href="#cb14-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1218"><a href="#cb14-1218" aria-hidden="true" tabindex="-1"></a>**Fully Connected Layers:**</span>
<span id="cb14-1219"><a href="#cb14-1219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Traditional neural network layers at end</span>
<span id="cb14-1220"><a href="#cb14-1220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flatten spatial features</span>
<span id="cb14-1221"><a href="#cb14-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Perform final classification</span>
<span id="cb14-1222"><a href="#cb14-1222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often replaced by Global Average Pooling in modern architectures</span>
<span id="cb14-1223"><a href="#cb14-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1224"><a href="#cb14-1224" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Popular CNN Architectures for EO</span></span>
<span id="cb14-1225"><a href="#cb14-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1226"><a href="#cb14-1226" aria-hidden="true" tabindex="-1"></a>**VGG Networks (VGG16, VGG19):**</span>
<span id="cb14-1227"><a href="#cb14-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deep architecture with small (3×3) convolutional filters</span>
<span id="cb14-1228"><a href="#cb14-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple, uniform design</span>
<span id="cb14-1229"><a href="#cb14-1229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>16 or 19 layers</span>
<span id="cb14-1230"><a href="#cb14-1230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large memory footprint</span>
<span id="cb14-1231"><a href="#cb14-1231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EO Application:** VGG16 with instance normalization applied for LULC classification</span>
<span id="cb14-1232"><a href="#cb14-1232" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for transfer learning from ImageNet</span>
<span id="cb14-1233"><a href="#cb14-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1234"><a href="#cb14-1234" aria-hidden="true" tabindex="-1"></a>**ResNet (Residual Networks):**</span>
<span id="cb14-1235"><a href="#cb14-1235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Innovation:** Skip connections address vanishing gradient problem</span>
<span id="cb14-1236"><a href="#cb14-1236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables very deep networks (50, 101, 152 layers)</span>
<span id="cb14-1237"><a href="#cb14-1237" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residual blocks: <span class="in">`output = F(x) + x`</span></span>
<span id="cb14-1238"><a href="#cb14-1238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance:** ResNet-18 and ResNet-50 widely used as encoders in semantic segmentation</span>
<span id="cb14-1239"><a href="#cb14-1239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EO Success:** U-Net with ResNet encoder achieved precision 0.943 and recall 0.954 for building extraction</span>
<span id="cb14-1240"><a href="#cb14-1240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Winner architecture in many EO competitions</span>
<span id="cb14-1241"><a href="#cb14-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1242"><a href="#cb14-1242" aria-hidden="true" tabindex="-1"></a>**Inception/GoogLeNet:**</span>
<span id="cb14-1243"><a href="#cb14-1243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale feature extraction</span>
<span id="cb14-1244"><a href="#cb14-1244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Parallel convolutions with different kernel sizes (1×1, 3×3, 5×5)</span>
<span id="cb14-1245"><a href="#cb14-1245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally efficient through 1×1 bottleneck layers</span>
<span id="cb14-1246"><a href="#cb14-1246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EO Application:** Multi-scale land cover classification</span>
<span id="cb14-1247"><a href="#cb14-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1248"><a href="#cb14-1248" aria-hidden="true" tabindex="-1"></a>**EfficientNet:**</span>
<span id="cb14-1249"><a href="#cb14-1249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Innovation:** Compound scaling of depth, width, and resolution</span>
<span id="cb14-1250"><a href="#cb14-1250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimal balance between accuracy and computational efficiency</span>
<span id="cb14-1251"><a href="#cb14-1251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>EfficientNet-B0 to B7 variants</span>
<span id="cb14-1252"><a href="#cb14-1252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**EO Application:** Increasingly popular for resource-constrained applications</span>
<span id="cb14-1253"><a href="#cb14-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mobile deployment, edge computing</span>
<span id="cb14-1254"><a href="#cb14-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1255"><a href="#cb14-1255" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-1256"><a href="#cb14-1256" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transfer Learning for EO</span></span>
<span id="cb14-1257"><a href="#cb14-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1258"><a href="#cb14-1258" aria-hidden="true" tabindex="-1"></a>Pre-trained CNNs (trained on ImageNet) are widely used in EO:</span>
<span id="cb14-1259"><a href="#cb14-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1260"><a href="#cb14-1260" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1261"><a href="#cb14-1261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduce training time</span>
<span id="cb14-1262"><a href="#cb14-1262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improve performance with limited data</span>
<span id="cb14-1263"><a href="#cb14-1263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lower layers learn generic features (edges, textures) applicable across domains</span>
<span id="cb14-1264"><a href="#cb14-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1265"><a href="#cb14-1265" aria-hidden="true" tabindex="-1"></a>**Considerations:**</span>
<span id="cb14-1266"><a href="#cb14-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ImageNet uses RGB images; EO often has more bands</span>
<span id="cb14-1267"><a href="#cb14-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Solutions: Use only RGB bands, or initialize additional channels with pre-trained weights</span>
<span id="cb14-1268"><a href="#cb14-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fine-tune all layers or freeze early layers</span>
<span id="cb14-1269"><a href="#cb14-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1270"><a href="#cb14-1270" aria-hidden="true" tabindex="-1"></a>**Recent Research (2024):**</span>
<span id="cb14-1271"><a href="#cb14-1271" aria-hidden="true" tabindex="-1"></a>Self-supervised pre-training on RS data (SatMAE, SatViT) offers modest improvements over ImageNet in few-shot settings, especially when pre-trained on domain-specific EO data.</span>
<span id="cb14-1272"><a href="#cb14-1272" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1273"><a href="#cb14-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1274"><a href="#cb14-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">### U-Net and Semantic Segmentation</span></span>
<span id="cb14-1275"><a href="#cb14-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1276"><a href="#cb14-1276" aria-hidden="true" tabindex="-1"></a><span class="fu">#### U-Net Architecture</span></span>
<span id="cb14-1277"><a href="#cb14-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1278"><a href="#cb14-1278" aria-hidden="true" tabindex="-1"></a>**Description:** Encoder-decoder architecture with skip connections, originally designed for biomedical image segmentation but widely adopted for Earth Observation.</span>
<span id="cb14-1279"><a href="#cb14-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1282"><a href="#cb14-1282" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-1283"><a href="#cb14-1283" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">U-Net Architecture for Semantic Segmentation (Flood Mapping Example)</span><span class="ot">"</span></span>
<span id="cb14-1284"><a href="#cb14-1284" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-1285"><a href="#cb14-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1286"><a href="#cb14-1286" aria-hidden="true" tabindex="-1"></a>flowchart TD</span>
<span id="cb14-1287"><a href="#cb14-1287" aria-hidden="true" tabindex="-1"></a>    subgraph Encoder[<span class="ot">"</span><span class="st">ENCODER (Contracting Path)</span><span class="ot">"</span>]</span>
<span id="cb14-1288"><a href="#cb14-1288" aria-hidden="true" tabindex="-1"></a>        A[Input&lt;br/&gt;SAR Image&lt;br/&gt;<span class="dv">256</span>x256x2&lt;br/&gt;VV, VH] --&gt; B[Conv <span class="dv">3</span>x3&lt;br/&gt;ReLU&lt;br/&gt;<span class="dv">64</span> filters]</span>
<span id="cb14-1289"><a href="#cb14-1289" aria-hidden="true" tabindex="-1"></a>        B --&gt; C[Conv <span class="dv">3</span>x3&lt;br/&gt;ReLU&lt;br/&gt;<span class="dv">64</span> filters]</span>
<span id="cb14-1290"><a href="#cb14-1290" aria-hidden="true" tabindex="-1"></a>        C --&gt; D[Max Pool <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">128</span>x128x64]</span>
<span id="cb14-1291"><a href="#cb14-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1292"><a href="#cb14-1292" aria-hidden="true" tabindex="-1"></a>        D --&gt; E[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">128</span> filters]</span>
<span id="cb14-1293"><a href="#cb14-1293" aria-hidden="true" tabindex="-1"></a>        E --&gt; F[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">128</span> filters]</span>
<span id="cb14-1294"><a href="#cb14-1294" aria-hidden="true" tabindex="-1"></a>        F --&gt; G[Max Pool <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">64</span>x64x128]</span>
<span id="cb14-1295"><a href="#cb14-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1296"><a href="#cb14-1296" aria-hidden="true" tabindex="-1"></a>        G --&gt; H[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">256</span> filters]</span>
<span id="cb14-1297"><a href="#cb14-1297" aria-hidden="true" tabindex="-1"></a>        H --&gt; I[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">256</span> filters]</span>
<span id="cb14-1298"><a href="#cb14-1298" aria-hidden="true" tabindex="-1"></a>        I --&gt; J[Max Pool <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">32</span>x32x256]</span>
<span id="cb14-1299"><a href="#cb14-1299" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-1300"><a href="#cb14-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1301"><a href="#cb14-1301" aria-hidden="true" tabindex="-1"></a>    subgraph Bottleneck[<span class="ot">"</span><span class="st">BOTTLENECK</span><span class="ot">"</span>]</span>
<span id="cb14-1302"><a href="#cb14-1302" aria-hidden="true" tabindex="-1"></a>        J --&gt; K[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">512</span> filters]</span>
<span id="cb14-1303"><a href="#cb14-1303" aria-hidden="true" tabindex="-1"></a>        K --&gt; L[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">512</span> filters]</span>
<span id="cb14-1304"><a href="#cb14-1304" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-1305"><a href="#cb14-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1306"><a href="#cb14-1306" aria-hidden="true" tabindex="-1"></a>    subgraph Decoder[<span class="ot">"</span><span class="st">DECODER (Expanding Path)</span><span class="ot">"</span>]</span>
<span id="cb14-1307"><a href="#cb14-1307" aria-hidden="true" tabindex="-1"></a>        L --&gt; M[Up-Conv <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">256</span> filters&lt;br/&gt;<span class="dv">64</span>x64x256]</span>
<span id="cb14-1308"><a href="#cb14-1308" aria-hidden="true" tabindex="-1"></a>        M --&gt; N[Concatenate&lt;br/&gt;with I]</span>
<span id="cb14-1309"><a href="#cb14-1309" aria-hidden="true" tabindex="-1"></a>        N --&gt; O[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">256</span> filters]</span>
<span id="cb14-1310"><a href="#cb14-1310" aria-hidden="true" tabindex="-1"></a>        O --&gt; P[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">256</span> filters]</span>
<span id="cb14-1311"><a href="#cb14-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1312"><a href="#cb14-1312" aria-hidden="true" tabindex="-1"></a>        P --&gt; Q[Up-Conv <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">128</span> filters&lt;br/&gt;<span class="dv">128</span>x128x128]</span>
<span id="cb14-1313"><a href="#cb14-1313" aria-hidden="true" tabindex="-1"></a>        Q --&gt; R[Concatenate&lt;br/&gt;with F]</span>
<span id="cb14-1314"><a href="#cb14-1314" aria-hidden="true" tabindex="-1"></a>        R --&gt; S[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">128</span> filters]</span>
<span id="cb14-1315"><a href="#cb14-1315" aria-hidden="true" tabindex="-1"></a>        S --&gt; T[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">128</span> filters]</span>
<span id="cb14-1316"><a href="#cb14-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1317"><a href="#cb14-1317" aria-hidden="true" tabindex="-1"></a>        T --&gt; U[Up-Conv <span class="dv">2</span>x2&lt;br/&gt;<span class="dv">64</span> filters&lt;br/&gt;<span class="dv">256</span>x256x64]</span>
<span id="cb14-1318"><a href="#cb14-1318" aria-hidden="true" tabindex="-1"></a>        U --&gt; V[Concatenate&lt;br/&gt;with C]</span>
<span id="cb14-1319"><a href="#cb14-1319" aria-hidden="true" tabindex="-1"></a>        V --&gt; W[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">64</span> filters]</span>
<span id="cb14-1320"><a href="#cb14-1320" aria-hidden="true" tabindex="-1"></a>        W --&gt; X[Conv <span class="dv">3</span>x3&lt;br/&gt;<span class="dv">64</span> filters]</span>
<span id="cb14-1321"><a href="#cb14-1321" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-1322"><a href="#cb14-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1323"><a href="#cb14-1323" aria-hidden="true" tabindex="-1"></a>    X --&gt; Y[Conv <span class="dv">1</span>x1&lt;br/&gt;<span class="dv">2</span> classes&lt;br/&gt;Sigmoid]</span>
<span id="cb14-1324"><a href="#cb14-1324" aria-hidden="true" tabindex="-1"></a>    Y --&gt; Z[Output&lt;br/&gt;Flood Mask&lt;br/&gt;<span class="dv">256</span>x256x2&lt;br/&gt;Water/No-Water]</span>
<span id="cb14-1325"><a href="#cb14-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1326"><a href="#cb14-1326" aria-hidden="true" tabindex="-1"></a>    I -.-&gt;|Skip Connection| N</span>
<span id="cb14-1327"><a href="#cb14-1327" aria-hidden="true" tabindex="-1"></a>    F -.-&gt;|Skip Connection| R</span>
<span id="cb14-1328"><a href="#cb14-1328" aria-hidden="true" tabindex="-1"></a>    C -.-&gt;|Skip Connection| V</span>
<span id="cb14-1329"><a href="#cb14-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1330"><a href="#cb14-1330" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#e6f3ff,stroke:#0066cc,stroke-width:2px</span></span>
<span id="cb14-1331"><a href="#cb14-1331" aria-hidden="true" tabindex="-1"></a>    style Encoder fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:2px</span></span>
<span id="cb14-1332"><a href="#cb14-1332" aria-hidden="true" tabindex="-1"></a>    style Bottleneck fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-1333"><a href="#cb14-1333" aria-hidden="true" tabindex="-1"></a>    style Decoder fill:<span class="co">#e6ffe6,stroke:#00aa44,stroke-width:2px</span></span>
<span id="cb14-1334"><a href="#cb14-1334" aria-hidden="true" tabindex="-1"></a>    style Z fill:<span class="co">#ccffcc,stroke:#00aa44,stroke-width:3px</span></span>
<span id="cb14-1335"><a href="#cb14-1335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1336"><a href="#cb14-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1337"><a href="#cb14-1337" aria-hidden="true" tabindex="-1"></a>**Architecture:**</span>
<span id="cb14-1338"><a href="#cb14-1338" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Encoder (Contracting Path):** Progressively downsamples input (convolutional + pooling), capturing context</span>
<span id="cb14-1339"><a href="#cb14-1339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decoder (Expanding Path):** Upsamples features (transpose convolution), enabling precise localization</span>
<span id="cb14-1340"><a href="#cb14-1340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Skip Connections:** Concatenate encoder features with decoder at same resolution, preserving spatial information</span>
<span id="cb14-1341"><a href="#cb14-1341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fully symmetric structure</span>
<span id="cb14-1342"><a href="#cb14-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1343"><a href="#cb14-1343" aria-hidden="true" tabindex="-1"></a>**Why U-Net Works Well:**</span>
<span id="cb14-1344"><a href="#cb14-1344" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Skip connections preserve fine-grained spatial information lost during downsampling</span>
<span id="cb14-1345"><a href="#cb14-1345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works with relatively small training datasets (important for EO where labels are expensive)</span>
<span id="cb14-1346"><a href="#cb14-1346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>End-to-end pixel-wise predictions</span>
<span id="cb14-1347"><a href="#cb14-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale feature fusion</span>
<span id="cb14-1348"><a href="#cb14-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1349"><a href="#cb14-1349" aria-hidden="true" tabindex="-1"></a>**Applications in EO:**</span>
<span id="cb14-1350"><a href="#cb14-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover semantic segmentation</span>
<span id="cb14-1351"><a href="#cb14-1351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building footprint extraction</span>
<span id="cb14-1352"><a href="#cb14-1352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Road network mapping</span>
<span id="cb14-1353"><a href="#cb14-1353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop field delineation</span>
<span id="cb14-1354"><a href="#cb14-1354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Water body detection</span>
<span id="cb14-1355"><a href="#cb14-1355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood extent mapping</span>
<span id="cb14-1356"><a href="#cb14-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1357"><a href="#cb14-1357" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-1358"><a href="#cb14-1358" aria-hidden="true" tabindex="-1"></a>**Philippine Case Study: Benguet Province Deforestation**</span>
<span id="cb14-1359"><a href="#cb14-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1360"><a href="#cb14-1360" aria-hidden="true" tabindex="-1"></a>**Study Details:**</span>
<span id="cb14-1361"><a href="#cb14-1361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Location: Benguet Province tropical montane forest</span>
<span id="cb14-1362"><a href="#cb14-1362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time period: 2015 to early 2022</span>
<span id="cb14-1363"><a href="#cb14-1363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Total deforestation detected: 417.93 km²</span>
<span id="cb14-1364"><a href="#cb14-1364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Significance:** First deep learning application in Southeast Asian montane forests</span>
<span id="cb14-1365"><a href="#cb14-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1366"><a href="#cb14-1366" aria-hidden="true" tabindex="-1"></a>**Methods:**</span>
<span id="cb14-1367"><a href="#cb14-1367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-1 SAR and Sentinel-2 optical fusion</span>
<span id="cb14-1368"><a href="#cb14-1368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net deep learning architecture</span>
<span id="cb14-1369"><a href="#cb14-1369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Comparison with Random Forest and K-Nearest Neighbors</span>
<span id="cb14-1370"><a href="#cb14-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1371"><a href="#cb14-1371" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1372"><a href="#cb14-1372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy: 99.73%** for binary forest/non-forest classification</span>
<span id="cb14-1373"><a href="#cb14-1373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Outperformed traditional ML methods</span>
<span id="cb14-1374"><a href="#cb14-1374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validated effectiveness of multi-sensor data fusion</span>
<span id="cb14-1375"><a href="#cb14-1375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Demonstrated U-Net suitability for tropical conditions</span>
<span id="cb14-1376"><a href="#cb14-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1377"><a href="#cb14-1377" aria-hidden="true" tabindex="-1"></a>**Technology Advantages:**</span>
<span id="cb14-1378"><a href="#cb14-1378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SAR cloud-penetrating capability fills observational gap in tropics</span>
<span id="cb14-1379"><a href="#cb14-1379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal analysis detects gradual and abrupt changes</span>
<span id="cb14-1380"><a href="#cb14-1380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automated processing enables continuous monitoring</span>
<span id="cb14-1381"><a href="#cb14-1381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scalable to national level</span>
<span id="cb14-1382"><a href="#cb14-1382" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1383"><a href="#cb14-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1384"><a href="#cb14-1384" aria-hidden="true" tabindex="-1"></a>**U-Net Variants and Improvements:**</span>
<span id="cb14-1385"><a href="#cb14-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1386"><a href="#cb14-1386" aria-hidden="true" tabindex="-1"></a>**UNet++:**</span>
<span id="cb14-1387"><a href="#cb14-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nested skip connections for improved gradient flow</span>
<span id="cb14-1388"><a href="#cb14-1388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dense skip pathways</span>
<span id="cb14-1389"><a href="#cb14-1389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better feature aggregation</span>
<span id="cb14-1390"><a href="#cb14-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1391"><a href="#cb14-1391" aria-hidden="true" tabindex="-1"></a>**Attention U-Net:**</span>
<span id="cb14-1392"><a href="#cb14-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Incorporates attention mechanisms to focus on relevant features</span>
<span id="cb14-1393"><a href="#cb14-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention gates highlight salient features</span>
<span id="cb14-1394"><a href="#cb14-1394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved performance on complex scenes</span>
<span id="cb14-1395"><a href="#cb14-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1396"><a href="#cb14-1396" aria-hidden="true" tabindex="-1"></a>**3D U-Net:**</span>
<span id="cb14-1397"><a href="#cb14-1397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extends to volumetric data or multi-temporal stacks</span>
<span id="cb14-1398"><a href="#cb14-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal convolutions for time series</span>
<span id="cb14-1399"><a href="#cb14-1399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Crop monitoring, change detection</span>
<span id="cb14-1400"><a href="#cb14-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1401"><a href="#cb14-1401" aria-hidden="true" tabindex="-1"></a>**U-Net with Advanced Encoders:**</span>
<span id="cb14-1402"><a href="#cb14-1402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**U-Net with ResNet encoder:** Combines U-Net decoder with ResNet encoder</span>
<span id="cb14-1403"><a href="#cb14-1403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**U-Net with SK-ResNeXt encoder:** Integrates selective kernel and ResNeXt for enhanced feature extraction</span>
<span id="cb14-1404"><a href="#cb14-1404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**UNetFormer:** Hybrid CNN encoder + Transformer decoder (discussed below)</span>
<span id="cb14-1405"><a href="#cb14-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1406"><a href="#cb14-1406" aria-hidden="true" tabindex="-1"></a>**Performance Metrics:**</span>
<span id="cb14-1407"><a href="#cb14-1407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mean IoU (mIoU): Average IoU across all classes</span>
<span id="cb14-1408"><a href="#cb14-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>F1-Score per class</span>
<span id="cb14-1409"><a href="#cb14-1409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Boundary accuracy for precise delineation</span>
<span id="cb14-1410"><a href="#cb14-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1411"><a href="#cb14-1411" aria-hidden="true" tabindex="-1"></a><span class="fu">### DeepLab Family</span></span>
<span id="cb14-1412"><a href="#cb14-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1413"><a href="#cb14-1413" aria-hidden="true" tabindex="-1"></a>**DeepLab:** State-of-the-art semantic segmentation architecture family</span>
<span id="cb14-1414"><a href="#cb14-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1415"><a href="#cb14-1415" aria-hidden="true" tabindex="-1"></a>**Key Innovations:**</span>
<span id="cb14-1416"><a href="#cb14-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1417"><a href="#cb14-1417" aria-hidden="true" tabindex="-1"></a>**Atrous (Dilated) Convolutions:**</span>
<span id="cb14-1418"><a href="#cb14-1418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enlarge receptive field without losing resolution</span>
<span id="cb14-1419"><a href="#cb14-1419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Insert "holes" in convolution kernel</span>
<span id="cb14-1420"><a href="#cb14-1420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capture multi-scale context efficiently</span>
<span id="cb14-1421"><a href="#cb14-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1422"><a href="#cb14-1422" aria-hidden="true" tabindex="-1"></a>**Atrous Spatial Pyramid Pooling (ASPP):**</span>
<span id="cb14-1423"><a href="#cb14-1423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Parallel atrous convolutions with different rates</span>
<span id="cb14-1424"><a href="#cb14-1424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures features at multiple scales</span>
<span id="cb14-1425"><a href="#cb14-1425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Aggregates information from different receptive fields</span>
<span id="cb14-1426"><a href="#cb14-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1427"><a href="#cb14-1427" aria-hidden="true" tabindex="-1"></a>**Encoder-Decoder Structure (DeepLabv3+):**</span>
<span id="cb14-1428"><a href="#cb14-1428" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Similar to U-Net philosophy</span>
<span id="cb14-1429"><a href="#cb14-1429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines ASPP with decoder for refined boundaries</span>
<span id="cb14-1430"><a href="#cb14-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1431"><a href="#cb14-1431" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1432"><a href="#cb14-1432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DeepLabv3+ shows superior performance compared to standard U-Net on many benchmarks</span>
<span id="cb14-1433"><a href="#cb14-1433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved Mean IoU</span>
<span id="cb14-1434"><a href="#cb14-1434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better boundary delineation</span>
<span id="cb14-1435"><a href="#cb14-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1436"><a href="#cb14-1436" aria-hidden="true" tabindex="-1"></a>**EO Applications:**</span>
<span id="cb14-1437"><a href="#cb14-1437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large-scale land cover mapping</span>
<span id="cb14-1438"><a href="#cb14-1438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Urban scene segmentation</span>
<span id="cb14-1439"><a href="#cb14-1439" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Agricultural field boundaries</span>
<span id="cb14-1440"><a href="#cb14-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1441"><a href="#cb14-1441" aria-hidden="true" tabindex="-1"></a>**Comparison: U-Net vs. DeepLab**</span>
<span id="cb14-1442"><a href="#cb14-1442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**U-Net:** Better with limited data, simpler architecture, faster training</span>
<span id="cb14-1443"><a href="#cb14-1443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepLab:** Better overall performance, more complex, requires more data</span>
<span id="cb14-1444"><a href="#cb14-1444" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Both:** Widely used in EO, choice depends on data availability and computational resources</span>
<span id="cb14-1445"><a href="#cb14-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1446"><a href="#cb14-1446" aria-hidden="true" tabindex="-1"></a><span class="fu">### Vision Transformers (ViTs)</span></span>
<span id="cb14-1447"><a href="#cb14-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1448"><a href="#cb14-1448" aria-hidden="true" tabindex="-1"></a>**Paradigm shift from convolutions to self-attention mechanisms**</span>
<span id="cb14-1449"><a href="#cb14-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1450"><a href="#cb14-1450" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Fundamentals</span></span>
<span id="cb14-1451"><a href="#cb14-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1452"><a href="#cb14-1452" aria-hidden="true" tabindex="-1"></a>**Architecture:**</span>
<span id="cb14-1453"><a href="#cb14-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1454"><a href="#cb14-1454" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Patch Embedding:** Divide image into fixed-size patches (e.g., 16×16 pixels)</span>
<span id="cb14-1455"><a href="#cb14-1455" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Linear Projection:** Flatten patches and project to embedding dimension (e.g., 768-D)</span>
<span id="cb14-1456"><a href="#cb14-1456" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Positional Encoding:** Add learnable position information to preserve spatial relationships</span>
<span id="cb14-1457"><a href="#cb14-1457" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Transformer Encoder:** Stack of multi-head self-attention and feed-forward layers</span>
<span id="cb14-1458"><a href="#cb14-1458" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Classification Head:** MLP (Multi-Layer Perceptron) for final prediction</span>
<span id="cb14-1459"><a href="#cb14-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1460"><a href="#cb14-1460" aria-hidden="true" tabindex="-1"></a>**Self-Attention Mechanism:**</span>
<span id="cb14-1461"><a href="#cb14-1461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models relationships between all image patches</span>
<span id="cb14-1462"><a href="#cb14-1462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each patch "attends to" all other patches</span>
<span id="cb14-1463"><a href="#cb14-1463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learns which patches are relevant for prediction</span>
<span id="cb14-1464"><a href="#cb14-1464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures long-range dependencies</span>
<span id="cb14-1465"><a href="#cb14-1465" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adaptively focuses on informative regions</span>
<span id="cb14-1466"><a href="#cb14-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1467"><a href="#cb14-1467" aria-hidden="true" tabindex="-1"></a>**Mathematical Formulation:**</span>
<span id="cb14-1468"><a href="#cb14-1468" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1469"><a href="#cb14-1469" aria-hidden="true" tabindex="-1"></a><span class="in">Attention(Q, K, V) = softmax(QK^T / √d_k) V</span></span>
<span id="cb14-1470"><a href="#cb14-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1471"><a href="#cb14-1471" aria-hidden="true" tabindex="-1"></a><span class="in">Q = Query (what am I looking for?)</span></span>
<span id="cb14-1472"><a href="#cb14-1472" aria-hidden="true" tabindex="-1"></a><span class="in">K = Key (what do I contain?)</span></span>
<span id="cb14-1473"><a href="#cb14-1473" aria-hidden="true" tabindex="-1"></a><span class="in">V = Value (what information do I pass?)</span></span>
<span id="cb14-1474"><a href="#cb14-1474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1475"><a href="#cb14-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1476"><a href="#cb14-1476" aria-hidden="true" tabindex="-1"></a>**Multi-Head Attention:**</span>
<span id="cb14-1477"><a href="#cb14-1477" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple attention mechanisms in parallel</span>
<span id="cb14-1478"><a href="#cb14-1478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each head learns different relationships</span>
<span id="cb14-1479"><a href="#cb14-1479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Aggregate outputs for richer representation</span>
<span id="cb14-1480"><a href="#cb14-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1481"><a href="#cb14-1481" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Advantages for EO</span></span>
<span id="cb14-1482"><a href="#cb14-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1483"><a href="#cb14-1483" aria-hidden="true" tabindex="-1"></a>**Global Context Modeling:**</span>
<span id="cb14-1484"><a href="#cb14-1484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention mechanism captures relationships across entire image from early layers</span>
<span id="cb14-1485"><a href="#cb14-1485" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CNNs build up receptive field gradually through layers</span>
<span id="cb14-1486"><a href="#cb14-1486" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Particularly valuable for EO where context matters (e.g., urban vs. rural forest)</span>
<span id="cb14-1487"><a href="#cb14-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1488"><a href="#cb14-1488" aria-hidden="true" tabindex="-1"></a>**Long-Range Dependencies:**</span>
<span id="cb14-1489"><a href="#cb14-1489" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can relate distant image regions</span>
<span id="cb14-1490"><a href="#cb14-1490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: Recognizing rice paddy requires context of surrounding infrastructure, water bodies</span>
<span id="cb14-1491"><a href="#cb14-1491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1492"><a href="#cb14-1492" aria-hidden="true" tabindex="-1"></a>**Effective for Large-Scale Imagery:**</span>
<span id="cb14-1493"><a href="#cb14-1493" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scales well to high-resolution satellite images</span>
<span id="cb14-1494"><a href="#cb14-1494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Efficient self-attention variants reduce computational cost</span>
<span id="cb14-1495"><a href="#cb14-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1496"><a href="#cb14-1496" aria-hidden="true" tabindex="-1"></a>**Strong Transfer Learning:**</span>
<span id="cb14-1497"><a href="#cb14-1497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained ViTs transfer well across tasks</span>
<span id="cb14-1498"><a href="#cb14-1498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SatViT: Pre-trained on 1.3 million satellite-derived RS images</span>
<span id="cb14-1499"><a href="#cb14-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1500"><a href="#cb14-1500" aria-hidden="true" tabindex="-1"></a>**Handles Variable Input Sizes:**</span>
<span id="cb14-1501"><a href="#cb14-1501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flexible patch-based approach</span>
<span id="cb14-1502"><a href="#cb14-1502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can adapt to different image resolutions</span>
<span id="cb14-1503"><a href="#cb14-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1504"><a href="#cb14-1504" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Variants for Remote Sensing</span></span>
<span id="cb14-1505"><a href="#cb14-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1506"><a href="#cb14-1506" aria-hidden="true" tabindex="-1"></a>**Swin Transformer:**</span>
<span id="cb14-1507"><a href="#cb14-1507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Innovation:** Hierarchical architecture with shifted windows</span>
<span id="cb14-1508"><a href="#cb14-1508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Local attention within windows (efficient computation)</span>
<span id="cb14-1509"><a href="#cb14-1509" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shifted window scheme enables cross-window connections</span>
<span id="cb14-1510"><a href="#cb14-1510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale feature representation (like CNN feature pyramids)</span>
<span id="cb14-1511"><a href="#cb14-1511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art performance on many EO benchmarks</span>
<span id="cb14-1512"><a href="#cb14-1512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1513"><a href="#cb14-1513" aria-hidden="true" tabindex="-1"></a>**ViT with Spectral Adaptation:**</span>
<span id="cb14-1514"><a href="#cb14-1514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modified patch embedding for multi-spectral inputs</span>
<span id="cb14-1515"><a href="#cb14-1515" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles variable number of spectral bands (not just RGB)</span>
<span id="cb14-1516"><a href="#cb14-1516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-training on large satellite image datasets</span>
<span id="cb14-1517"><a href="#cb14-1517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Hyperspectral classification, multi-sensor fusion</span>
<span id="cb14-1518"><a href="#cb14-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1519"><a href="#cb14-1519" aria-hidden="true" tabindex="-1"></a>**SatViT:**</span>
<span id="cb14-1520"><a href="#cb14-1520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained on 1.3 million satellite-derived RS images</span>
<span id="cb14-1521"><a href="#cb14-1521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Domain-specific Vision Transformer for remote sensing</span>
<span id="cb14-1522"><a href="#cb14-1522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved transfer learning performance over ImageNet pre-training</span>
<span id="cb14-1523"><a href="#cb14-1523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publicly available for EO community</span>
<span id="cb14-1524"><a href="#cb14-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1525"><a href="#cb14-1525" aria-hidden="true" tabindex="-1"></a>**MS-CLIP (IBM, 2024):**</span>
<span id="cb14-1526"><a href="#cb14-1526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First vision-language model for multi-spectral Sentinel-2 data</span>
<span id="cb14-1527"><a href="#cb14-1527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adapts CLIP dual-encoder architecture for 10+ spectral bands</span>
<span id="cb14-1528"><a href="#cb14-1528" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables zero-shot classification and image-text retrieval</span>
<span id="cb14-1529"><a href="#cb14-1529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example:** "Show me images with dense vegetation" without explicit classification</span>
<span id="cb14-1530"><a href="#cb14-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1531"><a href="#cb14-1531" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Challenges</span></span>
<span id="cb14-1532"><a href="#cb14-1532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1533"><a href="#cb14-1533" aria-hidden="true" tabindex="-1"></a>**Data Requirements:**</span>
<span id="cb14-1534"><a href="#cb14-1534" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ViTs require large training datasets (millions of samples)</span>
<span id="cb14-1535"><a href="#cb14-1535" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less effective with small datasets compared to CNNs</span>
<span id="cb14-1536"><a href="#cb14-1536" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Solution: Transfer learning from pre-trained models (SatViT, ImageNet)</span>
<span id="cb14-1537"><a href="#cb14-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1538"><a href="#cb14-1538" aria-hidden="true" tabindex="-1"></a>**Computational Cost:**</span>
<span id="cb14-1539"><a href="#cb14-1539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-attention quadratic complexity in number of patches</span>
<span id="cb14-1540"><a href="#cb14-1540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Memory intensive for high-resolution images</span>
<span id="cb14-1541"><a href="#cb14-1541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Solutions: Swin Transformer (local attention), efficient attention mechanisms</span>
<span id="cb14-1542"><a href="#cb14-1542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1543"><a href="#cb14-1543" aria-hidden="true" tabindex="-1"></a>**Interpretability:**</span>
<span id="cb14-1544"><a href="#cb14-1544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention maps provide some interpretability</span>
<span id="cb14-1545"><a href="#cb14-1545" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can visualize which patches model focuses on</span>
<span id="cb14-1546"><a href="#cb14-1546" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Still less intuitive than CNN filter visualizations</span>
<span id="cb14-1547"><a href="#cb14-1547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1548"><a href="#cb14-1548" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hybrid Architectures: UNetFormer</span></span>
<span id="cb14-1549"><a href="#cb14-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1550"><a href="#cb14-1550" aria-hidden="true" tabindex="-1"></a>**UNetFormer:** Combines CNN encoders with Transformer decoders</span>
<span id="cb14-1551"><a href="#cb14-1551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1552"><a href="#cb14-1552" aria-hidden="true" tabindex="-1"></a>**Description:**</span>
<span id="cb14-1553"><a href="#cb14-1553" aria-hidden="true" tabindex="-1"></a>Hybrid architecture leveraging strengths of both CNNs and Transformers</span>
<span id="cb14-1554"><a href="#cb14-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1555"><a href="#cb14-1555" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb14-1556"><a href="#cb14-1556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CNN Encoder:** ResNet18 captures local spatial features efficiently</span>
<span id="cb14-1557"><a href="#cb14-1557" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transformer Decoder:** Models global context and long-range dependencies</span>
<span id="cb14-1558"><a href="#cb14-1558" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hybrid Design:** Balances computational efficiency with modeling capacity</span>
<span id="cb14-1559"><a href="#cb14-1559" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Skip connections from encoder to decoder (like U-Net)</span>
<span id="cb14-1560"><a href="#cb14-1560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1561"><a href="#cb14-1561" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1562"><a href="#cb14-1562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art on remote sensing semantic segmentation benchmarks</span>
<span id="cb14-1563"><a href="#cb14-1563" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Particularly effective for urban scene imagery</span>
<span id="cb14-1564"><a href="#cb14-1564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Outperforms pure CNN and pure Transformer approaches on many tasks</span>
<span id="cb14-1565"><a href="#cb14-1565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1566"><a href="#cb14-1566" aria-hidden="true" tabindex="-1"></a>**Related Architectures:**</span>
<span id="cb14-1567"><a href="#cb14-1567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**UNeXt:** Efficient network optimizing depth, width, and resolution</span>
<span id="cb14-1568"><a href="#cb14-1568" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**UNetFormer with boundary enhancement:** Multi-scale approach for improved edge detection</span>
<span id="cb14-1569"><a href="#cb14-1569" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segformer:** Transformer encoder + lightweight decoder</span>
<span id="cb14-1570"><a href="#cb14-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1571"><a href="#cb14-1571" aria-hidden="true" tabindex="-1"></a>**When to Use:**</span>
<span id="cb14-1572"><a href="#cb14-1572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex EO scenes requiring both local detail and global context</span>
<span id="cb14-1573"><a href="#cb14-1573" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic segmentation tasks with diverse object scales</span>
<span id="cb14-1574"><a href="#cb14-1574" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When computational resources allow (more expensive than standard U-Net)</span>
<span id="cb14-1575"><a href="#cb14-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1576"><a href="#cb14-1576" aria-hidden="true" tabindex="-1"></a><span class="fu">### Temporal Models for Time Series</span></span>
<span id="cb14-1577"><a href="#cb14-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1578"><a href="#cb14-1578" aria-hidden="true" tabindex="-1"></a>**Multi-temporal satellite data captures dynamic processes - temporal models extract these patterns**</span>
<span id="cb14-1579"><a href="#cb14-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1580"><a href="#cb14-1580" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LSTM and GRU</span></span>
<span id="cb14-1581"><a href="#cb14-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1582"><a href="#cb14-1582" aria-hidden="true" tabindex="-1"></a>**LSTM (Long Short-Term Memory):**</span>
<span id="cb14-1583"><a href="#cb14-1583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Purpose:** Temporal pattern learning in sequential data</span>
<span id="cb14-1584"><a href="#cb14-1584" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Architecture:** Recurrent neural network with gating mechanisms</span>
<span id="cb14-1585"><a href="#cb14-1585" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gates:** Input gate, forget gate, output gate, cell state</span>
<span id="cb14-1586"><a href="#cb14-1586" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantage:** Learns long-term dependencies, avoids vanishing gradient problem of vanilla RNNs</span>
<span id="cb14-1587"><a href="#cb14-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1588"><a href="#cb14-1588" aria-hidden="true" tabindex="-1"></a>**Applications in EO:**</span>
<span id="cb14-1589"><a href="#cb14-1589" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Time series classification:** Crop type mapping from multi-temporal NDVI</span>
<span id="cb14-1590"><a href="#cb14-1590" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Phenology monitoring:** Extracting growing season characteristics</span>
<span id="cb14-1591"><a href="#cb14-1591" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yield prediction:** Forecasting crop yields from vegetation index time series</span>
<span id="cb14-1592"><a href="#cb14-1592" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Change detection:** Detecting disturbances in forest time series</span>
<span id="cb14-1593"><a href="#cb14-1593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Weather forecasting:** Climate variables prediction</span>
<span id="cb14-1594"><a href="#cb14-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1595"><a href="#cb14-1595" aria-hidden="true" tabindex="-1"></a>**GRU (Gated Recurrent Unit):**</span>
<span id="cb14-1596"><a href="#cb14-1596" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simplified version of LSTM</span>
<span id="cb14-1597"><a href="#cb14-1597" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fewer parameters (faster training)</span>
<span id="cb14-1598"><a href="#cb14-1598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often comparable performance to LSTM</span>
<span id="cb14-1599"><a href="#cb14-1599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good choice when computational resources are limited</span>
<span id="cb14-1600"><a href="#cb14-1600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1601"><a href="#cb14-1601" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1602"><a href="#cb14-1602" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Most Used:** RNNs applied in &gt;22% of EO time series studies</span>
<span id="cb14-1603"><a href="#cb14-1603" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LSTMs Preferred:** Used in &gt;40% of crop yield prediction studies</span>
<span id="cb14-1604"><a href="#cb14-1604" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy:** R² &gt; 0.93 for corn and soybean yield prediction</span>
<span id="cb14-1605"><a href="#cb14-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1606"><a href="#cb14-1606" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-1607"><a href="#cb14-1607" aria-hidden="true" tabindex="-1"></a>**Philippine Application: Crop Yield Forecasting with LSTM**</span>
<span id="cb14-1608"><a href="#cb14-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1609"><a href="#cb14-1609" aria-hidden="true" tabindex="-1"></a>**PRiSM Enhanced with Deep Learning:**</span>
<span id="cb14-1610"><a href="#cb14-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1611"><a href="#cb14-1611" aria-hidden="true" tabindex="-1"></a>**Data:**</span>
<span id="cb14-1612"><a href="#cb14-1612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal Sentinel-1 SAR backscatter (VV, VH polarizations)</span>
<span id="cb14-1613"><a href="#cb14-1613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 NDVI time series</span>
<span id="cb14-1614"><a href="#cb14-1614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PAGASA weather data (rainfall, temperature)</span>
<span id="cb14-1615"><a href="#cb14-1615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Historical yield records from PhilRice</span>
<span id="cb14-1616"><a href="#cb14-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1617"><a href="#cb14-1617" aria-hidden="true" tabindex="-1"></a>**Approach:**</span>
<span id="cb14-1618"><a href="#cb14-1618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LSTM network processes time series sequentially</span>
<span id="cb14-1619"><a href="#cb14-1619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures phenological patterns (planting, vegetative growth, reproductive phase, maturity)</span>
<span id="cb14-1620"><a href="#cb14-1620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integrates weather variables as auxiliary inputs</span>
<span id="cb14-1621"><a href="#cb14-1621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trained on multi-year data across provinces</span>
<span id="cb14-1622"><a href="#cb14-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1623"><a href="#cb14-1623" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1624"><a href="#cb14-1624" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Earlier and more accurate yield forecasts than statistical models</span>
<span id="cb14-1625"><a href="#cb14-1625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mid-season prediction (2-3 months before harvest) with acceptable accuracy</span>
<span id="cb14-1626"><a href="#cb14-1626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with PRiSM for operational deployment</span>
<span id="cb14-1627"><a href="#cb14-1627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1628"><a href="#cb14-1628" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1629"><a href="#cb14-1629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Food security early warning</span>
<span id="cb14-1630"><a href="#cb14-1630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop insurance (PCIC)</span>
<span id="cb14-1631"><a href="#cb14-1631" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Agricultural planning and market stabilization</span>
<span id="cb14-1632"><a href="#cb14-1632" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Disaster impact assessment</span>
<span id="cb14-1633"><a href="#cb14-1633" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1634"><a href="#cb14-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1635"><a href="#cb14-1635" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ConvLSTM</span></span>
<span id="cb14-1636"><a href="#cb14-1636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1637"><a href="#cb14-1637" aria-hidden="true" tabindex="-1"></a>**ConvLSTM:** Combines spatial convolutions with temporal LSTM</span>
<span id="cb14-1638"><a href="#cb14-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1639"><a href="#cb14-1639" aria-hidden="true" tabindex="-1"></a>**Architecture:**</span>
<span id="cb14-1640"><a href="#cb14-1640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Replaces matrix multiplications in LSTM with convolutional operations</span>
<span id="cb14-1641"><a href="#cb14-1641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Preserves spatial structure throughout temporal modeling</span>
<span id="cb14-1642"><a href="#cb14-1642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: 3D tensor (time, height, width)</span>
<span id="cb14-1643"><a href="#cb14-1643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Spatial predictions over time</span>
<span id="cb14-1644"><a href="#cb14-1644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1645"><a href="#cb14-1645" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1646"><a href="#cb14-1646" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures both spatial and temporal patterns simultaneously</span>
<span id="cb14-1647"><a href="#cb14-1647" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More parameter efficient than separate spatial and temporal models</span>
<span id="cb14-1648"><a href="#cb14-1648" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>End-to-end learning</span>
<span id="cb14-1649"><a href="#cb14-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1650"><a href="#cb14-1650" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1651"><a href="#cb14-1651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weather forecasting and precipitation nowcasting</span>
<span id="cb14-1652"><a href="#cb14-1652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood prediction from time series of meteorological variables</span>
<span id="cb14-1653"><a href="#cb14-1653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop monitoring with spatial context</span>
<span id="cb14-1654"><a href="#cb14-1654" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spatiotemporal land cover change</span>
<span id="cb14-1655"><a href="#cb14-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1656"><a href="#cb14-1656" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Temporal Attention</span></span>
<span id="cb14-1657"><a href="#cb14-1657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1658"><a href="#cb14-1658" aria-hidden="true" tabindex="-1"></a>**Lightweight Temporal Attention Encoder (L-TAE):**</span>
<span id="cb14-1659"><a href="#cb14-1659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Innovation:** Distributes channels among compact attention heads operating in parallel</span>
<span id="cb14-1660"><a href="#cb14-1660" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Outperforms RNNs with fewer parameters and reduced computational complexity</span>
<span id="cb14-1661"><a href="#cb14-1661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Particularly effective for satellite image time series (SITS) classification</span>
<span id="cb14-1662"><a href="#cb14-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1663"><a href="#cb14-1663" aria-hidden="true" tabindex="-1"></a>**Multi-Head Temporal Attention:**</span>
<span id="cb14-1664"><a href="#cb14-1664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each head attends to different temporal patterns</span>
<span id="cb14-1665"><a href="#cb14-1665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn complementary temporal representations</span>
<span id="cb14-1666"><a href="#cb14-1666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Aggregate outputs for final prediction</span>
<span id="cb14-1667"><a href="#cb14-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1668"><a href="#cb14-1668" aria-hidden="true" tabindex="-1"></a>**Advantages over LSTMs:**</span>
<span id="cb14-1669"><a href="#cb14-1669" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Parallelizable (faster training)</span>
<span id="cb14-1670"><a href="#cb14-1670" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Direct access to all time steps (no sequential bottleneck)</span>
<span id="cb14-1671"><a href="#cb14-1671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention weights provide interpretability (which dates are important?)</span>
<span id="cb14-1672"><a href="#cb14-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1673"><a href="#cb14-1673" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1674"><a href="#cb14-1674" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop type classification from Sentinel-2 time series</span>
<span id="cb14-1675"><a href="#cb14-1675" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover change detection</span>
<span id="cb14-1676"><a href="#cb14-1676" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Phenology extraction</span>
<span id="cb14-1677"><a href="#cb14-1677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1678"><a href="#cb14-1678" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Temporal Transformers</span></span>
<span id="cb14-1679"><a href="#cb14-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1680"><a href="#cb14-1680" aria-hidden="true" tabindex="-1"></a>**Transformer for Time Series:**</span>
<span id="cb14-1681"><a href="#cb14-1681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-attention over temporal sequence</span>
<span id="cb14-1682"><a href="#cb14-1682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Positional encoding preserves temporal order</span>
<span id="cb14-1683"><a href="#cb14-1683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can model arbitrarily long sequences</span>
<span id="cb14-1684"><a href="#cb14-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1685"><a href="#cb14-1685" aria-hidden="true" tabindex="-1"></a>**TiMo (2025):**</span>
<span id="cb14-1686"><a href="#cb14-1686" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Description:** Spatiotemporal vision transformer foundation model</span>
<span id="cb14-1687"><a href="#cb14-1687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Innovation:** Hierarchical gyroscope attention mechanism</span>
<span id="cb14-1688"><a href="#cb14-1688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures evolving multi-scale patterns across time and space</span>
<span id="cb14-1689"><a href="#cb14-1689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained on large satellite image time series datasets</span>
<span id="cb14-1690"><a href="#cb14-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1691"><a href="#cb14-1691" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1692"><a href="#cb14-1692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global temporal context from first layer</span>
<span id="cb14-1693"><a href="#cb14-1693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles variable-length sequences</span>
<span id="cb14-1694"><a href="#cb14-1694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art performance on temporal EO tasks</span>
<span id="cb14-1695"><a href="#cb14-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1696"><a href="#cb14-1696" aria-hidden="true" tabindex="-1"></a>**Challenges:**</span>
<span id="cb14-1697"><a href="#cb14-1697" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires large amounts of training data</span>
<span id="cb14-1698"><a href="#cb14-1698" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally expensive</span>
<span id="cb14-1699"><a href="#cb14-1699" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best suited for long time series (many observations)</span>
<span id="cb14-1700"><a href="#cb14-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1701"><a href="#cb14-1701" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection Architectures</span></span>
<span id="cb14-1702"><a href="#cb14-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1703"><a href="#cb14-1703" aria-hidden="true" tabindex="-1"></a>**Object detection identifies and localizes specific objects with bounding boxes**</span>
<span id="cb14-1704"><a href="#cb14-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1705"><a href="#cb14-1705" aria-hidden="true" tabindex="-1"></a><span class="fu">#### YOLO (You Only Look Once)</span></span>
<span id="cb14-1706"><a href="#cb14-1706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1707"><a href="#cb14-1707" aria-hidden="true" tabindex="-1"></a>**Characteristics:**</span>
<span id="cb14-1708"><a href="#cb14-1708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Real-time detection:** Single-pass architecture, processes entire image once</span>
<span id="cb14-1709"><a href="#cb14-1709" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fast inference:** 30-60+ FPS depending on variant</span>
<span id="cb14-1710"><a href="#cb14-1710" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Good accuracy-speed trade-off:** Suitable for operational systems</span>
<span id="cb14-1711"><a href="#cb14-1711" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Single-stage detector:** Predicts bounding boxes and class probabilities directly</span>
<span id="cb14-1712"><a href="#cb14-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1713"><a href="#cb14-1713" aria-hidden="true" tabindex="-1"></a>**Versions:**</span>
<span id="cb14-1714"><a href="#cb14-1714" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLOv3:** Introduced multi-scale predictions</span>
<span id="cb14-1715"><a href="#cb14-1715" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLOv4:** Enhanced training techniques, better accuracy</span>
<span id="cb14-1716"><a href="#cb14-1716" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLOv5:** Popular, well-documented, easy to use (Ultralytics implementation)</span>
<span id="cb14-1717"><a href="#cb14-1717" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLOv6-v8:** Latest, best performance, improved small object detection</span>
<span id="cb14-1718"><a href="#cb14-1718" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YOLO-NAS:** Neural Architecture Search, state-of-the-art accuracy</span>
<span id="cb14-1719"><a href="#cb14-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1720"><a href="#cb14-1720" aria-hidden="true" tabindex="-1"></a>**Applications in EO:**</span>
<span id="cb14-1721"><a href="#cb14-1721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Building detection:** Rapid mapping of structures for disaster damage assessment</span>
<span id="cb14-1722"><a href="#cb14-1722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vehicle detection:** Traffic monitoring, parking lot analysis</span>
<span id="cb14-1723"><a href="#cb14-1723" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ship detection:** Maritime surveillance, illegal fishing monitoring</span>
<span id="cb14-1724"><a href="#cb14-1724" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Small object detection:** Improved in recent versions (important for vehicles, individual trees)</span>
<span id="cb14-1725"><a href="#cb14-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1726"><a href="#cb14-1726" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1727"><a href="#cb14-1727" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fast training and inference</span>
<span id="cb14-1728"><a href="#cb14-1728" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good generalization</span>
<span id="cb14-1729"><a href="#cb14-1729" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Easy to deploy</span>
<span id="cb14-1730"><a href="#cb14-1730" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Active community and pre-trained models</span>
<span id="cb14-1731"><a href="#cb14-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1732"><a href="#cb14-1732" aria-hidden="true" tabindex="-1"></a><span class="fu">#### R-CNN Family</span></span>
<span id="cb14-1733"><a href="#cb14-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1734"><a href="#cb14-1734" aria-hidden="true" tabindex="-1"></a>**Faster R-CNN:**</span>
<span id="cb14-1735"><a href="#cb14-1735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Architecture:** Two-stage detector</span>
<span id="cb14-1736"><a href="#cb14-1736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stage 1:** Region Proposal Network (RPN) generates candidate object locations</span>
<span id="cb14-1737"><a href="#cb14-1737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stage 2:** Classifies proposals and refines bounding boxes</span>
<span id="cb14-1738"><a href="#cb14-1738" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantage:** High accuracy, especially for diverse object sizes</span>
<span id="cb14-1739"><a href="#cb14-1739" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantage:** Slower than single-stage detectors like YOLO</span>
<span id="cb14-1740"><a href="#cb14-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1741"><a href="#cb14-1741" aria-hidden="true" tabindex="-1"></a>**Mask R-CNN:**</span>
<span id="cb14-1742"><a href="#cb14-1742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extension of Faster R-CNN:** Adds instance segmentation branch</span>
<span id="cb14-1743"><a href="#cb14-1743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Bounding box + pixel-level mask for each object</span>
<span id="cb14-1744"><a href="#cb14-1744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:**</span>
<span id="cb14-1745"><a href="#cb14-1745" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Building footprints with precise boundaries</span>
<span id="cb14-1746"><a href="#cb14-1746" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Individual tree crown delineation</span>
<span id="cb14-1747"><a href="#cb14-1747" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Object-level change detection</span>
<span id="cb14-1748"><a href="#cb14-1748" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Counting objects (vehicles, animals) with high precision</span>
<span id="cb14-1749"><a href="#cb14-1749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1750"><a href="#cb14-1750" aria-hidden="true" tabindex="-1"></a>**Performance:**</span>
<span id="cb14-1751"><a href="#cb14-1751" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generally higher accuracy than YOLO</span>
<span id="cb14-1752"><a href="#cb14-1752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better for complex scenes with occlusions</span>
<span id="cb14-1753"><a href="#cb14-1753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Preferred when accuracy is more important than speed</span>
<span id="cb14-1754"><a href="#cb14-1754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1755"><a href="#cb14-1755" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RetinaNet</span></span>
<span id="cb14-1756"><a href="#cb14-1756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1757"><a href="#cb14-1757" aria-hidden="true" tabindex="-1"></a>**Key Innovation:**</span>
<span id="cb14-1758"><a href="#cb14-1758" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Focal Loss:** Addresses class imbalance by down-weighting well-classified examples</span>
<span id="cb14-1759"><a href="#cb14-1759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focuses training on hard examples</span>
<span id="cb14-1760"><a href="#cb14-1760" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Single-stage detector</span>
<span id="cb14-1761"><a href="#cb14-1761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1762"><a href="#cb14-1762" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1763"><a href="#cb14-1763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Excellent for imbalanced datasets (common in EO: rare objects like ships, rare land cover classes)</span>
<span id="cb14-1764"><a href="#cb14-1764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Competitive accuracy with two-stage detectors</span>
<span id="cb14-1765"><a href="#cb14-1765" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Faster than R-CNN family</span>
<span id="cb14-1766"><a href="#cb14-1766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1767"><a href="#cb14-1767" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1768"><a href="#cb14-1768" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rare object detection (e.g., informal settlements, landslides)</span>
<span id="cb14-1769"><a href="#cb14-1769" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-class detection with imbalanced classes</span>
<span id="cb14-1770"><a href="#cb14-1770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1771"><a href="#cb14-1771" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EfficientDet</span></span>
<span id="cb14-1772"><a href="#cb14-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1773"><a href="#cb14-1773" aria-hidden="true" tabindex="-1"></a>**Key Innovation:**</span>
<span id="cb14-1774"><a href="#cb14-1774" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Compound scaling:** Jointly scales resolution, depth, and width</span>
<span id="cb14-1775"><a href="#cb14-1775" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BiFPN (Bi-directional Feature Pyramid Network):** Efficient multi-scale feature fusion</span>
<span id="cb14-1776"><a href="#cb14-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1777"><a href="#cb14-1777" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb14-1778"><a href="#cb14-1778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimal balance between accuracy and efficiency</span>
<span id="cb14-1779"><a href="#cb14-1779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scalable (EfficientDet-D0 to D7)</span>
<span id="cb14-1780"><a href="#cb14-1780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Suitable for deployment on resource-constrained devices</span>
<span id="cb14-1781"><a href="#cb14-1781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1782"><a href="#cb14-1782" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1783"><a href="#cb14-1783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Edge computing and mobile deployment</span>
<span id="cb14-1784"><a href="#cb14-1784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Operational systems requiring fast inference</span>
<span id="cb14-1785"><a href="#cb14-1785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1786"><a href="#cb14-1786" aria-hidden="true" tabindex="-1"></a>**Comparison Table:**</span>
<span id="cb14-1787"><a href="#cb14-1787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1788"><a href="#cb14-1788" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Architecture <span class="pp">|</span> Speed <span class="pp">|</span> Accuracy <span class="pp">|</span> Best For <span class="pp">|</span></span>
<span id="cb14-1789"><a href="#cb14-1789" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------------|-------|----------|----------|</span></span>
<span id="cb14-1790"><a href="#cb14-1790" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **YOLO** <span class="pp">|</span> Very Fast <span class="pp">|</span> Good <span class="pp">|</span> Real-time applications, rapid mapping <span class="pp">|</span></span>
<span id="cb14-1791"><a href="#cb14-1791" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Faster R-CNN** <span class="pp">|</span> Slow <span class="pp">|</span> High <span class="pp">|</span> High-accuracy requirements, diverse object sizes <span class="pp">|</span></span>
<span id="cb14-1792"><a href="#cb14-1792" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Mask R-CNN** <span class="pp">|</span> Slow <span class="pp">|</span> High <span class="pp">|</span> Instance segmentation, precise boundaries <span class="pp">|</span></span>
<span id="cb14-1793"><a href="#cb14-1793" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **RetinaNet** <span class="pp">|</span> Moderate <span class="pp">|</span> High <span class="pp">|</span> Imbalanced datasets, rare objects <span class="pp">|</span></span>
<span id="cb14-1794"><a href="#cb14-1794" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **EfficientDet** <span class="pp">|</span> Fast-Moderate <span class="pp">|</span> High <span class="pp">|</span> Balanced accuracy/speed, deployment <span class="pp">|</span></span>
<span id="cb14-1795"><a href="#cb14-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1796"><a href="#cb14-1796" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Modal Architectures</span></span>
<span id="cb14-1797"><a href="#cb14-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1798"><a href="#cb14-1798" aria-hidden="true" tabindex="-1"></a>**Integrating data from multiple sensors for robust monitoring**</span>
<span id="cb14-1799"><a href="#cb14-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1800"><a href="#cb14-1800" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Optical-SAR Fusion</span></span>
<span id="cb14-1801"><a href="#cb14-1801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1802"><a href="#cb14-1802" aria-hidden="true" tabindex="-1"></a>**Complementary Information:**</span>
<span id="cb14-1803"><a href="#cb14-1803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optical:** Rich spectral information (13 bands for Sentinel-2), sensitive to biochemical properties</span>
<span id="cb14-1804"><a href="#cb14-1804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SAR:** Structural information (backscatter), penetrates clouds, sensitive to moisture and geometry</span>
<span id="cb14-1805"><a href="#cb14-1805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1806"><a href="#cb14-1806" aria-hidden="true" tabindex="-1"></a>**Fusion Strategies:**</span>
<span id="cb14-1807"><a href="#cb14-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1808"><a href="#cb14-1808" aria-hidden="true" tabindex="-1"></a>**Early Fusion (Input-level):**</span>
<span id="cb14-1809"><a href="#cb14-1809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Concatenate inputs at beginning</span>
<span id="cb14-1810"><a href="#cb14-1810" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: Stack Sentinel-2 bands with Sentinel-1 VV/VH as additional channels</span>
<span id="cb14-1811"><a href="#cb14-1811" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple, but assumes features align semantically</span>
<span id="cb14-1812"><a href="#cb14-1812" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantage:** Single model processes all modalities</span>
<span id="cb14-1813"><a href="#cb14-1813" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantage:** May not capture modality-specific patterns optimally</span>
<span id="cb14-1814"><a href="#cb14-1814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1815"><a href="#cb14-1815" aria-hidden="true" tabindex="-1"></a>**Late Fusion (Decision-level):**</span>
<span id="cb14-1816"><a href="#cb14-1816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Separate models for each modality</span>
<span id="cb14-1817"><a href="#cb14-1817" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combine predictions (average, weighted average, voting)</span>
<span id="cb14-1818"><a href="#cb14-1818" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantage:** Each modality processed optimally</span>
<span id="cb14-1819"><a href="#cb14-1819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantage:** Doesn't exploit inter-modality relationships</span>
<span id="cb14-1820"><a href="#cb14-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1821"><a href="#cb14-1821" aria-hidden="true" tabindex="-1"></a>**Intermediate Fusion (Feature-level):**</span>
<span id="cb14-1822"><a href="#cb14-1822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Merge features at middle layers</span>
<span id="cb14-1823"><a href="#cb14-1823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn joint representations</span>
<span id="cb14-1824"><a href="#cb14-1824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantage:** Balances early and late fusion benefits</span>
<span id="cb14-1825"><a href="#cb14-1825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantage:** More complex architecture design</span>
<span id="cb14-1826"><a href="#cb14-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1827"><a href="#cb14-1827" aria-hidden="true" tabindex="-1"></a>**Recent Approaches:**</span>
<span id="cb14-1828"><a href="#cb14-1828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1829"><a href="#cb14-1829" aria-hidden="true" tabindex="-1"></a>**Progressive Fusion Learning:**</span>
<span id="cb14-1830"><a href="#cb14-1830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradually integrates multimodal information</span>
<span id="cb14-1831"><a href="#cb14-1831" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Addresses semantic misalignment between modalities</span>
<span id="cb14-1832"><a href="#cb14-1832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Building extraction with optical + SAR</span>
<span id="cb14-1833"><a href="#cb14-1833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1834"><a href="#cb14-1834" aria-hidden="true" tabindex="-1"></a>**M2Caps (Multi-modal Capsule Networks, 2024):**</span>
<span id="cb14-1835"><a href="#cb14-1835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capsule networks for optical-SAR fusion</span>
<span id="cb14-1836"><a href="#cb14-1836" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applications: Land cover classification</span>
<span id="cb14-1837"><a href="#cb14-1837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles appearance disparities between modalities</span>
<span id="cb14-1838"><a href="#cb14-1838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1839"><a href="#cb14-1839" aria-hidden="true" tabindex="-1"></a>**Bi-modal Contrastive Learning:**</span>
<span id="cb14-1840"><a href="#cb14-1840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-supervised approach for joint representation</span>
<span id="cb14-1841"><a href="#cb14-1841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-training on unlabeled optical-SAR pairs</span>
<span id="cb14-1842"><a href="#cb14-1842" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fine-tune for specific tasks (crop classification, change detection)</span>
<span id="cb14-1843"><a href="#cb14-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1844"><a href="#cb14-1844" aria-hidden="true" tabindex="-1"></a>**Transformer Temporal-Spatial Model (TTSM):**</span>
<span id="cb14-1845"><a href="#cb14-1845" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Synergizes SAR and optical time-series for vegetation monitoring</span>
<span id="cb14-1846"><a href="#cb14-1846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance:** R² &gt; 0.88 for vegetation reconstruction</span>
<span id="cb14-1847"><a href="#cb14-1847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles missing data in one modality</span>
<span id="cb14-1848"><a href="#cb14-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1849"><a href="#cb14-1849" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-1850"><a href="#cb14-1850" aria-hidden="true" tabindex="-1"></a>**Philippine Application: All-Weather Rice Monitoring**</span>
<span id="cb14-1851"><a href="#cb14-1851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1852"><a href="#cb14-1852" aria-hidden="true" tabindex="-1"></a>**PRiSM Multi-Sensor Approach:**</span>
<span id="cb14-1853"><a href="#cb14-1853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1854"><a href="#cb14-1854" aria-hidden="true" tabindex="-1"></a>**Challenge:** Philippines has &gt;60% cloud cover during monsoon season (June-November)</span>
<span id="cb14-1855"><a href="#cb14-1855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1856"><a href="#cb14-1856" aria-hidden="true" tabindex="-1"></a>**Solution:** Sentinel-1 SAR + Sentinel-2 optical fusion</span>
<span id="cb14-1857"><a href="#cb14-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1858"><a href="#cb14-1858" aria-hidden="true" tabindex="-1"></a>**Methodology:**</span>
<span id="cb14-1859"><a href="#cb14-1859" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-1 SAR:** Primary data source during wet season (cloud-penetrating)</span>
<span id="cb14-1860"><a href="#cb14-1860" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-2 optical:** Complementary data during dry season and cloud-free periods</span>
<span id="cb14-1861"><a href="#cb14-1861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature-level fusion:** Combine SAR backscatter (VV, VH) with optical indices (NDVI, EVI)</span>
<span id="cb14-1862"><a href="#cb14-1862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forest classifier:** Trained on fused features</span>
<span id="cb14-1863"><a href="#cb14-1863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1864"><a href="#cb14-1864" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb14-1865"><a href="#cb14-1865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Year-round monitoring regardless of weather</span>
<span id="cb14-1866"><a href="#cb14-1866" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Higher accuracy than single-sensor approach</span>
<span id="cb14-1867"><a href="#cb14-1867" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduced data gaps</span>
<span id="cb14-1868"><a href="#cb14-1868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Continuous rice area tracking</span>
<span id="cb14-1869"><a href="#cb14-1869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1870"><a href="#cb14-1870" aria-hidden="true" tabindex="-1"></a>**Operational Impact:**</span>
<span id="cb14-1871"><a href="#cb14-1871" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reliable per-season mapping even during typhoons</span>
<span id="cb14-1872"><a href="#cb14-1872" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Supports disaster damage assessment</span>
<span id="cb14-1873"><a href="#cb14-1873" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved yield prediction with temporal SAR backscatter patterns</span>
<span id="cb14-1874"><a href="#cb14-1874" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1875"><a href="#cb14-1875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1876"><a href="#cb14-1876" aria-hidden="true" tabindex="-1"></a>**Challenges:**</span>
<span id="cb14-1877"><a href="#cb14-1877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modality alignment: Different imaging mechanisms (reflectance vs. backscatter)</span>
<span id="cb14-1878"><a href="#cb14-1878" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic misalignment: Features may not correspond across modalities</span>
<span id="cb14-1879"><a href="#cb14-1879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimal fusion level depends on task and data availability</span>
<span id="cb14-1880"><a href="#cb14-1880" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Increased computational cost</span>
<span id="cb14-1881"><a href="#cb14-1881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1882"><a href="#cb14-1882" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-1883"><a href="#cb14-1883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All-weather land cover classification</span>
<span id="cb14-1884"><a href="#cb14-1884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop monitoring during cloudy seasons</span>
<span id="cb14-1885"><a href="#cb14-1885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building extraction (optical for spectral, SAR for structure)</span>
<span id="cb14-1886"><a href="#cb14-1886" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood mapping (SAR for water extent, optical for pre-event land cover)</span>
<span id="cb14-1887"><a href="#cb14-1887" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Forest biomass estimation (optical for species, SAR for structure)</span>
<span id="cb14-1888"><a href="#cb14-1888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1889"><a href="#cb14-1889" aria-hidden="true" tabindex="-1"></a><span class="fu">### Foundation Models for Earth Observation</span></span>
<span id="cb14-1890"><a href="#cb14-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1891"><a href="#cb14-1891" aria-hidden="true" tabindex="-1"></a>**Foundation models are large, pre-trained models adaptable to various downstream tasks**</span>
<span id="cb14-1892"><a href="#cb14-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1893"><a href="#cb14-1893" aria-hidden="true" tabindex="-1"></a>Emerged as transformative trend in EO 2023-2025, dramatically reducing resources required for environmental monitoring.</span>
<span id="cb14-1894"><a href="#cb14-1894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1895"><a href="#cb14-1895" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Prithvi Family (IBM-NASA)</span></span>
<span id="cb14-1896"><a href="#cb14-1896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1897"><a href="#cb14-1897" aria-hidden="true" tabindex="-1"></a>**Prithvi-EO-1.0 (August 2023):**</span>
<span id="cb14-1898"><a href="#cb14-1898" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale:** 100 million parameters</span>
<span id="cb14-1899"><a href="#cb14-1899" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training Data:** NASA's Harmonized Landsat Sentinel-2 (HLS) dataset</span>
<span id="cb14-1900"><a href="#cb14-1900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pre-training Strategy:** Masked autoencoder (MAE) - self-supervised learning on unlabeled imagery</span>
<span id="cb14-1901"><a href="#cb14-1901" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Significance:** World's largest geospatial AI model at release</span>
<span id="cb14-1902"><a href="#cb14-1902" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Availability:** Open-source on Hugging Face</span>
<span id="cb14-1903"><a href="#cb14-1903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1904"><a href="#cb14-1904" aria-hidden="true" tabindex="-1"></a>**Prithvi-EO-2.0 (December 2024):**</span>
<span id="cb14-1905"><a href="#cb14-1905" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale:** 600 million parameters (6× larger than predecessor)</span>
<span id="cb14-1906"><a href="#cb14-1906" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training Data:** 4.2 million global time series samples from HLS at 30m resolution</span>
<span id="cb14-1907"><a href="#cb14-1907" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Architecture:** Temporal transformer with location and temporal embeddings</span>
<span id="cb14-1908"><a href="#cb14-1908" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance:** 75.6% average score on GEO-bench framework (8% improvement over 1.0)</span>
<span id="cb14-1909"><a href="#cb14-1909" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Availability:** Hugging Face and IBM's TerraTorch toolkit</span>
<span id="cb14-1910"><a href="#cb14-1910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1911"><a href="#cb14-1911" aria-hidden="true" tabindex="-1"></a>**Applications Demonstrated:**</span>
<span id="cb14-1912"><a href="#cb14-1912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flood Mapping:** Valencia, Spain floods (October 2024) using Sentinel-1 + Sentinel-2</span>
<span id="cb14-1913"><a href="#cb14-1913" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Burn Scar Detection:** Wildfire impact assessment</span>
<span id="cb14-1914"><a href="#cb14-1914" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud Gap Reconstruction:** Filling missing data in cloudy imagery</span>
<span id="cb14-1915"><a href="#cb14-1915" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-Temporal Crop Segmentation:** Mapping crop types across United States</span>
<span id="cb14-1916"><a href="#cb14-1916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1917"><a href="#cb14-1917" aria-hidden="true" tabindex="-1"></a>**Fine-Tuning Workflow:**</span>
<span id="cb14-1918"><a href="#cb14-1918" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load pre-trained Prithvi model</span>
<span id="cb14-1919"><a href="#cb14-1919" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Replace classification head for specific task</span>
<span id="cb14-1920"><a href="#cb14-1920" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fine-tune on small labeled dataset (hundreds to thousands of samples)</span>
<span id="cb14-1921"><a href="#cb14-1921" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Deploy for inference</span>
<span id="cb14-1922"><a href="#cb14-1922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1923"><a href="#cb14-1923" aria-hidden="true" tabindex="-1"></a>**Impact:**</span>
<span id="cb14-1924"><a href="#cb14-1924" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables users with limited ML expertise to deploy state-of-the-art models</span>
<span id="cb14-1925"><a href="#cb14-1925" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces labeled data requirements by 10-100×</span>
<span id="cb14-1926"><a href="#cb14-1926" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Democratizes access to advanced AI for EO</span>
<span id="cb14-1927"><a href="#cb14-1927" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Foundation for operational systems in resource-constrained settings</span>
<span id="cb14-1928"><a href="#cb14-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1929"><a href="#cb14-1929" aria-hidden="true" tabindex="-1"></a>**Deployment:**</span>
<span id="cb14-1930"><a href="#cb14-1930" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integrated into IBM's TerraTorch toolkit for easy fine-tuning</span>
<span id="cb14-1931"><a href="#cb14-1931" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model zoo with pre-trained variants</span>
<span id="cb14-1932"><a href="#cb14-1932" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tutorials and example notebooks</span>
<span id="cb14-1933"><a href="#cb14-1933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1934"><a href="#cb14-1934" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Other Foundation Models</span></span>
<span id="cb14-1935"><a href="#cb14-1935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1936"><a href="#cb14-1936" aria-hidden="true" tabindex="-1"></a>**SatMAE:**</span>
<span id="cb14-1937"><a href="#cb14-1937" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Masked autoencoding for satellite imagery</span>
<span id="cb14-1938"><a href="#cb14-1938" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-supervised pre-training on unlabeled data</span>
<span id="cb14-1939"><a href="#cb14-1939" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning for downstream tasks</span>
<span id="cb14-1940"><a href="#cb14-1940" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Competitive with ImageNet pre-training in few-shot scenarios</span>
<span id="cb14-1941"><a href="#cb14-1941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1942"><a href="#cb14-1942" aria-hidden="true" tabindex="-1"></a>**SatViT:**</span>
<span id="cb14-1943"><a href="#cb14-1943" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained Vision Transformer on 1.3 million satellite-derived RS images</span>
<span id="cb14-1944"><a href="#cb14-1944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Domain-specific for remote sensing</span>
<span id="cb14-1945"><a href="#cb14-1945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved transfer learning over ImageNet pre-training</span>
<span id="cb14-1946"><a href="#cb14-1946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publicly available for EO community</span>
<span id="cb14-1947"><a href="#cb14-1947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1948"><a href="#cb14-1948" aria-hidden="true" tabindex="-1"></a>**MS-CLIP (IBM, 2024):**</span>
<span id="cb14-1949"><a href="#cb14-1949" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First vision-language model for multi-spectral Sentinel-2 data</span>
<span id="cb14-1950"><a href="#cb14-1950" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dual encoder architecture adapted from CLIP</span>
<span id="cb14-1951"><a href="#cb14-1951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles 10+ spectral bands (not just RGB)</span>
<span id="cb14-1952"><a href="#cb14-1952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Capabilities:**</span>
<span id="cb14-1953"><a href="#cb14-1953" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Zero-shot classification: Classify without task-specific training</span>
<span id="cb14-1954"><a href="#cb14-1954" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Image-text retrieval: "Find images with rice paddies"</span>
<span id="cb14-1955"><a href="#cb14-1955" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Semantic search: Natural language queries over satellite archives</span>
<span id="cb14-1956"><a href="#cb14-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1957"><a href="#cb14-1957" aria-hidden="true" tabindex="-1"></a>**TiMo (2025):**</span>
<span id="cb14-1958"><a href="#cb14-1958" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spatiotemporal vision transformer foundation model for satellite image time series</span>
<span id="cb14-1959"><a href="#cb14-1959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hierarchical gyroscope attention mechanism</span>
<span id="cb14-1960"><a href="#cb14-1960" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures evolving multi-scale patterns across time and space</span>
<span id="cb14-1961"><a href="#cb14-1961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained on large temporal satellite datasets</span>
<span id="cb14-1962"><a href="#cb14-1962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1963"><a href="#cb14-1963" aria-hidden="true" tabindex="-1"></a>**Why Foundation Models Matter:**</span>
<span id="cb14-1964"><a href="#cb14-1964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1965"><a href="#cb14-1965" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data Efficiency:** Pre-training on massive unlabeled data, fine-tune with small labeled sets</span>
<span id="cb14-1966"><a href="#cb14-1966" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Generalization:** Learn robust representations applicable across tasks and regions</span>
<span id="cb14-1967"><a href="#cb14-1967" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Democratization:** Lower barrier to entry for EO AI applications</span>
<span id="cb14-1968"><a href="#cb14-1968" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Rapid Deployment:** Quickly adapt to new applications without training from scratch</span>
<span id="cb14-1969"><a href="#cb14-1969" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Transfer Across Domains:** Models pre-trained globally applicable to local Philippine contexts</span>
<span id="cb14-1970"><a href="#cb14-1970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1971"><a href="#cb14-1971" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-1972"><a href="#cb14-1972" aria-hidden="true" tabindex="-1"></a><span class="fu">## Self-Supervised Learning</span></span>
<span id="cb14-1973"><a href="#cb14-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1974"><a href="#cb14-1974" aria-hidden="true" tabindex="-1"></a>Foundation models typically use **self-supervised learning** for pre-training:</span>
<span id="cb14-1975"><a href="#cb14-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1976"><a href="#cb14-1976" aria-hidden="true" tabindex="-1"></a>**Masked Autoencoding (MAE):**</span>
<span id="cb14-1977"><a href="#cb14-1977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Randomly mask patches of input image</span>
<span id="cb14-1978"><a href="#cb14-1978" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model learns to reconstruct masked patches</span>
<span id="cb14-1979"><a href="#cb14-1979" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Forces model to learn semantic representations</span>
<span id="cb14-1980"><a href="#cb14-1980" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No labels needed - learns from structure of data itself</span>
<span id="cb14-1981"><a href="#cb14-1981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1982"><a href="#cb14-1982" aria-hidden="true" tabindex="-1"></a>**Contrastive Learning (MoCo, SimCLR):**</span>
<span id="cb14-1983"><a href="#cb14-1983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn representations by contrasting positive and negative pairs</span>
<span id="cb14-1984"><a href="#cb14-1984" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Augmented views of same image are positive pairs</span>
<span id="cb14-1985"><a href="#cb14-1985" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Different images are negative pairs</span>
<span id="cb14-1986"><a href="#cb14-1986" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model learns invariance to augmentations</span>
<span id="cb14-1987"><a href="#cb14-1987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1988"><a href="#cb14-1988" aria-hidden="true" tabindex="-1"></a>**SSL4EO-S12 Dataset:**</span>
<span id="cb14-1989"><a href="#cb14-1989" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large-scale, global, multimodal corpus from Sentinel-1 and Sentinel-2</span>
<span id="cb14-1990"><a href="#cb14-1990" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Supports self-supervised pre-training research</span>
<span id="cb14-1991"><a href="#cb14-1991" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-seasonal coverage</span>
<span id="cb14-1992"><a href="#cb14-1992" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables research on contrastive learning for remote sensing</span>
<span id="cb14-1993"><a href="#cb14-1993" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1994"><a href="#cb14-1994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1995"><a href="#cb14-1995" aria-hidden="true" tabindex="-1"></a><span class="fu">### Training Strategies</span></span>
<span id="cb14-1996"><a href="#cb14-1996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1997"><a href="#cb14-1997" aria-hidden="true" tabindex="-1"></a>**Transfer Learning:**</span>
<span id="cb14-1998"><a href="#cb14-1998" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Approach:** Pre-train on large dataset (ImageNet, SatViT, Prithvi), fine-tune on task-specific data</span>
<span id="cb14-1999"><a href="#cb14-1999" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Benefits:** Reduces training time, improves performance with limited data</span>
<span id="cb14-2000"><a href="#cb14-2000" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Best Practice:** Freeze early layers (generic features), fine-tune later layers (task-specific features)</span>
<span id="cb14-2001"><a href="#cb14-2001" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recent Research (2024):** Self-supervised pre-training on RS data offers modest improvements over ImageNet in few-shot settings</span>
<span id="cb14-2002"><a href="#cb14-2002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2003"><a href="#cb14-2003" aria-hidden="true" tabindex="-1"></a>**Data Augmentation:**</span>
<span id="cb14-2004"><a href="#cb14-2004" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation and flipping:** Particularly suitable for satellite imagery (no canonical orientation)</span>
<span id="cb14-2005"><a href="#cb14-2005" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Color jittering:** Simulate atmospheric variations</span>
<span id="cb14-2006"><a href="#cb14-2006" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random crops:** Increase spatial diversity</span>
<span id="cb14-2007"><a href="#cb14-2007" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixup and CutMix:** Regularization techniques for classification</span>
<span id="cb14-2008"><a href="#cb14-2008" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Caution:** Ensure augmentations are realistic for EO (e.g., don't vertically flip landscapes with clear sky/ground distinction)</span>
<span id="cb14-2009"><a href="#cb14-2009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2010"><a href="#cb14-2010" aria-hidden="true" tabindex="-1"></a>**Self-Supervised Learning:**</span>
<span id="cb14-2011"><a href="#cb14-2011" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Contrastive learning:** MoCo, SimCLR for learning representations</span>
<span id="cb14-2012"><a href="#cb14-2012" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Masked image modeling:** MAE for learning to reconstruct images</span>
<span id="cb14-2013"><a href="#cb14-2013" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-modal alignment:** CLIP-style vision-language pre-training</span>
<span id="cb14-2014"><a href="#cb14-2014" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SSL4EO-2024 Summer School:** First summer school on self-supervised learning for EO (July 2024, Copenhagen)</span>
<span id="cb14-2015"><a href="#cb14-2015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2016"><a href="#cb14-2016" aria-hidden="true" tabindex="-1"></a>**Few-Shot Learning:**</span>
<span id="cb14-2017"><a href="#cb14-2017" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Motivation:** Limited labeled data, expensive annotation</span>
<span id="cb14-2018"><a href="#cb14-2018" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Methods:** Metric learning, meta-learning, prototypical networks</span>
<span id="cb14-2019"><a href="#cb14-2019" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:** Novel land cover classes, rare object detection, new geographic regions</span>
<span id="cb14-2020"><a href="#cb14-2020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example:** Gerry Roxas Foundation deforestation classification achieved 43% accuracy with only 8% training data</span>
<span id="cb14-2021"><a href="#cb14-2021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2022"><a href="#cb14-2022" aria-hidden="true" tabindex="-1"></a>**Active Learning:**</span>
<span id="cb14-2023"><a href="#cb14-2023" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Strategy:** Iteratively select most informative samples for labeling</span>
<span id="cb14-2024"><a href="#cb14-2024" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Train model → Find uncertain predictions → Label those → Retrain</span>
<span id="cb14-2025"><a href="#cb14-2025" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Benefits:** Reduced annotation cost (27% improvement in mIoU with only 2% labeled data)</span>
<span id="cb14-2026"><a href="#cb14-2026" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**WeakAL Framework:** Combines active learning and weak supervision, computing &gt;90% of labels automatically while maintaining competitive performance</span>
<span id="cb14-2027"><a href="#cb14-2027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2028"><a href="#cb14-2028" aria-hidden="true" tabindex="-1"></a>**Best Practices:**</span>
<span id="cb14-2029"><a href="#cb14-2029" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Start with pre-trained weights when available (Prithvi, SatViT, ImageNet)</span>
<span id="cb14-2030"><a href="#cb14-2030" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use appropriate learning rate schedules (cosine annealing with warm-up)</span>
<span id="cb14-2031"><a href="#cb14-2031" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply batch normalization or layer normalization for training stability</span>
<span id="cb14-2032"><a href="#cb14-2032" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Monitor overfitting through validation metrics (gap between train and validation loss)</span>
<span id="cb14-2033"><a href="#cb14-2033" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement early stopping and model checkpointing</span>
<span id="cb14-2034"><a href="#cb14-2034" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use mixed-precision training (FP16) for faster training on modern GPUs</span>
<span id="cb14-2035"><a href="#cb14-2035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2036"><a href="#cb14-2036" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2037"><a href="#cb14-2037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2038"><a href="#cb14-2038" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 5: Benchmark Datasets for Training and Validation</span></span>
<span id="cb14-2039"><a href="#cb14-2039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2040"><a href="#cb14-2040" aria-hidden="true" tabindex="-1"></a>**Benchmark datasets enable standardized comparison of algorithms and serve as training resources**</span>
<span id="cb14-2041"><a href="#cb14-2041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2042"><a href="#cb14-2042" aria-hidden="true" tabindex="-1"></a><span class="fu">### Patch-Level Classification Datasets</span></span>
<span id="cb14-2043"><a href="#cb14-2043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2044"><a href="#cb14-2044" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EuroSAT</span></span>
<span id="cb14-2045"><a href="#cb14-2045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2046"><a href="#cb14-2046" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2047"><a href="#cb14-2047" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Images:** 27,000 labeled images</span>
<span id="cb14-2048"><a href="#cb14-2048" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classes:** 10 land cover types</span>
<span id="cb14-2049"><a href="#cb14-2049" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Size:** 64×64 pixel patches</span>
<span id="cb14-2050"><a href="#cb14-2050" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bands:** 13 (Sentinel-2 multispectral)</span>
<span id="cb14-2051"><a href="#cb14-2051" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coverage:** Europe</span>
<span id="cb14-2052"><a href="#cb14-2052" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification Accuracy:** 98.57% achieved with CNNs</span>
<span id="cb14-2053"><a href="#cb14-2053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2054"><a href="#cb14-2054" aria-hidden="true" tabindex="-1"></a>**Classes:**</span>
<span id="cb14-2055"><a href="#cb14-2055" aria-hidden="true" tabindex="-1"></a>Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial Buildings, Pasture, Permanent Crop, Residential Buildings, River, Sea/Lake</span>
<span id="cb14-2056"><a href="#cb14-2056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2057"><a href="#cb14-2057" aria-hidden="true" tabindex="-1"></a>**Access:**</span>
<span id="cb14-2058"><a href="#cb14-2058" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>GitHub: https://github.com/phelber/EuroSAT</span>
<span id="cb14-2059"><a href="#cb14-2059" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>TensorFlow Datasets</span>
<span id="cb14-2060"><a href="#cb14-2060" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PyTorch datasets</span>
<span id="cb14-2061"><a href="#cb14-2061" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Commonly used for benchmarking deep learning architectures</span>
<span id="cb14-2062"><a href="#cb14-2062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2063"><a href="#cb14-2063" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BigEarthNet v2.0</span></span>
<span id="cb14-2064"><a href="#cb14-2064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2065"><a href="#cb14-2065" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2066"><a href="#cb14-2066" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Patches:** 549,488 paired Sentinel-1 and Sentinel-2 patches</span>
<span id="cb14-2067"><a href="#cb14-2067" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Size:** 1.2×1.2 km on ground</span>
<span id="cb14-2068"><a href="#cb14-2068" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classes:** 19 (CORINE Land Cover nomenclature)</span>
<span id="cb14-2069"><a href="#cb14-2069" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Type:** Multi-label classification (multiple classes per patch)</span>
<span id="cb14-2070"><a href="#cb14-2070" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coverage:** 10 European countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland)</span>
<span id="cb14-2071"><a href="#cb14-2071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2072"><a href="#cb14-2072" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb14-2073"><a href="#cb14-2073" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-modal (optical + SAR)</span>
<span id="cb14-2074"><a href="#cb14-2074" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-label annotations (real-world complexity)</span>
<span id="cb14-2075"><a href="#cb14-2075" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large-scale (largest Sentinel dataset)</span>
<span id="cb14-2076"><a href="#cb14-2076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2077"><a href="#cb14-2077" aria-hidden="true" tabindex="-1"></a>**Access:**</span>
<span id="cb14-2078"><a href="#cb14-2078" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: https://bigearth.net/</span>
<span id="cb14-2079"><a href="#cb14-2079" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>TensorFlow Datasets</span>
<span id="cb14-2080"><a href="#cb14-2080" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Papers With Code</span>
<span id="cb14-2081"><a href="#cb14-2081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2082"><a href="#cb14-2082" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2083"><a href="#cb14-2083" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-label land cover classification</span>
<span id="cb14-2084"><a href="#cb14-2084" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-modal fusion research</span>
<span id="cb14-2085"><a href="#cb14-2085" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benchmark for semantic segmentation</span>
<span id="cb14-2086"><a href="#cb14-2086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2087"><a href="#cb14-2087" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LandCoverNet</span></span>
<span id="cb14-2088"><a href="#cb14-2088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2089"><a href="#cb14-2089" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2090"><a href="#cb14-2090" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global coverage</span>
<span id="cb14-2091"><a href="#cb14-2091" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 based</span>
<span id="cb14-2092"><a href="#cb14-2092" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal (annual)</span>
<span id="cb14-2093"><a href="#cb14-2093" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple continents</span>
<span id="cb14-2094"><a href="#cb14-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2095"><a href="#cb14-2095" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2096"><a href="#cb14-2096" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global land cover mapping benchmark</span>
<span id="cb14-2097"><a href="#cb14-2097" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal classification</span>
<span id="cb14-2098"><a href="#cb14-2098" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Seasonal analysis and phenology</span>
<span id="cb14-2099"><a href="#cb14-2099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2100"><a href="#cb14-2100" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection Datasets</span></span>
<span id="cb14-2101"><a href="#cb14-2101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2102"><a href="#cb14-2102" aria-hidden="true" tabindex="-1"></a><span class="fu">#### xView</span></span>
<span id="cb14-2103"><a href="#cb14-2103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2104"><a href="#cb14-2104" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2105"><a href="#cb14-2105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Objects:** &gt;1 million annotated objects</span>
<span id="cb14-2106"><a href="#cb14-2106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classes:** 60</span>
<span id="cb14-2107"><a href="#cb14-2107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Area:** &gt;1,400 km²</span>
<span id="cb14-2108"><a href="#cb14-2108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Resolution:** 0.3m (WorldView-3 satellite)</span>
<span id="cb14-2109"><a href="#cb14-2109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Format:** Bounding boxes</span>
<span id="cb14-2110"><a href="#cb14-2110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2111"><a href="#cb14-2111" aria-hidden="true" tabindex="-1"></a>**Purpose:**</span>
<span id="cb14-2112"><a href="#cb14-2112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Disaster response applications</span>
<span id="cb14-2113"><a href="#cb14-2113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Overhead imagery analysis</span>
<span id="cb14-2114"><a href="#cb14-2114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object detection benchmarking</span>
<span id="cb14-2115"><a href="#cb14-2115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small object detection</span>
<span id="cb14-2116"><a href="#cb14-2116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2117"><a href="#cb14-2117" aria-hidden="true" tabindex="-1"></a>**Access:**</span>
<span id="cb14-2118"><a href="#cb14-2118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: http://xviewdataset.org/</span>
<span id="cb14-2119"><a href="#cb14-2119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Papers With Code</span>
<span id="cb14-2120"><a href="#cb14-2120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Challenge competitions</span>
<span id="cb14-2121"><a href="#cb14-2121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2122"><a href="#cb14-2122" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DOTA (Dataset for Object Detection in Aerial Images)</span></span>
<span id="cb14-2123"><a href="#cb14-2123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2124"><a href="#cb14-2124" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2125"><a href="#cb14-2125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Instances:** 1,793,658 annotated objects</span>
<span id="cb14-2126"><a href="#cb14-2126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Categories:** 18 object types</span>
<span id="cb14-2127"><a href="#cb14-2127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Images:** 11,268</span>
<span id="cb14-2128"><a href="#cb14-2128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Annotation:** Oriented bounding boxes (OBB)</span>
<span id="cb14-2129"><a href="#cb14-2129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sources:** Google Earth, GF-2 Satellite, aerial platforms</span>
<span id="cb14-2130"><a href="#cb14-2130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2131"><a href="#cb14-2131" aria-hidden="true" tabindex="-1"></a>**Key Feature:**</span>
<span id="cb14-2132"><a href="#cb14-2132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Oriented annotations:** Captures object rotation (important for buildings, ships, aircraft)</span>
<span id="cb14-2133"><a href="#cb14-2133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Various object orientations and aspect ratios</span>
<span id="cb14-2134"><a href="#cb14-2134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple sensors and resolutions</span>
<span id="cb14-2135"><a href="#cb14-2135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2136"><a href="#cb14-2136" aria-hidden="true" tabindex="-1"></a>**Access:**</span>
<span id="cb14-2137"><a href="#cb14-2137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: https://captain-whu.github.io/DOTA/</span>
<span id="cb14-2138"><a href="#cb14-2138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Papers With Code</span>
<span id="cb14-2139"><a href="#cb14-2139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>GitHub repositories</span>
<span id="cb14-2140"><a href="#cb14-2140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2141"><a href="#cb14-2141" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Segmentation Datasets</span></span>
<span id="cb14-2142"><a href="#cb14-2142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2143"><a href="#cb14-2143" aria-hidden="true" tabindex="-1"></a><span class="fu">#### OpenEarthMap</span></span>
<span id="cb14-2144"><a href="#cb14-2144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2145"><a href="#cb14-2145" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2146"><a href="#cb14-2146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global high-resolution land cover mapping benchmark</span>
<span id="cb14-2147"><a href="#cb14-2147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple continents represented</span>
<span id="cb14-2148"><a href="#cb14-2148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic segmentation annotations</span>
<span id="cb14-2149"><a href="#cb14-2149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-resolution imagery</span>
<span id="cb14-2150"><a href="#cb14-2150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2151"><a href="#cb14-2151" aria-hidden="true" tabindex="-1"></a>**Purpose:**</span>
<span id="cb14-2152"><a href="#cb14-2152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global mapping challenges</span>
<span id="cb14-2153"><a href="#cb14-2153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-region training and generalization testing</span>
<span id="cb14-2154"><a href="#cb14-2154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standardized semantic segmentation evaluation</span>
<span id="cb14-2155"><a href="#cb14-2155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2156"><a href="#cb14-2156" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SpaceNet</span></span>
<span id="cb14-2157"><a href="#cb14-2157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2158"><a href="#cb14-2158" aria-hidden="true" tabindex="-1"></a>**Overview:**</span>
<span id="cb14-2159"><a href="#cb14-2159" aria-hidden="true" tabindex="-1"></a>Foundation dataset for building footprints and road networks</span>
<span id="cb14-2160"><a href="#cb14-2160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2161"><a href="#cb14-2161" aria-hidden="true" tabindex="-1"></a>**Versions:**</span>
<span id="cb14-2162"><a href="#cb14-2162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SpaceNet 1-7: Multiple cities, different tasks</span>
<span id="cb14-2163"><a href="#cb14-2163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building footprint extraction</span>
<span id="cb14-2164"><a href="#cb14-2164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Road network mapping</span>
<span id="cb14-2165"><a href="#cb14-2165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood impact assessment (SpaceNet 8)</span>
<span id="cb14-2166"><a href="#cb14-2166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Open competition with benchmark results</span>
<span id="cb14-2167"><a href="#cb14-2167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2168"><a href="#cb14-2168" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2169"><a href="#cb14-2169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building extraction algorithms</span>
<span id="cb14-2170"><a href="#cb14-2170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Road network detection</span>
<span id="cb14-2171"><a href="#cb14-2171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-sensor fusion (optical + SAR for SpaceNet 6)</span>
<span id="cb14-2172"><a href="#cb14-2172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2173"><a href="#cb14-2173" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scene Classification Datasets</span></span>
<span id="cb14-2174"><a href="#cb14-2174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2175"><a href="#cb14-2175" aria-hidden="true" tabindex="-1"></a><span class="fu">#### AID (Aerial Image Dataset)</span></span>
<span id="cb14-2176"><a href="#cb14-2176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2177"><a href="#cb14-2177" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2178"><a href="#cb14-2178" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Images:** 10,000</span>
<span id="cb14-2179"><a href="#cb14-2179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Categories:** 30 scene categories</span>
<span id="cb14-2180"><a href="#cb14-2180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Size:** 600×600 pixels</span>
<span id="cb14-2181"><a href="#cb14-2181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Resolution:** 0.5-8m spatial resolution</span>
<span id="cb14-2182"><a href="#cb14-2182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Source:** Google Earth imagery</span>
<span id="cb14-2183"><a href="#cb14-2183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2184"><a href="#cb14-2184" aria-hidden="true" tabindex="-1"></a>**Purpose:**</span>
<span id="cb14-2185"><a href="#cb14-2185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene classification benchmarking</span>
<span id="cb14-2186"><a href="#cb14-2186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning evaluation</span>
<span id="cb14-2187"><a href="#cb14-2187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature extraction research</span>
<span id="cb14-2188"><a href="#cb14-2188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2189"><a href="#cb14-2189" aria-hidden="true" tabindex="-1"></a><span class="fu">#### NWPU-RESISC45</span></span>
<span id="cb14-2190"><a href="#cb14-2190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2191"><a href="#cb14-2191" aria-hidden="true" tabindex="-1"></a>**Specifications:**</span>
<span id="cb14-2192"><a href="#cb14-2192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Categories:** 45 scene types</span>
<span id="cb14-2193"><a href="#cb14-2193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Images:** 31,500 (700 per class)</span>
<span id="cb14-2194"><a href="#cb14-2194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Size:** 256×256 pixels</span>
<span id="cb14-2195"><a href="#cb14-2195" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Source:** High-resolution aerial images</span>
<span id="cb14-2196"><a href="#cb14-2196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2197"><a href="#cb14-2197" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2198"><a href="#cb14-2198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene recognition</span>
<span id="cb14-2199"><a href="#cb14-2199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning source</span>
<span id="cb14-2200"><a href="#cb14-2200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benchmark comparisons</span>
<span id="cb14-2201"><a href="#cb14-2201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2202"><a href="#cb14-2202" aria-hidden="true" tabindex="-1"></a><span class="fu">### Time Series Datasets</span></span>
<span id="cb14-2203"><a href="#cb14-2203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2204"><a href="#cb14-2204" aria-hidden="true" tabindex="-1"></a><span class="fu">#### TiSeLaC (Time Series Land Cover)</span></span>
<span id="cb14-2205"><a href="#cb14-2205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2206"><a href="#cb14-2206" aria-hidden="true" tabindex="-1"></a>**Purpose:**</span>
<span id="cb14-2207"><a href="#cb14-2207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal classification</span>
<span id="cb14-2208"><a href="#cb14-2208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Phenology analysis</span>
<span id="cb14-2209"><a href="#cb14-2209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal pattern learning</span>
<span id="cb14-2210"><a href="#cb14-2210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2211"><a href="#cb14-2211" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2212"><a href="#cb14-2212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crop type mapping from time series</span>
<span id="cb14-2213"><a href="#cb14-2213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vegetation dynamics</span>
<span id="cb14-2214"><a href="#cb14-2214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Seasonal change detection</span>
<span id="cb14-2215"><a href="#cb14-2215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2216"><a href="#cb14-2216" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Satellite Image Time Series (SITS) Datasets</span></span>
<span id="cb14-2217"><a href="#cb14-2217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2218"><a href="#cb14-2218" aria-hidden="true" tabindex="-1"></a>**Various Sources:**</span>
<span id="cb14-2219"><a href="#cb14-2219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MODIS time series (daily, 250m-1km)</span>
<span id="cb14-2220"><a href="#cb14-2220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 time series (5-day, 10-20m)</span>
<span id="cb14-2221"><a href="#cb14-2221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Landsat time series (16-day, 30m)</span>
<span id="cb14-2222"><a href="#cb14-2222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2223"><a href="#cb14-2223" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb14-2224"><a href="#cb14-2224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LSTM and temporal attention training</span>
<span id="cb14-2225"><a href="#cb14-2225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Phenology extraction</span>
<span id="cb14-2226"><a href="#cb14-2226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover trajectory analysis</span>
<span id="cb14-2227"><a href="#cb14-2227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2228"><a href="#cb14-2228" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine-Specific Data Resources</span></span>
<span id="cb14-2229"><a href="#cb14-2229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2230"><a href="#cb14-2230" aria-hidden="true" tabindex="-1"></a>**Available Operational Data:**</span>
<span id="cb14-2231"><a href="#cb14-2231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2232"><a href="#cb14-2232" aria-hidden="true" tabindex="-1"></a>**PRiSM Products:**</span>
<span id="cb14-2233"><a href="#cb14-2233" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rice area maps (per season: wet and dry)</span>
<span id="cb14-2234"><a href="#cb14-2234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Seasonality information (planting dates, growth stages)</span>
<span id="cb14-2235"><a href="#cb14-2235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Yield estimates</span>
<span id="cb14-2236"><a href="#cb14-2236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Historical archive since 2014</span>
<span id="cb14-2237"><a href="#cb14-2237" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: https://prism.philrice.gov.ph/</span>
<span id="cb14-2238"><a href="#cb14-2238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2239"><a href="#cb14-2239" aria-hidden="true" tabindex="-1"></a>**PhilSA Products:**</span>
<span id="cb14-2240"><a href="#cb14-2240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood extent maps from DATOS system</span>
<span id="cb14-2241"><a href="#cb14-2241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mangrove extent maps (PhilSA-DENR collaboration)</span>
<span id="cb14-2242"><a href="#cb14-2242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover maps</span>
<span id="cb14-2243"><a href="#cb14-2243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Disaster damage assessment outputs</span>
<span id="cb14-2244"><a href="#cb14-2244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: https://philsa.gov.ph/</span>
<span id="cb14-2245"><a href="#cb14-2245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2246"><a href="#cb14-2246" aria-hidden="true" tabindex="-1"></a>**DOST-ASTI:**</span>
<span id="cb14-2247"><a href="#cb14-2247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DATOS disaster response maps</span>
<span id="cb14-2248"><a href="#cb14-2248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hazard maps (flood, landslide susceptibility)</span>
<span id="cb14-2249"><a href="#cb14-2249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>AI-powered rapid assessments</span>
<span id="cb14-2250"><a href="#cb14-2250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Website: https://hazardhunter.georisk.gov.ph/map</span>
<span id="cb14-2251"><a href="#cb14-2251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2252"><a href="#cb14-2252" aria-hidden="true" tabindex="-1"></a>**NAMRIA Geoportal:**</span>
<span id="cb14-2253"><a href="#cb14-2253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Topographic maps</span>
<span id="cb14-2254"><a href="#cb14-2254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover basemaps</span>
<span id="cb14-2255"><a href="#cb14-2255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Administrative boundaries</span>
<span id="cb14-2256"><a href="#cb14-2256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Digital Elevation Models</span>
<span id="cb14-2257"><a href="#cb14-2257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2258"><a href="#cb14-2258" aria-hidden="true" tabindex="-1"></a>**Importance of Benchmark Datasets:**</span>
<span id="cb14-2259"><a href="#cb14-2259" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Standardized Evaluation:** Compare algorithms objectively</span>
<span id="cb14-2260"><a href="#cb14-2260" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Training Resources:** Pre-labeled data for model training</span>
<span id="cb14-2261"><a href="#cb14-2261" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Transfer Learning:** Pre-train on large datasets, fine-tune for specific applications</span>
<span id="cb14-2262"><a href="#cb14-2262" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Research Reproducibility:** Enable comparison across studies</span>
<span id="cb14-2263"><a href="#cb14-2263" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Community Building:** Shared resources accelerate progress</span>
<span id="cb14-2264"><a href="#cb14-2264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2265"><a href="#cb14-2265" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2266"><a href="#cb14-2266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2267"><a href="#cb14-2267" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 6: Data-Centric AI in Earth Observation</span></span>
<span id="cb14-2268"><a href="#cb14-2268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2269"><a href="#cb14-2269" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Paradigm Shift (2025)</span></span>
<span id="cb14-2270"><a href="#cb14-2270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2271"><a href="#cb14-2271" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb14-2272"><a href="#cb14-2272" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data &gt; Models</span></span>
<span id="cb14-2273"><a href="#cb14-2273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2274"><a href="#cb14-2274" aria-hidden="true" tabindex="-1"></a>**Old paradigm (Model-Centric AI):**</span>
<span id="cb14-2275"><a href="#cb14-2275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus on developing better algorithms</span>
<span id="cb14-2276"><a href="#cb14-2276" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep data fixed, iterate on model architecture</span>
<span id="cb14-2277"><a href="#cb14-2277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Our new model achieves 92% accuracy!"</span>
<span id="cb14-2278"><a href="#cb14-2278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Endless hyperparameter tuning</span>
<span id="cb14-2279"><a href="#cb14-2279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2280"><a href="#cb14-2280" aria-hidden="true" tabindex="-1"></a>**New paradigm (Data-Centric AI):**</span>
<span id="cb14-2281"><a href="#cb14-2281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus on improving data quality and curation</span>
<span id="cb14-2282"><a href="#cb14-2282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep model fixed (use proven architectures), iterate on data</span>
<span id="cb14-2283"><a href="#cb14-2283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Better data improved our model from 85% to 95% accuracy!"</span>
<span id="cb14-2284"><a href="#cb14-2284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Systematic data improvement</span>
<span id="cb14-2285"><a href="#cb14-2285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2286"><a href="#cb14-2286" aria-hidden="true" tabindex="-1"></a>**Van der Schaar Lab's DC-Check Framework:** Argues that reliable ML hinges on characterizing, evaluating, and monitoring training data across the pipeline - not just model complexity.</span>
<span id="cb14-2287"><a href="#cb14-2287" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2288"><a href="#cb14-2288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2289"><a href="#cb14-2289" aria-hidden="true" tabindex="-1"></a>**Why the shift?**</span>
<span id="cb14-2290"><a href="#cb14-2290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2291"><a href="#cb14-2291" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Model architectures have matured:** ResNet, U-Net, LSTM, Transformers are well-established and publicly available</span>
<span id="cb14-2292"><a href="#cb14-2292" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Biggest gains come from data:** Research shows most underperforming models suffer from data issues, not algorithm deficiencies</span>
<span id="cb14-2293"><a href="#cb14-2293" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Real-world deployment:** Data quality determines operational success and trustworthiness</span>
<span id="cb14-2294"><a href="#cb14-2294" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Diminishing returns:** Incremental model improvements yield smaller gains than data improvements</span>
<span id="cb14-2295"><a href="#cb14-2295" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Foundation models:** Pre-trained models (Prithvi, SatViT) reduce need for architecture innovation</span>
<span id="cb14-2296"><a href="#cb14-2296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2297"><a href="#cb14-2297" aria-hidden="true" tabindex="-1"></a>**Data-Centric Principles:**</span>
<span id="cb14-2298"><a href="#cb14-2298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2299"><a href="#cb14-2299" aria-hidden="true" tabindex="-1"></a>From van der Schaar Lab's DC-Check framework:</span>
<span id="cb14-2300"><a href="#cb14-2300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Characterizing:** Understand training data distribution, coverage, biases</span>
<span id="cb14-2301"><a href="#cb14-2301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Evaluating:** Assess data quality, label accuracy, representation</span>
<span id="cb14-2302"><a href="#cb14-2302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Monitoring:** Track data drift, performance on subgroups, uncertainty</span>
<span id="cb14-2303"><a href="#cb14-2303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stratification:** Easy/Ambiguous/Hard samples require different treatment</span>
<span id="cb14-2304"><a href="#cb14-2304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data-SUITE:** Suitability, Usefulness, Insufficiency, Thoroughness, Expressiveness checks</span>
<span id="cb14-2305"><a href="#cb14-2305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2308"><a href="#cb14-2308" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-2309"><a href="#cb14-2309" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Data-Centric AI Framework for Earth Observation</span><span class="ot">"</span></span>
<span id="cb14-2310"><a href="#cb14-2310" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb14-2311"><a href="#cb14-2311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2312"><a href="#cb14-2312" aria-hidden="true" tabindex="-1"></a>flowchart TB</span>
<span id="cb14-2313"><a href="#cb14-2313" aria-hidden="true" tabindex="-1"></a>    subgraph ModelCentric[<span class="ot">"</span><span class="st">MODEL-CENTRIC AI (Old Paradigm)</span><span class="ot">"</span>]</span>
<span id="cb14-2314"><a href="#cb14-2314" aria-hidden="true" tabindex="-1"></a>        MC1[Fixed Data] --&gt; MC2[Iterate Models]</span>
<span id="cb14-2315"><a href="#cb14-2315" aria-hidden="true" tabindex="-1"></a>        MC2 --&gt; MC3[Tune Hyperparameters]</span>
<span id="cb14-2316"><a href="#cb14-2316" aria-hidden="true" tabindex="-1"></a>        MC3 --&gt; MC4[Try New Architectures]</span>
<span id="cb14-2317"><a href="#cb14-2317" aria-hidden="true" tabindex="-1"></a>        MC4 --&gt; MC5[<span class="dv">85</span>% → <span class="dv">87</span>% → <span class="dv">88</span>%&lt;br/&gt;Diminishing Returns]</span>
<span id="cb14-2318"><a href="#cb14-2318" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-2319"><a href="#cb14-2319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2320"><a href="#cb14-2320" aria-hidden="true" tabindex="-1"></a>    subgraph DataCentric[<span class="ot">"</span><span class="st">DATA-CENTRIC AI (2025 Paradigm)</span><span class="ot">"</span>]</span>
<span id="cb14-2321"><a href="#cb14-2321" aria-hidden="true" tabindex="-1"></a>        DC1[Proven Architecture&lt;br/&gt;ResNet, U-Net, ViT] --&gt; DC2[Improve Data Quality]</span>
<span id="cb14-2322"><a href="#cb14-2322" aria-hidden="true" tabindex="-1"></a>        DC2 --&gt; DC3[Increase Data Quantity]</span>
<span id="cb14-2323"><a href="#cb14-2323" aria-hidden="true" tabindex="-1"></a>        DC3 --&gt; DC4[Enhance Data Diversity]</span>
<span id="cb14-2324"><a href="#cb14-2324" aria-hidden="true" tabindex="-1"></a>        DC4 --&gt; DC5[Refine Annotations]</span>
<span id="cb14-2325"><a href="#cb14-2325" aria-hidden="true" tabindex="-1"></a>        DC5 --&gt; DC6[<span class="dv">85</span>% → <span class="dv">92</span>% → <span class="dv">95</span>%&lt;br/&gt;Significant Gains]</span>
<span id="cb14-2326"><a href="#cb14-2326" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-2327"><a href="#cb14-2327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2328"><a href="#cb14-2328" aria-hidden="true" tabindex="-1"></a>    subgraph Pillars[<span class="ot">"</span><span class="st">FOUR PILLARS OF DATA-CENTRIC AI</span><span class="ot">"</span>]</span>
<span id="cb14-2329"><a href="#cb14-2329" aria-hidden="true" tabindex="-1"></a>        P1[<span class="dv">1</span>. Quality&lt;br/&gt;Accurate, consistent&lt;br/&gt;Cloud-free&lt;br/&gt;Atmospherically corrected]</span>
<span id="cb14-2330"><a href="#cb14-2330" aria-hidden="true" tabindex="-1"></a>        P2[<span class="dv">2</span>. Quantity&lt;br/&gt;Sufficient samples&lt;br/&gt;Per class balance&lt;br/&gt;Training data scale]</span>
<span id="cb14-2331"><a href="#cb14-2331" aria-hidden="true" tabindex="-1"></a>        P3[<span class="dv">3</span>. Diversity&lt;br/&gt;Geographic coverage&lt;br/&gt;Temporal variation&lt;br/&gt;Seasonal representation]</span>
<span id="cb14-2332"><a href="#cb14-2332" aria-hidden="true" tabindex="-1"></a>        P4[<span class="dv">4</span>. Annotation&lt;br/&gt;Label accuracy&lt;br/&gt;Boundary precision&lt;br/&gt;Class consistency]</span>
<span id="cb14-2333"><a href="#cb14-2333" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-2334"><a href="#cb14-2334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2335"><a href="#cb14-2335" aria-hidden="true" tabindex="-1"></a>    DC2 --&gt; P1</span>
<span id="cb14-2336"><a href="#cb14-2336" aria-hidden="true" tabindex="-1"></a>    DC3 --&gt; P2</span>
<span id="cb14-2337"><a href="#cb14-2337" aria-hidden="true" tabindex="-1"></a>    DC4 --&gt; P3</span>
<span id="cb14-2338"><a href="#cb14-2338" aria-hidden="true" tabindex="-1"></a>    DC5 --&gt; P4</span>
<span id="cb14-2339"><a href="#cb14-2339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2340"><a href="#cb14-2340" aria-hidden="true" tabindex="-1"></a>    subgraph DCCheck[<span class="ot">"</span><span class="st">DC-CHECK FRAMEWORK</span><span class="ot">"</span>]</span>
<span id="cb14-2341"><a href="#cb14-2341" aria-hidden="true" tabindex="-1"></a>        DCC1[Characterize&lt;br/&gt;Data distribution&lt;br/&gt;Coverage, biases]</span>
<span id="cb14-2342"><a href="#cb14-2342" aria-hidden="true" tabindex="-1"></a>        DCC2[Evaluate&lt;br/&gt;Label quality&lt;br/&gt;Representation]</span>
<span id="cb14-2343"><a href="#cb14-2343" aria-hidden="true" tabindex="-1"></a>        DCC3[Monitor&lt;br/&gt;Data drift&lt;br/&gt;Performance tracking]</span>
<span id="cb14-2344"><a href="#cb14-2344" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-2345"><a href="#cb14-2345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2346"><a href="#cb14-2346" aria-hidden="true" tabindex="-1"></a>    P1 --&gt; DCC1</span>
<span id="cb14-2347"><a href="#cb14-2347" aria-hidden="true" tabindex="-1"></a>    P2 --&gt; DCC1</span>
<span id="cb14-2348"><a href="#cb14-2348" aria-hidden="true" tabindex="-1"></a>    P3 --&gt; DCC2</span>
<span id="cb14-2349"><a href="#cb14-2349" aria-hidden="true" tabindex="-1"></a>    P4 --&gt; DCC2</span>
<span id="cb14-2350"><a href="#cb14-2350" aria-hidden="true" tabindex="-1"></a>    DCC1 --&gt; DCC3</span>
<span id="cb14-2351"><a href="#cb14-2351" aria-hidden="true" tabindex="-1"></a>    DCC2 --&gt; DCC3</span>
<span id="cb14-2352"><a href="#cb14-2352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2353"><a href="#cb14-2353" aria-hidden="true" tabindex="-1"></a>    DCC3 --&gt; Result[Robust,&lt;br/&gt;Operational&lt;br/&gt;Models]</span>
<span id="cb14-2354"><a href="#cb14-2354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2355"><a href="#cb14-2355" aria-hidden="true" tabindex="-1"></a>    style ModelCentric fill:<span class="co">#ffe6e6,stroke:#cc0044,stroke-width:2px</span></span>
<span id="cb14-2356"><a href="#cb14-2356" aria-hidden="true" tabindex="-1"></a>    style DataCentric fill:<span class="co">#e6ffe6,stroke:#00aa44,stroke-width:3px</span></span>
<span id="cb14-2357"><a href="#cb14-2357" aria-hidden="true" tabindex="-1"></a>    style Pillars fill:<span class="co">#e6f3ff,stroke:#0066cc,stroke-width:2px</span></span>
<span id="cb14-2358"><a href="#cb14-2358" aria-hidden="true" tabindex="-1"></a>    style DCCheck fill:<span class="co">#fff4e6,stroke:#ff8800,stroke-width:2px</span></span>
<span id="cb14-2359"><a href="#cb14-2359" aria-hidden="true" tabindex="-1"></a>    style Result fill:<span class="co">#ccffcc,stroke:#00aa44,stroke-width:3px,color:#000</span></span>
<span id="cb14-2360"><a href="#cb14-2360" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-2361"><a href="#cb14-2361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2362"><a href="#cb14-2362" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 1: Data Quality</span></span>
<span id="cb14-2363"><a href="#cb14-2363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2364"><a href="#cb14-2364" aria-hidden="true" tabindex="-1"></a>**High-quality data is accurate, consistent, and properly processed**</span>
<span id="cb14-2365"><a href="#cb14-2365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2366"><a href="#cb14-2366" aria-hidden="true" tabindex="-1"></a>**For satellite imagery:**</span>
<span id="cb14-2367"><a href="#cb14-2367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2368"><a href="#cb14-2368" aria-hidden="true" tabindex="-1"></a>**Quality issues to address:**</span>
<span id="cb14-2369"><a href="#cb14-2369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2370"><a href="#cb14-2370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud contamination:** Use Level-2A with SCL cloud masks, aggressive filtering</span>
<span id="cb14-2371"><a href="#cb14-2371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Atmospheric effects:** Always use atmospherically corrected data (surface reflectance, not TOA)</span>
<span id="cb14-2372"><a href="#cb14-2372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensor artifacts:** Check for striping, banding, saturation, dead pixels</span>
<span id="cb14-2373"><a href="#cb14-2373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Geometric accuracy:** Ensure sub-pixel registration across time and sensors</span>
<span id="cb14-2374"><a href="#cb14-2374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radiometric consistency:** Calibrate across sensors and acquisition times</span>
<span id="cb14-2375"><a href="#cb14-2375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal alignment:** Match acquisition dates to ground conditions (phenology, seasonal changes)</span>
<span id="cb14-2376"><a href="#cb14-2376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2377"><a href="#cb14-2377" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-2378"><a href="#cb14-2378" aria-hidden="true" tabindex="-1"></a>**Philippine Challenge: Cloud Cover**</span>
<span id="cb14-2379"><a href="#cb14-2379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2380"><a href="#cb14-2380" aria-hidden="true" tabindex="-1"></a>Philippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon season).</span>
<span id="cb14-2381"><a href="#cb14-2381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2382"><a href="#cb14-2382" aria-hidden="true" tabindex="-1"></a>**Data quality solutions:**</span>
<span id="cb14-2383"><a href="#cb14-2383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-temporal compositing:** Median over 3-6 months to reduce cloud impact</span>
<span id="cb14-2384"><a href="#cb14-2384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-sensor fusion:** Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds</span>
<span id="cb14-2385"><a href="#cb14-2385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Aggressive cloud masking:** Accept fewer images for higher quality (quality &gt; quantity)</span>
<span id="cb14-2386"><a href="#cb14-2386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Leverage dry season:** December-May for optical data acquisition</span>
<span id="cb14-2387"><a href="#cb14-2387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep learning reconstruction:** Prithvi-EO-2.0 demonstrated cloud gap reconstruction</span>
<span id="cb14-2388"><a href="#cb14-2388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal interpolation:** Fill gaps using adjacent clear observations</span>
<span id="cb14-2389"><a href="#cb14-2389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2390"><a href="#cb14-2390" aria-hidden="true" tabindex="-1"></a>**DATOS System Approach:**</span>
<span id="cb14-2391"><a href="#cb14-2391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prioritize Sentinel-1 SAR during typhoon season (cloud-independent)</span>
<span id="cb14-2392"><a href="#cb14-2392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rapid processing (10-20 minutes) for disaster response</span>
<span id="cb14-2393"><a href="#cb14-2393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-temporal composites for flood extent mapping</span>
<span id="cb14-2394"><a href="#cb14-2394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with pre-event optical data for context</span>
<span id="cb14-2395"><a href="#cb14-2395" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2396"><a href="#cb14-2396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2397"><a href="#cb14-2397" aria-hidden="true" tabindex="-1"></a>**For training labels:**</span>
<span id="cb14-2398"><a href="#cb14-2398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2399"><a href="#cb14-2399" aria-hidden="true" tabindex="-1"></a>**Quality issues:**</span>
<span id="cb14-2400"><a href="#cb14-2400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2401"><a href="#cb14-2401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Positional error:** GPS drift (±5-10m common), georeferencing mismatch</span>
<span id="cb14-2402"><a href="#cb14-2402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal mismatch:** 2018 labels with 2020 imagery (land cover changes)</span>
<span id="cb14-2403"><a href="#cb14-2403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class ambiguity:** Unclear definitions (shrub vs. sparse forest? informal settlement vs. slum?)</span>
<span id="cb14-2404"><a href="#cb14-2404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mixed pixels:** Polygon boundaries include multiple classes (especially at coarse resolutions)</span>
<span id="cb14-2405"><a href="#cb14-2405" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Labeling inconsistency:** Different interpreters apply different criteria</span>
<span id="cb14-2406"><a href="#cb14-2406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Edge effects:** Boundaries between classes often have high uncertainty</span>
<span id="cb14-2407"><a href="#cb14-2407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale mismatch:** Labels created at different resolution than imagery</span>
<span id="cb14-2408"><a href="#cb14-2408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2409"><a href="#cb14-2409" aria-hidden="true" tabindex="-1"></a>**Best practices:**</span>
<span id="cb14-2410"><a href="#cb14-2410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2411"><a href="#cb14-2411" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Clear class definitions:** Document what each class includes/excludes with examples</span>
<span id="cb14-2412"><a href="#cb14-2412" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consistent methodology:** Same interpreter(s), same time of year, same reference imagery</span>
<span id="cb14-2413"><a href="#cb14-2413" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Quality control:** Multiple reviewers, consensus protocols, inter-annotator agreement metrics</span>
<span id="cb14-2414"><a href="#cb14-2414" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Temporal alignment:** Labels contemporary with imagery (within months for dynamic classes)</span>
<span id="cb14-2415"><a href="#cb14-2415" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Positional accuracy:** Use high-resolution reference imagery (VHR, Google Earth)</span>
<span id="cb14-2416"><a href="#cb14-2416" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Buffer boundaries:** Consider excluding mixed pixels at class boundaries from training</span>
<span id="cb14-2417"><a href="#cb14-2417" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Metadata:** Record labeling conditions, interpreter, date, confidence level</span>
<span id="cb14-2418"><a href="#cb14-2418" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Iterative refinement:** Use model predictions to identify and correct label errors</span>
<span id="cb14-2419"><a href="#cb14-2419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2420"><a href="#cb14-2420" aria-hidden="true" tabindex="-1"></a>**Training Data Errors Impact:**</span>
<span id="cb14-2421"><a href="#cb14-2421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2422"><a href="#cb14-2422" aria-hidden="true" tabindex="-1"></a>Research shows training data errors cause substantial errors in final predictions. Example scenarios:</span>
<span id="cb14-2423"><a href="#cb14-2423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mislabeled rice paddies → Model confuses rice with other crops</span>
<span id="cb14-2424"><a href="#cb14-2424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal mismatch → Model learns outdated patterns</span>
<span id="cb14-2425"><a href="#cb14-2425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Positional errors → Model learns from wrong pixels</span>
<span id="cb14-2426"><a href="#cb14-2426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inconsistent labels → Model learns noise rather than signal</span>
<span id="cb14-2427"><a href="#cb14-2427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2428"><a href="#cb14-2428" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 2: Data Quantity</span></span>
<span id="cb14-2429"><a href="#cb14-2429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2430"><a href="#cb14-2430" aria-hidden="true" tabindex="-1"></a>**More data (usually) improves performance, but quality matters more!**</span>
<span id="cb14-2431"><a href="#cb14-2431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2432"><a href="#cb14-2432" aria-hidden="true" tabindex="-1"></a>**How much data do you need?**</span>
<span id="cb14-2433"><a href="#cb14-2433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2434"><a href="#cb14-2434" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Algorithm <span class="pp">|</span> Typical Requirements <span class="pp">|</span> With Transfer Learning <span class="pp">|</span></span>
<span id="cb14-2435"><a href="#cb14-2435" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|---------------------|------------------------|</span></span>
<span id="cb14-2436"><a href="#cb14-2436" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Random Forest <span class="pp">|</span> 100s - 1000s samples per class <span class="pp">|</span> Same <span class="pp">|</span></span>
<span id="cb14-2437"><a href="#cb14-2437" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SVM <span class="pp">|</span> 100s - 1000s samples <span class="pp">|</span> Same <span class="pp">|</span></span>
<span id="cb14-2438"><a href="#cb14-2438" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Simple CNN <span class="pp">|</span> 1000s - 10,000s samples <span class="pp">|</span> 100s - 1000s <span class="pp">|</span></span>
<span id="cb14-2439"><a href="#cb14-2439" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Deep CNN (ResNet, U-Net) <span class="pp">|</span> 10,000s - 100,000s samples <span class="pp">|</span> 1000s - 10,000s <span class="pp">|</span></span>
<span id="cb14-2440"><a href="#cb14-2440" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Vision Transformer <span class="pp">|</span> 100,000s - millions <span class="pp">|</span> 10,000s - 100,000s <span class="pp">|</span></span>
<span id="cb14-2441"><a href="#cb14-2441" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Foundation Models (pre-training) <span class="pp">|</span> Millions - billions <span class="pp">|</span> N/A (already pre-trained) <span class="pp">|</span></span>
<span id="cb14-2442"><a href="#cb14-2442" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Foundation Models (fine-tuning) <span class="pp">|</span> N/A <span class="pp">|</span> 100s - 1000s <span class="pp">|</span></span>
<span id="cb14-2443"><a href="#cb14-2443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2444"><a href="#cb14-2444" aria-hidden="true" tabindex="-1"></a>**Strategies when labeled data is limited:**</span>
<span id="cb14-2445"><a href="#cb14-2445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2446"><a href="#cb14-2446" aria-hidden="true" tabindex="-1"></a>**1. Data Augmentation**</span>
<span id="cb14-2447"><a href="#cb14-2447" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Geometric:** Rotation, flipping, cropping, scaling, translation</span>
<span id="cb14-2448"><a href="#cb14-2448" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Photometric:** Brightness, contrast, saturation adjustments</span>
<span id="cb14-2449"><a href="#cb14-2449" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Noise addition:** Gaussian noise, salt-and-pepper</span>
<span id="cb14-2450"><a href="#cb14-2450" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Spectral:** Band dropout, mixup between spectral signatures</span>
<span id="cb14-2451"><a href="#cb14-2451" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Caution:** Ensure augmentations are realistic for EO (e.g., don't flip images with clear up/down orientation)</span>
<span id="cb14-2452"><a href="#cb14-2452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2453"><a href="#cb14-2453" aria-hidden="true" tabindex="-1"></a>**2. Transfer Learning**</span>
<span id="cb14-2454"><a href="#cb14-2454" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Use model pre-trained on large dataset (ImageNet, SatMAE, Prithvi)</span>
<span id="cb14-2455"><a href="#cb14-2455" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fine-tune on your small dataset</span>
<span id="cb14-2456"><a href="#cb14-2456" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Leverages learned features from similar tasks</span>
<span id="cb14-2457"><a href="#cb14-2457" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Reduces data requirements by 10-100×**</span>
<span id="cb14-2458"><a href="#cb14-2458" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Philippine poverty mapping example: 14.1% improvement using transfer learning</span>
<span id="cb14-2459"><a href="#cb14-2459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2460"><a href="#cb14-2460" aria-hidden="true" tabindex="-1"></a>**3. Active Learning**</span>
<span id="cb14-2461"><a href="#cb14-2461" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Process:** Iteratively train model → find uncertain predictions → label those → retrain</span>
<span id="cb14-2462"><a href="#cb14-2462" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Efficiently focuses labeling effort where it matters most</span>
<span id="cb14-2463"><a href="#cb14-2463" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Research shows 27% improvement in mIoU with only 2% labeled data</span>
<span id="cb14-2464"><a href="#cb14-2464" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Prioritize samples near decision boundaries</span>
<span id="cb14-2465"><a href="#cb14-2465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2466"><a href="#cb14-2466" aria-hidden="true" tabindex="-1"></a>**4. Few-Shot Learning**</span>
<span id="cb14-2467"><a href="#cb14-2467" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Methods:** Metric learning, meta-learning, prototypical networks</span>
<span id="cb14-2468"><a href="#cb14-2468" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Learn from very few examples per class</span>
<span id="cb14-2469"><a href="#cb14-2469" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Gerry Roxas Foundation deforestation: 43% accuracy with only 8% training data</span>
<span id="cb14-2470"><a href="#cb14-2470" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Useful for rare classes or novel geographic regions</span>
<span id="cb14-2471"><a href="#cb14-2471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2472"><a href="#cb14-2472" aria-hidden="true" tabindex="-1"></a>**5. Weak Supervision**</span>
<span id="cb14-2473"><a href="#cb14-2473" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Leverage noisy or incomplete labels</span>
<span id="cb14-2474"><a href="#cb14-2474" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**WeakAL framework:** Combines active learning and weak supervision</span>
<span id="cb14-2475"><a href="#cb14-2475" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Computes &gt;90% of labels automatically while maintaining competitive performance</span>
<span id="cb14-2476"><a href="#cb14-2476" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Trade-off: Lower individual label quality, but much larger quantity</span>
<span id="cb14-2477"><a href="#cb14-2477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2478"><a href="#cb14-2478" aria-hidden="true" tabindex="-1"></a>**6. Synthetic Data**</span>
<span id="cb14-2479"><a href="#cb14-2479" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Generate training data via simulation or GANs</span>
<span id="cb14-2480"><a href="#cb14-2480" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Example: Simulated SAR scenes for flood detection</span>
<span id="cb14-2481"><a href="#cb14-2481" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Useful when real data is dangerous/expensive to collect</span>
<span id="cb14-2482"><a href="#cb14-2482" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Caution: Domain gap between synthetic and real data</span>
<span id="cb14-2483"><a href="#cb14-2483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2484"><a href="#cb14-2484" aria-hidden="true" tabindex="-1"></a>**7. Self-Supervised Pre-training**</span>
<span id="cb14-2485"><a href="#cb14-2485" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Pre-train on unlabeled data (masked autoencoding, contrastive learning)</span>
<span id="cb14-2486"><a href="#cb14-2486" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fine-tune on small labeled dataset</span>
<span id="cb14-2487"><a href="#cb14-2487" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Foundation models (Prithvi) exemplify this approach</span>
<span id="cb14-2488"><a href="#cb14-2488" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**SSL4EO-S12:** Large-scale dataset for self-supervised learning</span>
<span id="cb14-2489"><a href="#cb14-2489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2490"><a href="#cb14-2490" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-2491"><a href="#cb14-2491" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2024 Research: Data Efficiency</span></span>
<span id="cb14-2492"><a href="#cb14-2492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2493"><a href="#cb14-2493" aria-hidden="true" tabindex="-1"></a>**Findings from "Data-Centric Machine Learning for Earth Observation" (arXiv 2024):**</span>
<span id="cb14-2494"><a href="#cb14-2494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2495"><a href="#cb14-2495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some EO tasks reach optimal accuracy with **&lt;20% of temporal instances**</span>
<span id="cb14-2496"><a href="#cb14-2496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Single band** from single sensor can be sufficient for specific tasks</span>
<span id="cb14-2497"><a href="#cb14-2497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Implication:** Smart data selection &gt; brute force data collection</span>
<span id="cb14-2498"><a href="#cb14-2498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature selection and dimensionality reduction crucial</span>
<span id="cb14-2499"><a href="#cb14-2499" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use PCA, tree-based feature importance, or domain knowledge to identify essential features</span>
<span id="cb14-2500"><a href="#cb14-2500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2501"><a href="#cb14-2501" aria-hidden="true" tabindex="-1"></a>**Takeaway:** Focus on acquiring diverse, high-quality samples rather than maximizing quantity indiscriminately.</span>
<span id="cb14-2502"><a href="#cb14-2502" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2503"><a href="#cb14-2503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2504"><a href="#cb14-2504" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-2505"><a href="#cb14-2505" aria-hidden="true" tabindex="-1"></a>**Philippine Solution: ALaM Project (DOST-ASTI)**</span>
<span id="cb14-2506"><a href="#cb14-2506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2507"><a href="#cb14-2507" aria-hidden="true" tabindex="-1"></a>**Automated Labeling Machine (ALaM)** addresses annotation bottleneck:</span>
<span id="cb14-2508"><a href="#cb14-2508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2509"><a href="#cb14-2509" aria-hidden="true" tabindex="-1"></a>**Approach:**</span>
<span id="cb14-2510"><a href="#cb14-2510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automated labeling:** ML models generate initial labels</span>
<span id="cb14-2511"><a href="#cb14-2511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Crowdsourcing:** Distributed verification and correction</span>
<span id="cb14-2512"><a href="#cb14-2512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Human-in-the-loop quality control:** Expert review of uncertain labels</span>
<span id="cb14-2513"><a href="#cb14-2513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Active learning integration:** Prioritize samples for human review</span>
<span id="cb14-2514"><a href="#cb14-2514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2515"><a href="#cb14-2515" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb14-2516"><a href="#cb14-2516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Significantly reduces labeling time and cost</span>
<span id="cb14-2517"><a href="#cb14-2517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scales to national coverage</span>
<span id="cb14-2518"><a href="#cb14-2518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with DIMER model repository for continuous improvement</span>
<span id="cb14-2519"><a href="#cb14-2519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Democratizes access to labeled training data</span>
<span id="cb14-2520"><a href="#cb14-2520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2521"><a href="#cb14-2521" aria-hidden="true" tabindex="-1"></a>**Integration with SkAI-Pinas:**</span>
<span id="cb14-2522"><a href="#cb14-2522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Part of national AI framework</span>
<span id="cb14-2523"><a href="#cb14-2523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Addresses gap between abundant remote sensing data and sustainable AI pipelines</span>
<span id="cb14-2524"><a href="#cb14-2524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Supports operational systems like DATOS and PRiSM</span>
<span id="cb14-2525"><a href="#cb14-2525" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2526"><a href="#cb14-2526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2527"><a href="#cb14-2527" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 3: Data Diversity</span></span>
<span id="cb14-2528"><a href="#cb14-2528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2529"><a href="#cb14-2529" aria-hidden="true" tabindex="-1"></a>**Representative data covers the full range of scenarios the model will encounter**</span>
<span id="cb14-2530"><a href="#cb14-2530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2531"><a href="#cb14-2531" aria-hidden="true" tabindex="-1"></a>Models trained on narrow data distributions fail when deployed in diverse real-world conditions. Diversity ensures robustness and generalization.</span>
<span id="cb14-2532"><a href="#cb14-2532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2533"><a href="#cb14-2533" aria-hidden="true" tabindex="-1"></a>**Dimensions of diversity:**</span>
<span id="cb14-2534"><a href="#cb14-2534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2535"><a href="#cb14-2535" aria-hidden="true" tabindex="-1"></a>**1. Geographic diversity**</span>
<span id="cb14-2536"><a href="#cb14-2536" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different regions (Luzon, Visayas, Mindanao)</span>
<span id="cb14-2537"><a href="#cb14-2537" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different ecosystems (lowland rainforest, montane cloud forest, mangrove, coral reef)</span>
<span id="cb14-2538"><a href="#cb14-2538" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different climate zones (Type I-IV Philippine climate classification)</span>
<span id="cb14-2539"><a href="#cb14-2539" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Urban, peri-urban, rural contexts</span>
<span id="cb14-2540"><a href="#cb14-2540" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different topography (flat, hilly, mountainous)</span>
<span id="cb14-2541"><a href="#cb14-2541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2542"><a href="#cb14-2542" aria-hidden="true" tabindex="-1"></a>**2. Temporal diversity**</span>
<span id="cb14-2543"><a href="#cb14-2543" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different seasons (wet season: June-Nov, dry season: Dec-May)</span>
<span id="cb14-2544"><a href="#cb14-2544" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different years (inter-annual variability, El Niño vs. La Niña)</span>
<span id="cb14-2545"><a href="#cb14-2545" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different phenological stages (rice: planting, vegetative, reproductive, maturity)</span>
<span id="cb14-2546"><a href="#cb14-2546" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different times of day (for SAR: morning vs. evening passes)</span>
<span id="cb14-2547"><a href="#cb14-2547" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Historical baselines and recent conditions</span>
<span id="cb14-2548"><a href="#cb14-2548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2549"><a href="#cb14-2549" aria-hidden="true" tabindex="-1"></a>**3. Class diversity**</span>
<span id="cb14-2550"><a href="#cb14-2550" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Multiple examples per class capturing intra-class variability</span>
<span id="cb14-2551"><a href="#cb14-2551" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Edge cases and rare types (e.g., burned forest, flooded agriculture)</span>
<span id="cb14-2552"><a href="#cb14-2552" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Transitional zones (forest-agriculture boundary, urban-rural fringe)</span>
<span id="cb14-2553"><a href="#cb14-2553" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different sub-types (e.g., rice varieties, mangrove species, building materials)</span>
<span id="cb14-2554"><a href="#cb14-2554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2555"><a href="#cb14-2555" aria-hidden="true" tabindex="-1"></a>**4. Sensor diversity**</span>
<span id="cb14-2556"><a href="#cb14-2556" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different satellites (Sentinel-2A, 2B, 2C)</span>
<span id="cb14-2557"><a href="#cb14-2557" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different atmospheric conditions (clear, hazy, dusty)</span>
<span id="cb14-2558"><a href="#cb14-2558" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different viewing angles (SAR: ascending vs. descending)</span>
<span id="cb14-2559"><a href="#cb14-2559" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different processing baselines (if applicable)</span>
<span id="cb14-2560"><a href="#cb14-2560" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Multi-sensor when relevant (optical + SAR)</span>
<span id="cb14-2561"><a href="#cb14-2561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2562"><a href="#cb14-2562" aria-hidden="true" tabindex="-1"></a>**5. Socioeconomic diversity**</span>
<span id="cb14-2563"><a href="#cb14-2563" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different development contexts (high-density urban, informal settlements, rural villages)</span>
<span id="cb14-2564"><a href="#cb14-2564" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different agricultural practices (mechanized, traditional, mixed)</span>
<span id="cb14-2565"><a href="#cb14-2565" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Different infrastructure quality (paved roads, dirt tracks)</span>
<span id="cb14-2566"><a href="#cb14-2566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2567"><a href="#cb14-2567" aria-hidden="true" tabindex="-1"></a>**Example: Urban classification**</span>
<span id="cb14-2568"><a href="#cb14-2568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2569"><a href="#cb14-2569" aria-hidden="true" tabindex="-1"></a>**Poor diversity:** All training samples from Metro Manila CBD (Central Business District)</span>
<span id="cb14-2570"><a href="#cb14-2570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2571"><a href="#cb14-2571" aria-hidden="true" tabindex="-1"></a>**Result:** Model fails on:</span>
<span id="cb14-2572"><a href="#cb14-2572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small provincial towns (different building density, height, materials)</span>
<span id="cb14-2573"><a href="#cb14-2573" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Informal settlements (different patterns, materials, roof types)</span>
<span id="cb14-2574"><a href="#cb14-2574" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Peri-urban areas (mixed land cover, agriculture near buildings)</span>
<span id="cb14-2575"><a href="#cb14-2575" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Historical centers (older building styles)</span>
<span id="cb14-2576"><a href="#cb14-2576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2577"><a href="#cb14-2577" aria-hidden="true" tabindex="-1"></a>**Good diversity:** Samples from:</span>
<span id="cb14-2578"><a href="#cb14-2578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Large cities:** Manila, Cebu, Davao (high-density, modern buildings)</span>
<span id="cb14-2579"><a href="#cb14-2579" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Medium towns:** Baguio, Iloilo, Cagayan de Oro (mixed density)</span>
<span id="cb14-2580"><a href="#cb14-2580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Small municipalities:** Various provinces</span>
<span id="cb14-2581"><a href="#cb14-2581" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Different building materials:** Concrete, metal roofing, nipa huts, wood</span>
<span id="cb14-2582"><a href="#cb14-2582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Different periods:** Capture urban growth and change</span>
<span id="cb14-2583"><a href="#cb14-2583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Informal settlements:** Slums, squatter areas</span>
<span id="cb14-2584"><a href="#cb14-2584" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Peri-urban:** Transition zones</span>
<span id="cb14-2585"><a href="#cb14-2585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2586"><a href="#cb14-2586" aria-hidden="true" tabindex="-1"></a>**Result:** Model generalizes well across Philippines</span>
<span id="cb14-2587"><a href="#cb14-2587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2588"><a href="#cb14-2588" aria-hidden="true" tabindex="-1"></a>**Validation of Diversity:**</span>
<span id="cb14-2589"><a href="#cb14-2589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2590"><a href="#cb14-2590" aria-hidden="true" tabindex="-1"></a>Test model performance on stratified subsets:</span>
<span id="cb14-2591"><a href="#cb14-2591" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Per-region accuracy (does it work in all islands?)</span>
<span id="cb14-2592"><a href="#cb14-2592" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Per-season accuracy (dry vs. wet season)</span>
<span id="cb14-2593"><a href="#cb14-2593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Per-class accuracy (all classes represented equally well?)</span>
<span id="cb14-2594"><a href="#cb14-2594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cross-region generalization (train on Luzon, test on Mindanao)</span>
<span id="cb14-2595"><a href="#cb14-2595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2596"><a href="#cb14-2596" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pillar 4: Annotation Strategy</span></span>
<span id="cb14-2597"><a href="#cb14-2597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2598"><a href="#cb14-2598" aria-hidden="true" tabindex="-1"></a>**How you label data profoundly impacts model performance**</span>
<span id="cb14-2599"><a href="#cb14-2599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2600"><a href="#cb14-2600" aria-hidden="true" tabindex="-1"></a>Annotation is often the most expensive and time-consuming part of ML workflow. Strategic annotation maximizes value.</span>
<span id="cb14-2601"><a href="#cb14-2601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2602"><a href="#cb14-2602" aria-hidden="true" tabindex="-1"></a>**Annotation approaches:**</span>
<span id="cb14-2603"><a href="#cb14-2603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2604"><a href="#cb14-2604" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Point sampling:** Fast, but limited context, suitable for classification</span>
<span id="cb14-2605"><a href="#cb14-2605" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Polygon delineation:** More information, more time-consuming, required for semantic segmentation</span>
<span id="cb14-2606"><a href="#cb14-2606" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Pixel-level labeling:** Maximum detail, most expensive, essential for precise segmentation</span>
<span id="cb14-2607"><a href="#cb14-2607" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Image-level labels:** Easiest, suitable for scene classification, limited spatial information</span>
<span id="cb14-2608"><a href="#cb14-2608" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Bounding boxes:** For object detection, faster than pixel-level masks</span>
<span id="cb14-2609"><a href="#cb14-2609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2610"><a href="#cb14-2610" aria-hidden="true" tabindex="-1"></a>**Best practices:**</span>
<span id="cb14-2611"><a href="#cb14-2611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2612"><a href="#cb14-2612" aria-hidden="true" tabindex="-1"></a>**1. Expert involvement**</span>
<span id="cb14-2613"><a href="#cb14-2613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use domain experts for complex classes (forest types, crop stages, mangrove species)</span>
<span id="cb14-2614"><a href="#cb14-2614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train labelers thoroughly on class definitions with examples</span>
<span id="cb14-2615"><a href="#cb14-2615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regular calibration sessions to maintain consistency</span>
<span id="cb14-2616"><a href="#cb14-2616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Document difficult cases and edge cases</span>
<span id="cb14-2617"><a href="#cb14-2617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2618"><a href="#cb14-2618" aria-hidden="true" tabindex="-1"></a>**2. Quality over quantity**</span>
<span id="cb14-2619"><a href="#cb14-2619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**500 high-quality labels &gt; 5000 noisy labels**</span>
<span id="cb14-2620"><a href="#cb14-2620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Invest in review and correction processes</span>
<span id="cb14-2621"><a href="#cb14-2621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Document difficult cases and ambiguous examples</span>
<span id="cb14-2622"><a href="#cb14-2622" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use confidence scores to flag uncertain labels</span>
<span id="cb14-2623"><a href="#cb14-2623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2624"><a href="#cb14-2624" aria-hidden="true" tabindex="-1"></a>**3. Class balance**</span>
<span id="cb14-2625"><a href="#cb14-2625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensure adequate representation of minority classes</span>
<span id="cb14-2626"><a href="#cb14-2626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stratified sampling by class (not just random)</span>
<span id="cb14-2627"><a href="#cb14-2627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider class weights in training if imbalanced</span>
<span id="cb14-2628"><a href="#cb14-2628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Oversampling rare classes or undersampling common classes</span>
<span id="cb14-2629"><a href="#cb14-2629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Imbalanced classes:** Major challenge in EO (e.g., rare disasters, rare land cover types)</span>
<span id="cb14-2630"><a href="#cb14-2630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2631"><a href="#cb14-2631" aria-hidden="true" tabindex="-1"></a>**4. Consensus protocols**</span>
<span id="cb14-2632"><a href="#cb14-2632" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple labelers per sample (especially for ambiguous cases)</span>
<span id="cb14-2633"><a href="#cb14-2633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Majority vote or adjudication for disagreements</span>
<span id="cb14-2634"><a href="#cb14-2634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measure inter-annotator agreement (Cohen's Kappa, Krippendorff's Alpha)</span>
<span id="cb14-2635"><a href="#cb14-2635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Establish minimum agreement threshold (e.g., 80%)</span>
<span id="cb14-2636"><a href="#cb14-2636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2637"><a href="#cb14-2637" aria-hidden="true" tabindex="-1"></a>**5. Iterative refinement**</span>
<span id="cb14-2638"><a href="#cb14-2638" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use model predictions to find label errors (disagreement between model and label)</span>
<span id="cb14-2639"><a href="#cb14-2639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Retrain after improving labels (data-centric iteration)</span>
<span id="cb14-2640"><a href="#cb14-2640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus effort on low-confidence predictions</span>
<span id="cb14-2641"><a href="#cb14-2641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model-in-the-loop labeling:** Model suggests labels, humans verify</span>
<span id="cb14-2642"><a href="#cb14-2642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2643"><a href="#cb14-2643" aria-hidden="true" tabindex="-1"></a>**6. Annotation tools and platforms**</span>
<span id="cb14-2644"><a href="#cb14-2644" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use efficient labeling tools (LabelMe, CVAT, Label Studio, Labelbox)</span>
<span id="cb14-2645"><a href="#cb14-2645" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For EO: Tools supporting geospatial formats (GeoTIFF, shapefiles)</span>
<span id="cb14-2646"><a href="#cb14-2646" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with cloud platforms (Google Earth Engine, QGIS)</span>
<span id="cb14-2647"><a href="#cb14-2647" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Export to ML-ready formats</span>
<span id="cb14-2648"><a href="#cb14-2648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2649"><a href="#cb14-2649" aria-hidden="true" tabindex="-1"></a>**7. Crowdsourcing considerations**</span>
<span id="cb14-2650"><a href="#cb14-2650" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Clear instructions and examples</span>
<span id="cb14-2651"><a href="#cb14-2651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Quality control through redundancy and expert review</span>
<span id="cb14-2652"><a href="#cb14-2652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gamification to maintain engagement</span>
<span id="cb14-2653"><a href="#cb14-2653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Examples: Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping</span>
<span id="cb14-2654"><a href="#cb14-2654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2655"><a href="#cb14-2655" aria-hidden="true" tabindex="-1"></a>**EO-Specific Annotation Challenges:**</span>
<span id="cb14-2656"><a href="#cb14-2656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2657"><a href="#cb14-2657" aria-hidden="true" tabindex="-1"></a>From Kili Technology's Earth Observation Data Labeling Guide:</span>
<span id="cb14-2658"><a href="#cb14-2658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensor diversity:** Different spectral bands, resolutions, formats</span>
<span id="cb14-2659"><a href="#cb14-2659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Massive data volumes:** Petabyte-scale archives ("four Vs": Volume, Velocity, Variety, Veracity)</span>
<span id="cb14-2660"><a href="#cb14-2660" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Domain expertise requirements:** Complex classes require specialized knowledge</span>
<span id="cb14-2661"><a href="#cb14-2661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Weak labeling approaches:** Leverage noisy labels, distant supervision</span>
<span id="cb14-2662"><a href="#cb14-2662" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Active learning integration:** Prioritize informative samples</span>
<span id="cb14-2663"><a href="#cb14-2663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stakeholder-friendly tooling:** Tools accessible to non-ML experts</span>
<span id="cb14-2664"><a href="#cb14-2664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2665"><a href="#cb14-2665" aria-hidden="true" tabindex="-1"></a>::: {.philippine-context}</span>
<span id="cb14-2666"><a href="#cb14-2666" aria-hidden="true" tabindex="-1"></a>**Philippine Annotation Ecosystem:**</span>
<span id="cb14-2667"><a href="#cb14-2667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2668"><a href="#cb14-2668" aria-hidden="true" tabindex="-1"></a>**ALaM (Automated Labeling Machine - DOST-ASTI):**</span>
<span id="cb14-2669"><a href="#cb14-2669" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines automated labeling with crowdsourcing</span>
<span id="cb14-2670"><a href="#cb14-2670" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Human-in-the-loop quality control</span>
<span id="cb14-2671"><a href="#cb14-2671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with DIMER model repository</span>
<span id="cb14-2672"><a href="#cb14-2672" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces labeling time and cost significantly</span>
<span id="cb14-2673"><a href="#cb14-2673" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Workflow:** Automated labels → Crowdsourced verification → Expert review → Training data</span>
<span id="cb14-2674"><a href="#cb14-2674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2675"><a href="#cb14-2675" aria-hidden="true" tabindex="-1"></a>**DATOS (DOST-ASTI):**</span>
<span id="cb14-2676"><a href="#cb14-2676" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rapid disaster mapping (10-20 minute response)</span>
<span id="cb14-2677"><a href="#cb14-2677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>On-the-fly labeling during disaster response</span>
<span id="cb14-2678"><a href="#cb14-2678" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Iterative refinement based on ground validation</span>
<span id="cb14-2679"><a href="#cb14-2679" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with LGU feedback</span>
<span id="cb14-2680"><a href="#cb14-2680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2681"><a href="#cb14-2681" aria-hidden="true" tabindex="-1"></a>**Academic Partnerships:**</span>
<span id="cb14-2682"><a href="#cb14-2682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>University of the Philippines - remote sensing courses with labeling components</span>
<span id="cb14-2683"><a href="#cb14-2683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilRice - rice field delineation and crop stage labeling</span>
<span id="cb14-2684"><a href="#cb14-2684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DENR - forest and mangrove mapping with expert foresters</span>
<span id="cb14-2685"><a href="#cb14-2685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2686"><a href="#cb14-2686" aria-hidden="true" tabindex="-1"></a>**International Support:**</span>
<span id="cb14-2687"><a href="#cb14-2687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping</span>
<span id="cb14-2688"><a href="#cb14-2688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CoPhil training programs on labeling best practices</span>
<span id="cb14-2689"><a href="#cb14-2689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>European Copernicus expertise transfer</span>
<span id="cb14-2690"><a href="#cb14-2690" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2691"><a href="#cb14-2691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2692"><a href="#cb14-2692" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2025 Examples: Data-Centric Success Stories</span></span>
<span id="cb14-2693"><a href="#cb14-2693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2694"><a href="#cb14-2694" aria-hidden="true" tabindex="-1"></a><span class="fu">#### NASA-IBM Geospatial Foundation Model (Prithvi)</span></span>
<span id="cb14-2695"><a href="#cb14-2695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2696"><a href="#cb14-2696" aria-hidden="true" tabindex="-1"></a>**Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)**</span>
<span id="cb14-2697"><a href="#cb14-2697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2698"><a href="#cb14-2698" aria-hidden="true" tabindex="-1"></a>**Data-centric approach:**</span>
<span id="cb14-2699"><a href="#cb14-2699" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale:** Millions of satellite images from HLS (30m resolution, global coverage)</span>
<span id="cb14-2700"><a href="#cb14-2700" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Self-supervised pre-training:** Masked autoencoding (no labels needed)</span>
<span id="cb14-2701"><a href="#cb14-2701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data quality:** HLS provides analysis-ready data (atmospheric correction, BRDF normalization, co-registration)</span>
<span id="cb14-2702"><a href="#cb14-2702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuned for specific tasks:** With small labeled datasets (100s-1000s samples)</span>
<span id="cb14-2703"><a href="#cb14-2703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2704"><a href="#cb14-2704" aria-hidden="true" tabindex="-1"></a>**Result:**</span>
<span id="cb14-2705"><a href="#cb14-2705" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art performance on multiple EO tasks (flood mapping, burn scar detection, crop segmentation)</span>
<span id="cb14-2706"><a href="#cb14-2706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reduces labeled data requirements by 10-100×**</span>
<span id="cb14-2707"><a href="#cb14-2707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Democratizes access to powerful EO AI</span>
<span id="cb14-2708"><a href="#cb14-2708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Foundation for operational systems worldwide</span>
<span id="cb14-2709"><a href="#cb14-2709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2710"><a href="#cb14-2710" aria-hidden="true" tabindex="-1"></a>**Key Insight:** Investment in massive, high-quality pre-training data enables downstream applications with minimal task-specific labels.</span>
<span id="cb14-2711"><a href="#cb14-2711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2712"><a href="#cb14-2712" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ESA Φsat-2 On-Board AI (Launched 2024)</span></span>
<span id="cb14-2713"><a href="#cb14-2713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2714"><a href="#cb14-2714" aria-hidden="true" tabindex="-1"></a>**22cm CubeSat with on-board AI processing**</span>
<span id="cb14-2715"><a href="#cb14-2715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2716"><a href="#cb14-2716" aria-hidden="true" tabindex="-1"></a>**Data-centric innovation:**</span>
<span id="cb14-2717"><a href="#cb14-2717" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Processes imagery directly on satellite</span>
<span id="cb14-2718"><a href="#cb14-2718" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data quality selection happens in space!**</span>
<span id="cb14-2719"><a href="#cb14-2719" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Only transmits actionable information (not raw data)</span>
<span id="cb14-2720"><a href="#cb14-2720" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cloud filtering: Only clear, usable images sent to Earth</span>
<span id="cb14-2721"><a href="#cb14-2721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces bandwidth requirements by orders of magnitude</span>
<span id="cb14-2722"><a href="#cb14-2722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables real-time event detection (fires, ships, clouds)</span>
<span id="cb14-2723"><a href="#cb14-2723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2724"><a href="#cb14-2724" aria-hidden="true" tabindex="-1"></a>**Rationale:**</span>
<span id="cb14-2725"><a href="#cb14-2725" aria-hidden="true" tabindex="-1"></a>With 1,052 active EO satellites generating thousands of terabytes daily, traditional radio frequency communication cannot relay this volume. On-board AI filters data at source.</span>
<span id="cb14-2726"><a href="#cb14-2726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2727"><a href="#cb14-2727" aria-hidden="true" tabindex="-1"></a>**Implication:**</span>
<span id="cb14-2728"><a href="#cb14-2728" aria-hidden="true" tabindex="-1"></a>Data quality and relevance prioritized over quantity. Shift from "collect everything" to "collect intelligently."</span>
<span id="cb14-2729"><a href="#cb14-2729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2730"><a href="#cb14-2730" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EarthDaily Constellation</span></span>
<span id="cb14-2731"><a href="#cb14-2731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2732"><a href="#cb14-2732" aria-hidden="true" tabindex="-1"></a>**10-satellite constellation for daily global coverage at 5-10m resolution**</span>
<span id="cb14-2733"><a href="#cb14-2733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2734"><a href="#cb14-2734" aria-hidden="true" tabindex="-1"></a>**Focus on AI-ready data:**</span>
<span id="cb14-2735"><a href="#cb14-2735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scientific-grade calibration:** Rigorous radiometric accuracy</span>
<span id="cb14-2736"><a href="#cb14-2736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Consistent, reliable acquisitions:** Predictable revisit times</span>
<span id="cb14-2737"><a href="#cb14-2737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optimized spectral bands for ML:** Bands selected based on ML feature importance</span>
<span id="cb14-2738"><a href="#cb14-2738" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Emphasis on data quality for algorithm performance:** Analysis-ready data products</span>
<span id="cb14-2739"><a href="#cb14-2739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2740"><a href="#cb14-2740" aria-hidden="true" tabindex="-1"></a>**Philosophy:**</span>
<span id="cb14-2741"><a href="#cb14-2741" aria-hidden="true" tabindex="-1"></a>Data quality and consistency are first-class design criteria, not afterthoughts. Build satellites around AI needs.</span>
<span id="cb14-2742"><a href="#cb14-2742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2743"><a href="#cb14-2743" aria-hidden="true" tabindex="-1"></a><span class="fu">#### WeakAL Framework (Active Learning + Weak Supervision)</span></span>
<span id="cb14-2744"><a href="#cb14-2744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2745"><a href="#cb14-2745" aria-hidden="true" tabindex="-1"></a>**Research from remote sensing ML community**</span>
<span id="cb14-2746"><a href="#cb14-2746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2747"><a href="#cb14-2747" aria-hidden="true" tabindex="-1"></a>**Approach:**</span>
<span id="cb14-2748"><a href="#cb14-2748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines active learning (select informative samples) with weak supervision (leverage noisy labels)</span>
<span id="cb14-2749"><a href="#cb14-2749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computes **&gt;90% of labels automatically** while maintaining competitive performance</span>
<span id="cb14-2750"><a href="#cb14-2750" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Human effort focused on most uncertain/informative samples</span>
<span id="cb14-2751"><a href="#cb14-2751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2752"><a href="#cb14-2752" aria-hidden="true" tabindex="-1"></a>**Results:**</span>
<span id="cb14-2753"><a href="#cb14-2753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>27% improvement in mIoU with only 2% manually labeled data</span>
<span id="cb14-2754"><a href="#cb14-2754" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Demonstrates data-efficient learning</span>
<span id="cb14-2755"><a href="#cb14-2755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Practical for large-scale operational mapping</span>
<span id="cb14-2756"><a href="#cb14-2756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2757"><a href="#cb14-2757" aria-hidden="true" tabindex="-1"></a>**Key Insight:** Strategic data selection and semi-automated labeling can achieve strong performance with minimal human effort.</span>
<span id="cb14-2758"><a href="#cb14-2758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2759"><a href="#cb14-2759" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2760"><a href="#cb14-2760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2761"><a href="#cb14-2761" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 7: Explainable AI (XAI) for Earth Observation</span></span>
<span id="cb14-2762"><a href="#cb14-2762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2763"><a href="#cb14-2763" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why XAI Matters in EO</span></span>
<span id="cb14-2764"><a href="#cb14-2764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2765"><a href="#cb14-2765" aria-hidden="true" tabindex="-1"></a>**The Problem:**</span>
<span id="cb14-2766"><a href="#cb14-2766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2767"><a href="#cb14-2767" aria-hidden="true" tabindex="-1"></a>Deep learning models are often **"black boxes"** - they produce accurate predictions, but we don't understand why. For operational EO systems, this creates challenges:</span>
<span id="cb14-2768"><a href="#cb14-2768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2769"><a href="#cb14-2769" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Scientific Insights:** Can't extract physical understanding from model decisions</span>
<span id="cb14-2770"><a href="#cb14-2770" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Bias Detection:** Can't identify if model relies on spurious correlations (e.g., cloud shadows, artifacts)</span>
<span id="cb14-2771"><a href="#cb14-2771" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Trust and Adoption:** Stakeholders reluctant to use models they don't understand</span>
<span id="cb14-2772"><a href="#cb14-2772" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Debugging:** Difficult to diagnose errors and improve models</span>
<span id="cb14-2773"><a href="#cb14-2773" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Regulatory/Policy:** Some applications require explainability (e.g., disaster fund allocation)</span>
<span id="cb14-2774"><a href="#cb14-2774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2775"><a href="#cb14-2775" aria-hidden="true" tabindex="-1"></a>**Recent Efforts (2023-2025):**</span>
<span id="cb14-2776"><a href="#cb14-2776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2777"><a href="#cb14-2777" aria-hidden="true" tabindex="-1"></a>Despite significant advances in deep learning for remote sensing, **lack of explainability remains a major criticism**. The community is increasingly exploring Explainable AI techniques:</span>
<span id="cb14-2778"><a href="#cb14-2778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2779"><a href="#cb14-2779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Increasingly intensive exploration of XAI methods for EO</span>
<span id="cb14-2780"><a href="#cb14-2780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration of attention visualization in transformer architectures</span>
<span id="cb14-2781"><a href="#cb14-2781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Saliency maps and feature attribution techniques</span>
<span id="cb14-2782"><a href="#cb14-2782" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trade-off studies: accuracy vs. interpretability</span>
<span id="cb14-2783"><a href="#cb14-2783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2784"><a href="#cb14-2784" aria-hidden="true" tabindex="-1"></a><span class="fu">### XAI Methods for EO</span></span>
<span id="cb14-2785"><a href="#cb14-2785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2786"><a href="#cb14-2786" aria-hidden="true" tabindex="-1"></a>**Gradient-Based Methods:**</span>
<span id="cb14-2787"><a href="#cb14-2787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2788"><a href="#cb14-2788" aria-hidden="true" tabindex="-1"></a>**Grad-CAM (Gradient-weighted Class Activation Mapping):**</span>
<span id="cb14-2789"><a href="#cb14-2789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Compute gradients of target class with respect to final convolutional layer</span>
<span id="cb14-2790"><a href="#cb14-2790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Heatmap highlighting regions important for prediction</span>
<span id="cb14-2791"><a href="#cb14-2791" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** Most interpretable method, computationally efficient, works with any CNN</span>
<span id="cb14-2792"><a href="#cb14-2792" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:** Visualize which parts of satellite image model focuses on (e.g., "model detects water by focusing on blue spectral signature")</span>
<span id="cb14-2793"><a href="#cb14-2793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2794"><a href="#cb14-2794" aria-hidden="true" tabindex="-1"></a>**Guided Backpropagation:**</span>
<span id="cb14-2795"><a href="#cb14-2795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualizes pixels contributing to prediction</span>
<span id="cb14-2796"><a href="#cb14-2796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sharper visualizations than Grad-CAM</span>
<span id="cb14-2797"><a href="#cb14-2797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Highlights fine-grained features</span>
<span id="cb14-2798"><a href="#cb14-2798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2799"><a href="#cb14-2799" aria-hidden="true" tabindex="-1"></a>**Integrated Gradients:**</span>
<span id="cb14-2800"><a href="#cb14-2800" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accumulates gradients along path from baseline to input</span>
<span id="cb14-2801"><a href="#cb14-2801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>More robust attributions than simple gradients</span>
<span id="cb14-2802"><a href="#cb14-2802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Satisfies desirable axioms (sensitivity, implementation invariance)</span>
<span id="cb14-2803"><a href="#cb14-2803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2804"><a href="#cb14-2804" aria-hidden="true" tabindex="-1"></a>**Perturbation-Based Methods:**</span>
<span id="cb14-2805"><a href="#cb14-2805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2806"><a href="#cb14-2806" aria-hidden="true" tabindex="-1"></a>**Occlusion:**</span>
<span id="cb14-2807"><a href="#cb14-2807" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Block image regions and observe prediction change</span>
<span id="cb14-2808"><a href="#cb14-2808" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Sensitivity map showing which regions are critical</span>
<span id="cb14-2809"><a href="#cb14-2809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** High interpretability, intuitive</span>
<span id="cb14-2810"><a href="#cb14-2810" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantages:** Computationally expensive (must test many occlusions)</span>
<span id="cb14-2811"><a href="#cb14-2811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2812"><a href="#cb14-2812" aria-hidden="true" tabindex="-1"></a>**LIME (Local Interpretable Model-agnostic Explanations):**</span>
<span id="cb14-2813"><a href="#cb14-2813" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Train simple, interpretable model (e.g., linear) to approximate complex model locally</span>
<span id="cb14-2814"><a href="#cb14-2814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Feature importances for specific prediction</span>
<span id="cb14-2815"><a href="#cb14-2815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** Model-agnostic, interpretable</span>
<span id="cb14-2816"><a href="#cb14-2816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Disadvantages:** Expensive computation, local rather than global explanation</span>
<span id="cb14-2817"><a href="#cb14-2817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2818"><a href="#cb14-2818" aria-hidden="true" tabindex="-1"></a>**Model-Based Methods:**</span>
<span id="cb14-2819"><a href="#cb14-2819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2820"><a href="#cb14-2820" aria-hidden="true" tabindex="-1"></a>**SHAP (SHapley Additive exPlanations):**</span>
<span id="cb14-2821"><a href="#cb14-2821" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Game theory approach - compute contribution of each feature</span>
<span id="cb14-2822"><a href="#cb14-2822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Feature importance values for prediction</span>
<span id="cb14-2823"><a href="#cb14-2823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** Theoretically grounded, consistent</span>
<span id="cb14-2824"><a href="#cb14-2824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:** Explain which spectral bands, indices, or temporal features drive predictions</span>
<span id="cb14-2825"><a href="#cb14-2825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2826"><a href="#cb14-2826" aria-hidden="true" tabindex="-1"></a>**Attention Visualization (for Transformers):**</span>
<span id="cb14-2827"><a href="#cb14-2827" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Process:** Visualize attention weights from self-attention mechanism</span>
<span id="cb14-2828"><a href="#cb14-2828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Heatmap showing which patches/regions model attends to</span>
<span id="cb14-2829"><a href="#cb14-2829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** Built into architecture, interpretable</span>
<span id="cb14-2830"><a href="#cb14-2830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:** Vision Transformers (ViT), UNetFormer - see which spatial regions model focuses on</span>
<span id="cb14-2831"><a href="#cb14-2831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2832"><a href="#cb14-2832" aria-hidden="true" tabindex="-1"></a>**Feature Importance (for Tree-Based Models):**</span>
<span id="cb14-2833"><a href="#cb14-2833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random Forest, XGBoost provide feature importance scores</span>
<span id="cb14-2834"><a href="#cb14-2834" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Ranking of features by contribution to predictions</span>
<span id="cb14-2835"><a href="#cb14-2835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advantages:** Simple, intuitive, built-in</span>
<span id="cb14-2836"><a href="#cb14-2836" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Applications:** Understand which spectral bands, indices, temporal features are most informative</span>
<span id="cb14-2837"><a href="#cb14-2837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2838"><a href="#cb14-2838" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applications in EO</span></span>
<span id="cb14-2839"><a href="#cb14-2839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2840"><a href="#cb14-2840" aria-hidden="true" tabindex="-1"></a>**1. Understanding Model Decisions:**</span>
<span id="cb14-2841"><a href="#cb14-2841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize which spectral bands contribute most (e.g., does model rely on SWIR for burn detection?)</span>
<span id="cb14-2842"><a href="#cb14-2842" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify spatial patterns model focuses on (e.g., texture vs. spectral signature)</span>
<span id="cb14-2843"><a href="#cb14-2843" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Discover unexpected correlations (e.g., model using cloud shadows instead of actual land cover)</span>
<span id="cb14-2844"><a href="#cb14-2844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2845"><a href="#cb14-2845" aria-hidden="true" tabindex="-1"></a>**2. Discovering Scientific Insights:**</span>
<span id="cb14-2846"><a href="#cb14-2846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify which vegetation indices are most predictive for crop types</span>
<span id="cb14-2847"><a href="#cb14-2847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand temporal patterns in multi-date imagery (which dates are critical for classification?)</span>
<span id="cb14-2848"><a href="#cb14-2848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extract biophysical relationships learned by model</span>
<span id="cb14-2849"><a href="#cb14-2849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2850"><a href="#cb14-2850" aria-hidden="true" tabindex="-1"></a>**3. Detecting and Mitigating Biases:**</span>
<span id="cb14-2851"><a href="#cb14-2851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify if model relies on artifacts (e.g., sensor striping, JPEG compression)</span>
<span id="cb14-2852"><a href="#cb14-2852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Detect geographic biases (model works in training region, fails elsewhere due to spurious features)</span>
<span id="cb14-2853"><a href="#cb14-2853" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensure model uses physically meaningful features</span>
<span id="cb14-2854"><a href="#cb14-2854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2855"><a href="#cb14-2855" aria-hidden="true" tabindex="-1"></a>**4. Building Trust with Stakeholders:**</span>
<span id="cb14-2856"><a href="#cb14-2856" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Demonstrate to policymakers that model decisions are reasonable</span>
<span id="cb14-2857"><a href="#cb14-2857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Show LGUs which features drive disaster risk predictions</span>
<span id="cb14-2858"><a href="#cb14-2858" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain to farmers why certain fields are flagged for attention</span>
<span id="cb14-2859"><a href="#cb14-2859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2860"><a href="#cb14-2860" aria-hidden="true" tabindex="-1"></a>**5. Debugging and Improving Models:**</span>
<span id="cb14-2861"><a href="#cb14-2861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify when model makes errors (e.g., confuses rice with water due to flooding)</span>
<span id="cb14-2862"><a href="#cb14-2862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Guide data collection (which features need more training samples?)</span>
<span id="cb14-2863"><a href="#cb14-2863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inform feature engineering (which derived features would help?)</span>
<span id="cb14-2864"><a href="#cb14-2864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2865"><a href="#cb14-2865" aria-hidden="true" tabindex="-1"></a><span class="fu">### Challenges and Trade-Offs</span></span>
<span id="cb14-2866"><a href="#cb14-2866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2867"><a href="#cb14-2867" aria-hidden="true" tabindex="-1"></a>**Accuracy vs. Interpretability:**</span>
<span id="cb14-2868"><a href="#cb14-2868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple models (decision trees, linear regression) are interpretable but less accurate</span>
<span id="cb14-2869"><a href="#cb14-2869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex models (deep CNNs, transformers) are more accurate but less interpretable</span>
<span id="cb14-2870"><a href="#cb14-2870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Trade-off:** Choose based on application criticality and stakeholder needs</span>
<span id="cb14-2871"><a href="#cb14-2871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2872"><a href="#cb14-2872" aria-hidden="true" tabindex="-1"></a>**Computational Cost:**</span>
<span id="cb14-2873"><a href="#cb14-2873" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Post-hoc explanation methods (LIME, occlusion) can be expensive</span>
<span id="cb14-2874"><a href="#cb14-2874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradient-based methods (Grad-CAM) are fast</span>
<span id="cb14-2875"><a href="#cb14-2875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider explanation cost for operational systems</span>
<span id="cb14-2876"><a href="#cb14-2876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2877"><a href="#cb14-2877" aria-hidden="true" tabindex="-1"></a>**Faithfulness:**</span>
<span id="cb14-2878"><a href="#cb14-2878" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Do explanations truly reflect model's reasoning, or are they misleading?</span>
<span id="cb14-2879"><a href="#cb14-2879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Saliency maps can be noisy or highlight irrelevant features</span>
<span id="cb14-2880"><a href="#cb14-2880" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validation: Compare explanations against domain knowledge</span>
<span id="cb14-2881"><a href="#cb14-2881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2882"><a href="#cb14-2882" aria-hidden="true" tabindex="-1"></a>**Global vs. Local:**</span>
<span id="cb14-2883"><a href="#cb14-2883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Local explanations (single prediction) may not generalize</span>
<span id="cb14-2884"><a href="#cb14-2884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Global explanations (entire model behavior) are harder to compute and interpret</span>
<span id="cb14-2885"><a href="#cb14-2885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Need both perspectives for complete understanding</span>
<span id="cb14-2886"><a href="#cb14-2886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2887"><a href="#cb14-2887" aria-hidden="true" tabindex="-1"></a><span class="fu">### Best Practices for XAI in EO</span></span>
<span id="cb14-2888"><a href="#cb14-2888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2889"><a href="#cb14-2889" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Use Multiple Methods:** Different XAI methods can reveal complementary insights</span>
<span id="cb14-2890"><a href="#cb14-2890" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Validate Explanations:** Check against domain knowledge, physical understanding</span>
<span id="cb14-2891"><a href="#cb14-2891" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Integrate into Workflow:** Make XAI routine part of model development, not afterthought</span>
<span id="cb14-2892"><a href="#cb14-2892" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Communicate Effectively:** Visualize explanations clearly for stakeholders (heatmaps, feature importance plots)</span>
<span id="cb14-2893"><a href="#cb14-2893" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Document Limitations:** Be transparent about what explanations can and cannot tell us</span>
<span id="cb14-2894"><a href="#cb14-2894" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Balance Complexity:** For operational systems, consider interpretable models when accuracy difference is small</span>
<span id="cb14-2895"><a href="#cb14-2895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2896"><a href="#cb14-2896" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-2897"><a href="#cb14-2897" aria-hidden="true" tabindex="-1"></a><span class="fu">## XAI Resources for EO</span></span>
<span id="cb14-2898"><a href="#cb14-2898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2899"><a href="#cb14-2899" aria-hidden="true" tabindex="-1"></a>**Tools:**</span>
<span id="cb14-2900"><a href="#cb14-2900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Captum (PyTorch):** Library for model interpretability (Grad-CAM, Integrated Gradients, SHAP)</span>
<span id="cb14-2901"><a href="#cb14-2901" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SHAP Library:** SHapley Additive exPlanations for Python</span>
<span id="cb14-2902"><a href="#cb14-2902" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Grad-CAM Implementations:** Available for TensorFlow/Keras and PyTorch</span>
<span id="cb14-2903"><a href="#cb14-2903" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention Visualization:** Built into transformer implementations (HuggingFace Transformers)</span>
<span id="cb14-2904"><a href="#cb14-2904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2905"><a href="#cb14-2905" aria-hidden="true" tabindex="-1"></a>**Research:**</span>
<span id="cb14-2906"><a href="#cb14-2906" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Explainable AI for Earth Observation: A Review" (ongoing research area)</span>
<span id="cb14-2907"><a href="#cb14-2907" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SSL4EO-2024 Summer School included XAI sessions</span>
<span id="cb14-2908"><a href="#cb14-2908" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Growing number of papers combining EO and XAI at IGARSS, ISPRS, ML4Earth conferences</span>
<span id="cb14-2909"><a href="#cb14-2909" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2910"><a href="#cb14-2910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2911"><a href="#cb14-2911" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2912"><a href="#cb14-2912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2913"><a href="#cb14-2913" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb14-2914"><a href="#cb14-2914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2915"><a href="#cb14-2915" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb14-2916"><a href="#cb14-2916" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 2 Summary</span></span>
<span id="cb14-2917"><a href="#cb14-2917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2918"><a href="#cb14-2918" aria-hidden="true" tabindex="-1"></a><span class="fu">### Core Concepts</span></span>
<span id="cb14-2919"><a href="#cb14-2919" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**AI/ML learns patterns from data** rather than explicit programming - enables automated analysis of massive satellite archives</span>
<span id="cb14-2920"><a href="#cb14-2920" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**The EO workflow** spans problem definition → data acquisition → preprocessing → features → training → validation → deployment</span>
<span id="cb14-2921"><a href="#cb14-2921" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Supervised learning** (classification &amp; regression) is dominant for EO because we need specific outputs; unsupervised (clustering) useful for exploration</span>
<span id="cb14-2922"><a href="#cb14-2922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2923"><a href="#cb14-2923" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning Architectures</span></span>
<span id="cb14-2924"><a href="#cb14-2924" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**CNNs** are foundation of EO image analysis - automatic feature extraction, spatial awareness, hierarchical learning</span>
<span id="cb14-2925"><a href="#cb14-2925" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**U-Net** excels at semantic segmentation with encoder-decoder + skip connections (e.g., Benguet deforestation: 99.73% accuracy)</span>
<span id="cb14-2926"><a href="#cb14-2926" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Vision Transformers** capture global context and long-range dependencies via self-attention (SatViT, MS-CLIP for multi-spectral data)</span>
<span id="cb14-2927"><a href="#cb14-2927" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**LSTMs/RNNs** model temporal patterns in time series (PRiSM rice monitoring, crop yield prediction: R² &gt; 0.93)</span>
<span id="cb14-2928"><a href="#cb14-2928" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Object Detection** (YOLO, Faster R-CNN) localize objects with bounding boxes (buildings, ships, vehicles)</span>
<span id="cb14-2929"><a href="#cb14-2929" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>**Foundation Models** (Prithvi-EO-2.0: 600M parameters) enable fine-tuning with 10-100× less labeled data</span>
<span id="cb14-2930"><a href="#cb14-2930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2931"><a href="#cb14-2931" aria-hidden="true" tabindex="-1"></a><span class="fu">### Advanced Techniques</span></span>
<span id="cb14-2932"><a href="#cb14-2932" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>**Multi-modal fusion** combines optical + SAR for all-weather monitoring (critical for Philippine monsoon season)</span>
<span id="cb14-2933"><a href="#cb14-2933" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>**Transfer learning** dramatically reduces data requirements - pre-train on large dataset, fine-tune on small task-specific dataset</span>
<span id="cb14-2934"><a href="#cb14-2934" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>**Self-supervised learning** pre-trains on unlabeled data via masked autoencoding (Prithvi) or contrastive learning</span>
<span id="cb14-2935"><a href="#cb14-2935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2936"><a href="#cb14-2936" aria-hidden="true" tabindex="-1"></a><span class="fu">### Benchmark Datasets</span></span>
<span id="cb14-2937"><a href="#cb14-2937" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>**EuroSAT** (27,000 images, 10 classes, 98.57% accuracy), **BigEarthNet** (549,488 patches, multi-modal), **xView** (&gt;1M objects, 60 classes)</span>
<span id="cb14-2938"><a href="#cb14-2938" aria-hidden="true" tabindex="-1"></a><span class="ss">14. </span>Benchmarks enable standardized evaluation, provide training resources, support transfer learning</span>
<span id="cb14-2939"><a href="#cb14-2939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2940"><a href="#cb14-2940" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data-Centric AI (2025 Paradigm)</span></span>
<span id="cb14-2941"><a href="#cb14-2941" aria-hidden="true" tabindex="-1"></a><span class="ss">15. </span>**Data quality &gt; model complexity:** Improving data from 85% → 95% accuracy beats endless model tuning</span>
<span id="cb14-2942"><a href="#cb14-2942" aria-hidden="true" tabindex="-1"></a><span class="ss">16. </span>**Four Pillars:** Quality (accurate, consistent, properly processed), Quantity (sufficient samples, augmentation), Diversity (geographic, temporal, class, sensor), Annotation (strategic, high-quality labeling)</span>
<span id="cb14-2943"><a href="#cb14-2943" aria-hidden="true" tabindex="-1"></a><span class="ss">17. </span>**Philippine Solutions:** DOST-ASTI ALaM (Automated Labeling Machine), DIMER model repository, active learning</span>
<span id="cb14-2944"><a href="#cb14-2944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2945"><a href="#cb14-2945" aria-hidden="true" tabindex="-1"></a><span class="fu">### Explainable AI</span></span>
<span id="cb14-2946"><a href="#cb14-2946" aria-hidden="true" tabindex="-1"></a><span class="ss">18. </span>**XAI crucial for operational systems:** Builds trust, enables debugging, extracts scientific insights, detects biases</span>
<span id="cb14-2947"><a href="#cb14-2947" aria-hidden="true" tabindex="-1"></a><span class="ss">19. </span>**Methods:** Grad-CAM (heatmaps), SHAP (feature importance), Attention visualization (transformers)</span>
<span id="cb14-2948"><a href="#cb14-2948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2949"><a href="#cb14-2949" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine Operational Context</span></span>
<span id="cb14-2950"><a href="#cb14-2950" aria-hidden="true" tabindex="-1"></a><span class="ss">20. </span>**DATOS (DOST-ASTI):** 10-20 minute AI-powered flood mapping from Sentinel-1 SAR</span>
<span id="cb14-2951"><a href="#cb14-2951" aria-hidden="true" tabindex="-1"></a><span class="ss">21. </span>**PRiSM (PhilRice-IRRI):** Operational since 2014, all-weather rice monitoring combining SAR + optical</span>
<span id="cb14-2952"><a href="#cb14-2952" aria-hidden="true" tabindex="-1"></a><span class="ss">22. </span>**PhilSA-DENR:** Nationwide mangrove mapping with U-Net (99.73% accuracy)</span>
<span id="cb14-2953"><a href="#cb14-2953" aria-hidden="true" tabindex="-1"></a><span class="ss">23. </span>**CoPhil Data Centre (2025):** Local, high-bandwidth access to Sentinel data, cloud-native distribution</span>
<span id="cb14-2954"><a href="#cb14-2954" aria-hidden="true" tabindex="-1"></a><span class="ss">24. </span>**Leverage existing infrastructure:** DIMER, AIPI, ALaM, CoPhil to operationalize AI/ML workflows</span>
<span id="cb14-2955"><a href="#cb14-2955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2956"><a href="#cb14-2956" aria-hidden="true" tabindex="-1"></a>**Next steps:** Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!</span>
<span id="cb14-2957"><a href="#cb14-2957" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2958"><a href="#cb14-2958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2959"><a href="#cb14-2959" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2960"><a href="#cb14-2960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2961"><a href="#cb14-2961" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion Questions</span></span>
<span id="cb14-2962"><a href="#cb14-2962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2963"><a href="#cb14-2963" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-2964"><a href="#cb14-2964" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reflect &amp; Discuss</span></span>
<span id="cb14-2965"><a href="#cb14-2965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2966"><a href="#cb14-2966" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**What EO problem in your work** could benefit from ML? Is it classification, regression, segmentation, or object detection? Which architecture would you choose?</span>
<span id="cb14-2967"><a href="#cb14-2967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2968"><a href="#cb14-2968" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Data quality in Philippine context:** How do you address cloud cover, temporal dynamics, and atmospheric effects in your satellite data?</span>
<span id="cb14-2969"><a href="#cb14-2969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2970"><a href="#cb14-2970" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Foundation models:** How could Prithvi-EO-2.0 or other pre-trained models reduce barriers for your organization? What Philippine-specific fine-tuning would be needed?</span>
<span id="cb14-2971"><a href="#cb14-2971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2972"><a href="#cb14-2972" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Multi-modal fusion:** When would you combine Sentinel-2 optical with Sentinel-1 SAR? What are practical challenges?</span>
<span id="cb14-2973"><a href="#cb14-2973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2974"><a href="#cb14-2974" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Data-centric approach:** What are biggest data quality issues you face? How could ALaM or active learning help?</span>
<span id="cb14-2975"><a href="#cb14-2975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2976"><a href="#cb14-2976" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Benchmark datasets:** Which international datasets could you use for pre-training? How to ensure models generalize to Philippines?</span>
<span id="cb14-2977"><a href="#cb14-2977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2978"><a href="#cb14-2978" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Explainable AI:** For your application, why would explainability matter? Which XAI method would you use?</span>
<span id="cb14-2979"><a href="#cb14-2979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2980"><a href="#cb14-2980" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**DIMER and AIPI platforms:** How might these reduce barriers to deploying ML in your organization? What models would you contribute or use?</span>
<span id="cb14-2981"><a href="#cb14-2981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2982"><a href="#cb14-2982" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>**Temporal modeling:** For what applications would LSTM or temporal attention be valuable? What data would you need?</span>
<span id="cb14-2983"><a href="#cb14-2983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2984"><a href="#cb14-2984" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>**CoPhil opportunities:** How can you leverage the upcoming Data Centre and training programs? What collaborations would be valuable?</span>
<span id="cb14-2985"><a href="#cb14-2985" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-2986"><a href="#cb14-2986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2987"><a href="#cb14-2987" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-2988"><a href="#cb14-2988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2989"><a href="#cb14-2989" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb14-2990"><a href="#cb14-2990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2991"><a href="#cb14-2991" aria-hidden="true" tabindex="-1"></a><span class="fu">### Foundational Concepts</span></span>
<span id="cb14-2992"><a href="#cb14-2992" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">NASA ARSET: Fundamentals of Machine Learning for Earth Science</span><span class="co">](https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science)</span></span>
<span id="cb14-2993"><a href="#cb14-2993" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Data-Centric AI: Better, Not Just More</span><span class="co">](https://arxiv.org/abs/2312.05327)</span></span>
<span id="cb14-2994"><a href="#cb14-2994" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Van der Schaar Lab: What is Data-Centric AI?</span><span class="co">](https://www.vanderschaar-lab.com/dc-check/what-is-data-centric-ai/)</span></span>
<span id="cb14-2995"><a href="#cb14-2995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2996"><a href="#cb14-2996" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning Architectures</span></span>
<span id="cb14-2997"><a href="#cb14-2997" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Deep Learning Book (Goodfellow et al.)</span><span class="co">](https://www.deeplearningbook.org/)</span> - Free online</span>
<span id="cb14-2998"><a href="#cb14-2998" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Neural Networks and Deep Learning (Nielsen)</span><span class="co">](http://neuralnetworksanddeeplearning.com/)</span> - Interactive tutorial</span>
<span id="cb14-2999"><a href="#cb14-2999" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Satellite Image Deep Learning Techniques</span><span class="co">](https://github.com/satellite-image-deep-learning/techniques)</span> - Comprehensive GitHub repository</span>
<span id="cb14-3000"><a href="#cb14-3000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3001"><a href="#cb14-3001" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning for EO</span></span>
<span id="cb14-3002"><a href="#cb14-3002" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Deep Learning for Land Use and Land Cover Classification</span><span class="co">](https://www.mdpi.com/2072-4292/12/15/2495)</span> - 2020 review</span>
<span id="cb14-3003"><a href="#cb14-3003" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Deep Learning for Remote Sensing Image Segmentation</span><span class="co">](https://www.tandfonline.com/doi/full/10.1080/17538947.2024.2328827)</span> - 2024 review</span>
<span id="cb14-3004"><a href="#cb14-3004" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Object Detection and Image Segmentation with Deep Learning on EO Data</span><span class="co">](https://www.mdpi.com/2072-4292/12/10/1667)</span></span>
<span id="cb14-3005"><a href="#cb14-3005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3006"><a href="#cb14-3006" aria-hidden="true" tabindex="-1"></a><span class="fu">### Foundation Models</span></span>
<span id="cb14-3007"><a href="#cb14-3007" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">IBM-NASA Prithvi Models on Hugging Face</span><span class="co">](https://huggingface.co/ibm-nasa-geospatial)</span></span>
<span id="cb14-3008"><a href="#cb14-3008" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model</span><span class="co">](https://arxiv.org/abs/2412.02732)</span></span>
<span id="cb14-3009"><a href="#cb14-3009" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">IBM Research: Prithvi-EO-2.0 Blog</span><span class="co">](https://research.ibm.com/blog/prithvi2-geospatial)</span></span>
<span id="cb14-3010"><a href="#cb14-3010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3011"><a href="#cb14-3011" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Supervised Learning</span></span>
<span id="cb14-3012"><a href="#cb14-3012" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SSL4EO-2024 Summer School Review</span><span class="co">](https://langnico.github.io/posts/SSL4EO-2024-review/)</span></span>
<span id="cb14-3013"><a href="#cb14-3013" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Multi-Label Guided Soft Contrastive Learning for EO</span><span class="co">](https://arxiv.org/abs/2405.20462)</span></span>
<span id="cb14-3014"><a href="#cb14-3014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3015"><a href="#cb14-3015" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data-Centric AI</span></span>
<span id="cb14-3016"><a href="#cb14-3016" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Data-Centric Machine Learning for Earth Observation</span><span class="co">](https://arxiv.org/html/2408.11384v1)</span></span>
<span id="cb14-3017"><a href="#cb14-3017" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Kili Technology: Earth Observation Data Labeling Guide</span><span class="co">](https://kili-technology.com/data-labeling/earth-observation-data-labeling-guide)</span></span>
<span id="cb14-3018"><a href="#cb14-3018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3019"><a href="#cb14-3019" aria-hidden="true" tabindex="-1"></a><span class="fu">### Explainable AI</span></span>
<span id="cb14-3020"><a href="#cb14-3020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Captum: Model Interpretability for PyTorch</span><span class="co">](https://captum.ai/)</span></span>
<span id="cb14-3021"><a href="#cb14-3021" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SHAP Library Documentation</span><span class="co">](https://shap.readthedocs.io/)</span></span>
<span id="cb14-3022"><a href="#cb14-3022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3023"><a href="#cb14-3023" aria-hidden="true" tabindex="-1"></a><span class="fu">### EO-Specific ML</span></span>
<span id="cb14-3024"><a href="#cb14-3024" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">EO College: Introduction to Machine Learning for Earth Observation</span><span class="co">](https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/)</span></span>
<span id="cb14-3025"><a href="#cb14-3025" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">ML4Earth Resources</span><span class="co">](https://ml4earth.de/)</span></span>
<span id="cb14-3026"><a href="#cb14-3026" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Climate Change AI: Earth Observation &amp; Monitoring</span><span class="co">](https://www.climatechange.ai/subject_areas/earth_observation_monitoring)</span></span>
<span id="cb14-3027"><a href="#cb14-3027" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">A Review of Practical AI for Remote Sensing in Earth Sciences</span><span class="co">](https://www.mdpi.com/2072-4292/15/16/4112)</span> - 2023</span>
<span id="cb14-3028"><a href="#cb14-3028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3029"><a href="#cb14-3029" aria-hidden="true" tabindex="-1"></a><span class="fu">### Benchmark Datasets</span></span>
<span id="cb14-3030"><a href="#cb14-3030" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">EuroSAT GitHub</span><span class="co">](https://github.com/phelber/EuroSAT)</span></span>
<span id="cb14-3031"><a href="#cb14-3031" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">BigEarthNet Website</span><span class="co">](https://bigearth.net/)</span></span>
<span id="cb14-3032"><a href="#cb14-3032" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">xView Dataset</span><span class="co">](http://xviewdataset.org/)</span></span>
<span id="cb14-3033"><a href="#cb14-3033" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DOTA: Dataset for Object Detection in Aerial Images</span><span class="co">](https://captain-whu.github.io/DOTA/)</span></span>
<span id="cb14-3034"><a href="#cb14-3034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3035"><a href="#cb14-3035" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine AI Initiatives</span></span>
<span id="cb14-3036"><a href="#cb14-3036" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DOST-ASTI: Remote Sensing and Data Science (DATOS) Help Desk</span><span class="co">](https://asti.dost.gov.ph/projects/datos)</span></span>
<span id="cb14-3037"><a href="#cb14-3037" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Philippine News Agency: DOST AI R&amp;D Projects</span><span class="co">](https://www.pna.gov.ph/articles/1136226)</span> - SkAI-Pinas, DIMER, AIPI</span>
<span id="cb14-3038"><a href="#cb14-3038" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PRiSM: Philippine Rice Information System</span><span class="co">](https://prism.philrice.gov.ph/)</span></span>
<span id="cb14-3039"><a href="#cb14-3039" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PhilSA: Philippine Space Agency</span><span class="co">](https://philsa.gov.ph/)</span></span>
<span id="cb14-3040"><a href="#cb14-3040" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">CoPhil Centre</span><span class="co">](https://copphil.philsa.gov.ph/)</span></span>
<span id="cb14-3041"><a href="#cb14-3041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3042"><a href="#cb14-3042" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recent Advances</span></span>
<span id="cb14-3043"><a href="#cb14-3043" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Artificial Intelligence to Advance Earth Observation: A Review</span><span class="co">](https://arxiv.org/abs/2305.08413)</span> - 2023</span>
<span id="cb14-3044"><a href="#cb14-3044" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Advancing Earth Observation with AI</span><span class="co">](https://arxiv.org/html/2501.12030v1)</span> - 2025</span>
<span id="cb14-3045"><a href="#cb14-3045" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">ESA AI for Earth Observation</span><span class="co">](https://ai4eo.eu/)</span></span>
<span id="cb14-3046"><a href="#cb14-3046" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Awesome Earth Observation Code</span><span class="co">](https://github.com/acgeospatial/awesome-earthobservation-code)</span></span>
<span id="cb14-3047"><a href="#cb14-3047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3048"><a href="#cb14-3048" aria-hidden="true" tabindex="-1"></a>---</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:info@philsa.gov.ph">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>