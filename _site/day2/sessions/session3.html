<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="CoPhil Advanced Training Program">
<meta name="dcterms.date" content="2025-10-16">

<title>Session 3: Introduction to Deep Learning and CNNs – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day2/sessions/session4.html" rel="next">
<link href="../../day2/sessions/session2.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-4a074efccdeff27617fbc72d37c1244e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a15f5dce5650fb3fe5aba34e3b6df9a9.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cophil-training-v1.0"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 2: Machine Learning for Earth Observation</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Supervised Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 3: Introduction to Deep Learning and CNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-on Lab</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_theory_notebook_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Theory: Understanding Random Forest for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_hands_on_lab_student.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Hands-on Lab: Palawan Land Cover Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session2_extended_lab_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Classification Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session3_theory_interactive.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Deep Learning &amp; CNN Theory - Interactive Notebook</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_cnn_classification_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-On Lab - EuroSAT Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_transfer_learning_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4 Part B: Transfer Learning with ResNet50</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-3-introduction-to-deep-learning-and-cnns" id="toc-session-3-introduction-to-deep-learning-and-cnns" class="nav-link active" data-scroll-target="#session-3-introduction-to-deep-learning-and-cnns">Session 3: Introduction to Deep Learning and CNNs</a>
  <ul class="collapse">
  <li><a href="#neural-networks-and-convolutional-architectures-for-earth-observation" id="toc-neural-networks-and-convolutional-architectures-for-earth-observation" class="nav-link" data-scroll-target="#neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</a></li>
  </ul></li>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link" data-scroll-target="#session-overview">Session Overview</a></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link" data-scroll-target="#what-youll-learn">What You’ll Learn</a></li>
  <li><a href="#session-structure" id="toc-session-structure" class="nav-link" data-scroll-target="#session-structure">Session Structure</a>
  <ul class="collapse">
  <li><a href="#part-a-from-machine-learning-to-deep-learning-15-minutes" id="toc-part-a-from-machine-learning-to-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</a></li>
  <li><a href="#part-b-neural-network-fundamentals-25-minutes" id="toc-part-b-neural-network-fundamentals-25-minutes" class="nav-link" data-scroll-target="#part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</a></li>
  <li><a href="#part-c-convolutional-neural-networks-30-minutes" id="toc-part-c-convolutional-neural-networks-30-minutes" class="nav-link" data-scroll-target="#part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</a></li>
  <li><a href="#part-d-cnns-for-earth-observation-25-minutes" id="toc-part-d-cnns-for-earth-observation-25-minutes" class="nav-link" data-scroll-target="#part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</a></li>
  <li><a href="#part-e-practical-considerations-for-eo-deep-learning-15-minutes" id="toc-part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a>
  <ul class="collapse">
  <li><a href="#automatic-feature-learning" id="toc-automatic-feature-learning" class="nav-link" data-scroll-target="#automatic-feature-learning">Automatic Feature Learning</a></li>
  <li><a href="#receptive-field" id="toc-receptive-field" class="nav-link" data-scroll-target="#receptive-field">Receptive Field</a></li>
  <li><a href="#translation-invariance" id="toc-translation-invariance" class="nav-link" data-scroll-target="#translation-invariance">Translation Invariance</a></li>
  <li><a href="#gradient-descent-and-backpropagation" id="toc-gradient-descent-and-backpropagation" class="nav-link" data-scroll-target="#gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</a></li>
  </ul></li>
  <li><a href="#interactive-demonstrations" id="toc-interactive-demonstrations" class="nav-link" data-scroll-target="#interactive-demonstrations">Interactive Demonstrations</a>
  <ul class="collapse">
  <li><a href="#demo-1-perceptron-playground" id="toc-demo-1-perceptron-playground" class="nav-link" data-scroll-target="#demo-1-perceptron-playground">Demo 1: Perceptron Playground</a></li>
  <li><a href="#demo-2-activation-function-gallery" id="toc-demo-2-activation-function-gallery" class="nav-link" data-scroll-target="#demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</a></li>
  <li><a href="#demo-3-manual-convolution-on-sentinel-2" id="toc-demo-3-manual-convolution-on-sentinel-2" class="nav-link" data-scroll-target="#demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</a></li>
  <li><a href="#demo-4-pooling-demonstration" id="toc-demo-4-pooling-demonstration" class="nav-link" data-scroll-target="#demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</a></li>
  <li><a href="#demo-5-architecture-exploration" id="toc-demo-5-architecture-exploration" class="nav-link" data-scroll-target="#demo-5-architecture-exploration">Demo 5: Architecture Exploration</a></li>
  </ul></li>
  <li><a href="#philippine-eo-applications" id="toc-philippine-eo-applications" class="nav-link" data-scroll-target="#philippine-eo-applications">Philippine EO Applications</a>
  <ul class="collapse">
  <li><a href="#philsa-space-dashboard" id="toc-philsa-space-dashboard" class="nav-link" data-scroll-target="#philsa-space-dashboard">PhilSA Space+ Dashboard</a></li>
  <li><a href="#denr-forest-monitoring" id="toc-denr-forest-monitoring" class="nav-link" data-scroll-target="#denr-forest-monitoring">DENR Forest Monitoring</a></li>
  <li><a href="#lgu-applications-session-4-focus" id="toc-lgu-applications-session-4-focus" class="nav-link" data-scroll-target="#lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</a></li>
  </ul></li>
  <li><a href="#expected-outcomes" id="toc-expected-outcomes" class="nav-link" data-scroll-target="#expected-outcomes">Expected Outcomes</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding">Conceptual Understanding</a></li>
  <li><a href="#technical-skills" id="toc-technical-skills" class="nav-link" data-scroll-target="#technical-skills">Technical Skills</a></li>
  <li><a href="#practical-readiness-for-session-4" id="toc-practical-readiness-for-session-4" class="nav-link" data-scroll-target="#practical-readiness-for-session-4">Practical Readiness for Session 4</a></li>
  </ul></li>
  <li><a href="#hands-on-notebooks" id="toc-hands-on-notebooks" class="nav-link" data-scroll-target="#hands-on-notebooks">Hands-On Notebooks</a>
  <ul class="collapse">
  <li><a href="#access-the-interactive-materials" id="toc-access-the-interactive-materials" class="nav-link" data-scroll-target="#access-the-interactive-materials">Access the Interactive Materials</a></li>
  <li><a href="#supporting-documentation" id="toc-supporting-documentation" class="nav-link" data-scroll-target="#supporting-documentation">Supporting Documentation</a></li>
  </ul></li>
  <li><a href="#troubleshooting" id="toc-troubleshooting" class="nav-link" data-scroll-target="#troubleshooting">Troubleshooting</a>
  <ul class="collapse">
  <li><a href="#common-conceptual-questions" id="toc-common-conceptual-questions" class="nav-link" data-scroll-target="#common-conceptual-questions">Common Conceptual Questions</a></li>
  <li><a href="#technical-issues" id="toc-technical-issues" class="nav-link" data-scroll-target="#technical-issues">Technical Issues</a></li>
  <li><a href="#getting-help" id="toc-getting-help" class="nav-link" data-scroll-target="#getting-help">Getting Help</a></li>
  </ul></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a>
  <ul class="collapse">
  <li><a href="#foundational-learning" id="toc-foundational-learning" class="nav-link" data-scroll-target="#foundational-learning">Foundational Learning</a></li>
  <li><a href="#earth-observation-deep-learning" id="toc-earth-observation-deep-learning" class="nav-link" data-scroll-target="#earth-observation-deep-learning">Earth Observation Deep Learning</a></li>
  <li><a href="#philippine-context" id="toc-philippine-context" class="nav-link" data-scroll-target="#philippine-context">Philippine Context</a></li>
  </ul></li>
  <li><a href="#assessment" id="toc-assessment" class="nav-link" data-scroll-target="#assessment">Assessment</a>
  <ul class="collapse">
  <li><a href="#formative-assessment-during-session" id="toc-formative-assessment-during-session" class="nav-link" data-scroll-target="#formative-assessment-during-session">Formative Assessment (During Session)</a></li>
  <li><a href="#summative-assessment-end-of-session" id="toc-summative-assessment-end-of-session" class="nav-link" data-scroll-target="#summative-assessment-end-of-session">Summative Assessment (End of Session)</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a>
  <ul class="collapse">
  <li><a href="#recommended-pre-work-for-session-4" id="toc-recommended-pre-work-for-session-4" class="nav-link" data-scroll-target="#recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</a></li>
  <li><a href="#extended-learning-paths" id="toc-extended-learning-paths" class="nav-link" data-scroll-target="#extended-learning-paths">Extended Learning Paths</a></li>
  </ul></li>
  <li><a href="#quick-links" id="toc-quick-links" class="nav-link" data-scroll-target="#quick-links">Quick Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Session 3: Introduction to Deep Learning and CNNs</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Neural Networks and Convolutional Architectures for Earth Observation</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Instructor</div>
    <div class="quarto-title-meta-contents">
             <p>CoPhil Advanced Training Program </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 2</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 3</span>
</nav>
<section id="session-3-introduction-to-deep-learning-and-cnns" class="level1 hero">
<h1>Session 3: Introduction to Deep Learning and CNNs</h1>
<section id="neural-networks-and-convolutional-architectures-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</h3>
<p>Transitioning from feature engineering to feature learning</p>
</section>
</section>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p><strong>Duration:</strong> 2.5 hours | <strong>Type:</strong> Theory + Interactive Demonstrations | <strong>Difficulty:</strong> Intermediate</p>
<hr>
<p>This pivotal session bridges traditional machine learning (Sessions 1-2) and modern deep learning approaches. You’ll understand the fundamental shift from manual feature engineering to automatic feature learning through neural networks, with specific focus on Convolutional Neural Networks (CNNs) for Earth observation applications.</p>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/session3_deep_learning.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Prerequisites
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>✓ Complete Sessions 1-2 (Random Forest classification)</li>
<li>✓ Understanding of classification concepts (accuracy, confusion matrix)</li>
<li>✓ Basic Python and NumPy familiarity</li>
<li>✓ Colab environment with GPU runtime enabled</li>
<li>✓ Conceptual understanding of matrix operations</li>
</ul>
</div>
</div>
<hr>
</section>
<section id="what-youll-learn" class="level2">
<h2 class="anchored" data-anchor-id="what-youll-learn">What You’ll Learn</h2>
<p>After completing this session, you will be able to:</p>
<ol type="1">
<li><strong>Understand the ML → DL Transition</strong>
<ul>
<li>Recognize when to use traditional ML vs deep learning</li>
<li>Explain the automatic feature learning paradigm</li>
<li>Identify computational requirements and trade-offs</li>
<li>Understand data requirements for deep learning success</li>
</ul></li>
<li><strong>Master Neural Network Fundamentals</strong>
<ul>
<li>Build simple perceptrons from scratch using NumPy</li>
<li>Implement activation functions (ReLU, sigmoid, softmax)</li>
<li>Understand forward propagation and backpropagation</li>
<li>Visualize decision boundaries and learning dynamics</li>
</ul></li>
<li><strong>Comprehend Convolutional Neural Networks</strong>
<ul>
<li>Explain convolution operations and their purpose</li>
<li>Understand pooling, padding, and stride concepts</li>
<li>Visualize filter responses on satellite imagery</li>
<li>Compare CNN architectures (LeNet, VGG, ResNet, U-Net)</li>
</ul></li>
<li><strong>Apply CNNs to Earth Observation Tasks</strong>
<ul>
<li>Identify appropriate CNN architectures for EO problems</li>
<li>Understand scene classification vs semantic segmentation</li>
<li>Recognize object detection and change detection approaches</li>
<li>Connect CNN capabilities to Philippine EO applications</li>
</ul></li>
<li><strong>Navigate Practical Considerations</strong>
<ul>
<li>Address data-centric AI principles for EO</li>
<li>Handle limited training data scenarios</li>
<li>Understand transfer learning and pre-trained models</li>
<li>Recognize computational constraints and optimization strategies</li>
</ul></li>
</ol>
<hr>
</section>
<section id="session-structure" class="level2">
<h2 class="anchored" data-anchor-id="session-structure">Session Structure</h2>
<section id="part-a-from-machine-learning-to-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</h3>
<p>Understanding the paradigm shift from manual to automatic feature learning.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>🔧 Traditional ML (Sessions 1-2)</strong></p>
<p><strong>What you did:</strong> - Manually calculated NDVI, NDWI, NDBI - Engineered GLCM texture features - Extracted temporal statistics - Combined features thoughtfully</p>
<p><strong>Pros:</strong> Interpretable, works with small datasets <strong>Cons:</strong> Requires domain expertise, limited by imagination</p>
</div>
<div class="feature-card">
<p><strong>🧠 Deep Learning (Sessions 3-4)</strong></p>
<p><strong>What CNNs do:</strong> - Learn features automatically from raw pixels - Discover hidden patterns humans miss - Build hierarchical representations - Optimize end-to-end</p>
<p><strong>Pros:</strong> No feature engineering, state-of-the-art accuracy <strong>Cons:</strong> Needs large datasets, computationally intensive</p>
</div>
<div class="feature-card">
<p><strong>⚖️ When to Use Which?</strong></p>
<p><strong>Use Random Forest when:</strong> - Limited training data (&lt;1000 samples) - Need interpretability (DENR reports) - Have domain features (indices) - Fast prototyping needed</p>
<p><strong>Use CNNs when:</strong> - Large labeled datasets (&gt;10,000 samples) - Complex spatial patterns - Maximum accuracy required - GPU resources available</p>
</div>
<div class="feature-card">
<p><strong>🇵🇭 Philippine EO Context</strong></p>
<p><strong>PhilSA Applications:</strong> - Scene classification (land cover) - Cloud detection (Sentinel-2) - Building footprint extraction - Flood extent mapping</p>
<p><strong>Why CNNs?</strong> Handle complex tropical landscapes, monsoon cloud patterns, informal settlements</p>
</div>
</div>
<p><strong>Key Insight:</strong> <em>“In Sessions 1-2, you manually engineered features. CNNs will learn these features automatically—and discover new ones you never imagined!”</em></p>
</section>
<section id="part-b-neural-network-fundamentals-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</h3>
<p>Building intuition from the ground up using interactive Jupyter notebooks.</p>
<section id="b.1-the-perceptron---simplest-neural-unit" class="level4">
<h4 class="anchored" data-anchor-id="b.1-the-perceptron---simplest-neural-unit">B.1: The Perceptron - Simplest Neural Unit</h4>
<p>Understanding the building block of all neural networks.</p>
<p><strong>Mathematical Foundation:</strong> <span class="math display">\[
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
\]</span></p>
<p>Where: - <span class="math inline">\(x_i\)</span>: Input features (e.g., pixel values, NDVI) - <span class="math inline">\(w_i\)</span>: Learned weights - <span class="math inline">\(b\)</span>: Bias term - <span class="math inline">\(f\)</span>: Activation function - <span class="math inline">\(y\)</span>: Output prediction</p>
<p><strong>Hands-On:</strong> Build a perceptron from scratch to classify “Water vs Non-Water” using NDWI.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple perceptron implementation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        linear_output <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(linear_output)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, z):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>  <span class="co"># Step function</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="b.2-activation-functions---adding-non-linearity" class="level4">
<h4 class="anchored" data-anchor-id="b.2-activation-functions---adding-non-linearity">B.2: Activation Functions - Adding Non-Linearity</h4>
<p>Why neural networks need activation functions to solve complex problems.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>Sigmoid</strong> <span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (0, 1) <strong>Use:</strong> Binary classification output <strong>EO Example:</strong> Cloud probability</p>
</div>
<div class="feature-card">
<p><strong>ReLU (Rectified Linear Unit)</strong> <span class="math display">\[\text{ReLU}(z) = \max(0, z)\]</span></p>
<p><strong>Range:</strong> [0, ∞) <strong>Use:</strong> Hidden layers (most popular) <strong>Why:</strong> Fast, sparse activation</p>
</div>
<div class="feature-card">
<p><strong>Softmax</strong> <span class="math display">\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</span></p>
<p><strong>Range:</strong> (0, 1), sums to 1 <strong>Use:</strong> Multi-class classification <strong>EO Example:</strong> Land cover classes</p>
</div>
<div class="feature-card">
<p><strong>Tanh</strong> <span class="math display">\[\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (-1, 1) <strong>Use:</strong> When centered data needed <strong>Note:</strong> Less common than ReLU</p>
</div>
</div>
<p><strong>Interactive Demo:</strong> Visualize how each activation function transforms Sentinel-2 spectral values.</p>
</section>
<section id="b.3-multi-layer-networks---learning-complex-patterns" class="level4">
<h4 class="anchored" data-anchor-id="b.3-multi-layer-networks---learning-complex-patterns">B.3: Multi-Layer Networks - Learning Complex Patterns</h4>
<p>Stacking layers to learn hierarchical representations.</p>
<p><strong>Architecture:</strong></p>
<pre><code>Input Layer (e.g., Sentinel-2 bands)
    ↓
Hidden Layer 1 (64 neurons, ReLU)
    ↓
Hidden Layer 2 (32 neurons, ReLU)
    ↓
Output Layer (8 classes, Softmax)</code></pre>
<p><strong>What Each Layer Learns:</strong> - <strong>Layer 1:</strong> Low-level features (edges, textures) - <strong>Layer 2:</strong> Mid-level features (shapes, patterns) - <strong>Layer 3:</strong> High-level features (objects, scenes) - <strong>Output:</strong> Class probabilities</p>
<p><strong>Hands-On:</strong> Train a 2-layer network to classify Palawan land cover using spectral features (from Session 2).</p>
</section>
<section id="b.4-training-process---learning-from-data" class="level4">
<h4 class="anchored" data-anchor-id="b.4-training-process---learning-from-data">B.4: Training Process - Learning from Data</h4>
<p>Understanding gradient descent and backpropagation intuitively.</p>
<p><strong>Training Loop:</strong> 1. <strong>Forward Pass:</strong> Input → Hidden → Output → Prediction 2. <strong>Calculate Loss:</strong> Compare prediction to true label 3. <strong>Backward Pass:</strong> Compute gradients (how much each weight contributed to error) 4. <strong>Update Weights:</strong> <span class="math inline">\(w_{new} = w_{old} - \alpha \cdot \frac{\partial L}{\partial w}\)</span> 5. <strong>Repeat:</strong> Until loss converges</p>
<p><strong>Key Hyperparameters:</strong> - <strong>Learning Rate (<span class="math inline">\(\alpha\)</span>):</strong> Step size for weight updates (0.001 - 0.1) - <strong>Batch Size:</strong> Number of samples per gradient update (32, 64, 128) - <strong>Epochs:</strong> Complete passes through training data (10-100)</p>
<p><strong>Interactive Exploration:</strong> Experiment with learning rates to see overfitting vs underfitting.</p>
</section>
</section>
<section id="part-c-convolutional-neural-networks-30-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</h3>
<p>Deep dive into the architecture that revolutionized computer vision and Earth observation.</p>
<section id="c.1-why-cnns-for-images" class="level4">
<h4 class="anchored" data-anchor-id="c.1-why-cnns-for-images">C.1: Why CNNs for Images?</h4>
<p><strong>Problem with Regular Neural Networks:</strong> - A 10m Sentinel-2 image chip (256×256×10 bands) = 655,360 parameters just for first layer! - No spatial awareness (treats nearby pixels same as distant ones) - Computationally infeasible</p>
<p><strong>CNN Solutions:</strong> - <strong>Local Connectivity:</strong> Each neuron connects to small spatial region - <strong>Parameter Sharing:</strong> Same filter applied across entire image - <strong>Translation Invariance:</strong> Detect features anywhere in image</p>
<p><strong>Result:</strong> Millions fewer parameters, spatially-aware learning!</p>
</section>
<section id="c.2-convolution-operation---the-heart-of-cnns" class="level4">
<h4 class="anchored" data-anchor-id="c.2-convolution-operation---the-heart-of-cnns">C.2: Convolution Operation - The Heart of CNNs</h4>
<p>Understanding how convolutions extract features from satellite imagery.</p>
<p><strong>Mathematical Definition:</strong> <span class="math display">\[
(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m,n)
\]</span></p>
<p>Where: - <span class="math inline">\(I\)</span>: Input image (e.g., Sentinel-2 NIR band) - <span class="math inline">\(K\)</span>: Filter/kernel (e.g., 3×3 edge detector) - <span class="math inline">\(*\)</span>: Convolution operator</p>
<p><strong>Visual Example:</strong></p>
<pre><code>Sentinel-2 NIR Image (5×5)    Edge Detection Filter (3×3)
┌─────────────────┐           ┌─────────┐
│ 120 115 118 122 │           │ -1  -1  -1 │
│ 118 245 242 125 │     *     │  0   0   0 │
│ 119 248 244 121 │           │  1   1   1 │
│ 121 116 119 123 │           └─────────┘
└─────────────────┘
                ↓
    Feature Map (detects water edges)</code></pre>
<p><strong>Hands-On:</strong> - Apply manual convolutions to Sentinel-2 patches - Visualize classic filters (edge detection, blur, sharpen) - See how filters respond to forests, water, urban areas</p>
</section>
<section id="c.3-cnn-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="c.3-cnn-building-blocks">C.3: CNN Building Blocks</h4>
<p><strong>1. Convolutional Layer</strong> - Applies multiple filters to input - Each filter learns to detect different feature - Output: Feature maps (activations)</p>
<p><strong>Parameters:</strong> - <code>filters</code>: Number of filters (32, 64, 128…) - <code>kernel_size</code>: Filter dimensions (3×3, 5×5) - <code>stride</code>: Step size for filter movement (usually 1) - <code>padding</code>: Border handling (‘same’ or ‘valid’)</p>
<p><strong>2. Pooling Layer</strong> - Reduces spatial dimensions (downsampling) - Provides translation invariance - Most common: Max pooling</p>
<p><strong>Max Pooling (2×2):</strong></p>
<pre><code>Input (4×4)                Output (2×2)
┌────────────┐            ┌──────┐
│ 1  3  2  4 │            │ 3  4 │
│ 2  3  1  2 │    →       │ 7  9 │
│ 5  7  8  9 │            └──────┘
│ 1  2  3  4 │
└────────────┘
Takes maximum in each 2×2 window</code></pre>
<p><strong>3. Fully Connected Layer</strong> - Traditional neural network layer - Connects all features to output classes - Usually at end of network</p>
<p><strong>4. Dropout Layer</strong> - Randomly deactivates neurons during training - Prevents overfitting - Common rate: 0.3-0.5</p>
</section>
<section id="c.4-classic-cnn-architectures" class="level4">
<h4 class="anchored" data-anchor-id="c.4-classic-cnn-architectures">C.4: Classic CNN Architectures</h4>
<p>Understanding architectures used in EO applications.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>LeNet-5 (1998)</strong></p>
<p><strong>Structure:</strong> - Conv → Pool → Conv → Pool → FC - 60K parameters - Original: Handwritten digits</p>
<p><strong>EO Use:</strong> - Simple scene classification - Educational examples - Quick prototypes</p>
</div>
<div class="feature-card">
<p><strong>VGG-16 (2014)</strong></p>
<p><strong>Structure:</strong> - 13 Conv layers + 3 FC - 138M parameters - Small 3×3 filters stacked</p>
<p><strong>EO Use:</strong> - Scene classification - Pre-trained on ImageNet - Transfer learning baseline</p>
</div>
<div class="feature-card">
<p><strong>ResNet-50 (2015)</strong></p>
<p><strong>Innovation:</strong> - Skip connections (residual blocks) - Solves vanishing gradient - 50 layers deep</p>
<p><strong>EO Use:</strong> - High-accuracy classification - Feature extraction - PhilSA scene classifier</p>
</div>
<div class="feature-card">
<p><strong>U-Net (2015)</strong></p>
<p><strong>Innovation:</strong> - Encoder-decoder architecture - Skip connections preserve detail - Outputs same-size segmentation</p>
<p><strong>EO Use:</strong> - Semantic segmentation - Flood mapping - Building extraction - <strong>Session 4 focus!</strong></p>
</div>
</div>
<p><strong>Interactive Visualization:</strong> Explore how different architectures process Sentinel-2 imagery.</p>
</section>
</section>
<section id="part-d-cnns-for-earth-observation-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</h3>
<p>Connecting CNN capabilities to Philippine EO operational needs.</p>
<section id="d.1-scene-classification" class="level4">
<h4 class="anchored" data-anchor-id="d.1-scene-classification">D.1: Scene Classification</h4>
<p><strong>Task:</strong> Assign single label to entire image patch</p>
<p><strong>Architecture:</strong> ResNet, VGG, EfficientNet (classification head)</p>
<p><strong>Philippine Applications:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Application</th>
<th>Classes</th>
<th>Dataset</th>
<th>Stakeholder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Land Cover</strong></td>
<td>Forest, Urban, Agriculture, Water, Bare</td>
<td>PhilSA Sentinel-2</td>
<td>DENR, LGUs</td>
</tr>
<tr class="even">
<td><strong>Cloud Detection</strong></td>
<td>Clear, Thin Cloud, Thick Cloud, Shadow</td>
<td>Sentinel-2 Level-1C</td>
<td>PhilSA (preprocessing)</td>
</tr>
<tr class="odd">
<td><strong>Rice Field Stage</strong></td>
<td>Land Prep, Transplanting, Vegetative, Harvest</td>
<td>PlanetScope + field data</td>
<td>DA, PhilRice</td>
</tr>
<tr class="even">
<td><strong>Disaster Assessment</strong></td>
<td>Damaged, Undamaged, Debris</td>
<td>Drones + Sentinel-2</td>
<td>NDRRMC, PAGASA</td>
</tr>
</tbody>
</table>
<p><strong>Data Requirements:</strong> - Typical: 1,000-10,000 labeled image chips per class - PhilSA strategy: Start with 500/class, augment with rotations/flips</p>
<p><strong>Hands-On (Session 4):</strong> Build ResNet-based classifier for Palawan land cover</p>
</section>
<section id="d.2-semantic-segmentation" class="level4">
<h4 class="anchored" data-anchor-id="d.2-semantic-segmentation">D.2: Semantic Segmentation</h4>
<p><strong>Task:</strong> Classify every pixel in image (pixel-wise labels)</p>
<p><strong>Architecture:</strong> U-Net, DeepLabv3+, SegNet</p>
<p><strong>Philippine Applications:</strong></p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>🌊 Flood Mapping</strong></p>
<p><strong>Challenge:</strong> Rapid post-disaster assessment</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-1 SAR (pre + post event) - Output: Flooded/Non-flooded pixel map - Architecture: U-Net</p>
<p><strong>PhilSA Use Case:</strong> Pampanga flood 2023 - 6-hour processing time (vs 2 days manual)</p>
</div>
<div class="feature-card">
<p><strong>🏘️ Informal Settlements</strong></p>
<p><strong>Challenge:</strong> Map slums for disaster planning</p>
<p><strong>CNN Solution:</strong> - Input: High-res imagery (PlanetScope, drones) - Output: Building footprints - Architecture: U-Net + post-processing</p>
<p><strong>NEDA Application:</strong> Metro Manila vulnerability assessment</p>
</div>
<div class="feature-card">
<p><strong>🌳 Forest Degradation</strong></p>
<p><strong>Challenge:</strong> Detect selective logging</p>
<p><strong>CNN Solution:</strong> - Input: Multi-temporal Sentinel-2 - Output: Degraded forest pixels - Architecture: U-Net + LSTM</p>
<p><strong>DENR Use:</strong> Protected area monitoring</p>
</div>
<div class="feature-card">
<p><strong>⛏️ Mining Activity</strong></p>
<p><strong>Challenge:</strong> Illegal mining detection</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-2 + Sentinel-1 - Output: Mining site polygons - Post-process: Vectorize</p>
<p><strong>MGB Application:</strong> Permit compliance checking</p>
</div>
</div>
<p><strong>Data Requirements:</strong> - Annotation intensive: Need pixel-level labels - Typical: 100-500 labeled images (256×256 chips) - Tools: QGIS, LabelMe, CVAT</p>
<p><strong>Hands-On (Session 4):</strong> Implement U-Net for flood mapping in Central Luzon</p>
</section>
<section id="d.3-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.3-object-detection">D.3: Object Detection</h4>
<p><strong>Task:</strong> Find and localize objects with bounding boxes</p>
<p><strong>Architecture:</strong> Faster R-CNN, YOLO, RetinaNet</p>
<p><strong>Philippine Applications:</strong> - <strong>Ship Detection:</strong> Illegal fishing monitoring (Sentinel-1) - <strong>Building Detection:</strong> Infrastructure mapping (high-res) - <strong>Tree Counting:</strong> Forest inventory (drone imagery) - <strong>Vehicle Detection:</strong> Traffic monitoring (PlanetScope)</p>
<p><strong>Data Format:</strong> Bounding boxes + class labels (COCO, PASCAL VOC formats)</p>
<p><strong>Computational Note:</strong> More complex than classification, requires anchor boxes and region proposals</p>
</section>
<section id="d.4-change-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.4-change-detection">D.4: Change Detection</h4>
<p><strong>Task:</strong> Identify what changed between two time points</p>
<p><strong>CNN Approaches:</strong></p>
<ol type="1">
<li><strong>Siamese Networks:</strong> Compare two images with shared weights</li>
<li><strong>Early Fusion:</strong> Stack temporal images as input channels</li>
<li><strong>Late Fusion:</strong> Separate encoders + change decoder</li>
</ol>
<p><strong>Philippine Applications:</strong> - Deforestation (Palawan, Mindanao) - Urban expansion (Metro Manila, Cebu) - Post-disaster damage (typhoon impacts) - Agricultural change (conversion detection)</p>
<p><strong>Challenge:</strong> Need paired labeled change data (before + after + change mask)</p>
</section>
</section>
<section id="part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</h3>
<p>Real-world challenges and solutions for Philippine EO practitioners.</p>
<section id="e.1-the-data-challenge" class="level4">
<h4 class="anchored" data-anchor-id="e.1-the-data-challenge">E.1: The Data Challenge</h4>
<p><strong>How Much Data Do You Need?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 36%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Model Complexity</th>
<th>Typical Requirement</th>
<th>Philippine Reality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN (5 layers)</td>
<td>5,000-10,000 samples</td>
<td>✓ Achievable</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>100,000+ samples</td>
<td>✗ Rarely available</td>
</tr>
<tr class="odd">
<td>ResNet-50 (fine-tuned)</td>
<td>1,000-5,000 samples</td>
<td>✓ Achievable with augmentation</td>
</tr>
<tr class="even">
<td>U-Net (segmentation)</td>
<td>100-500 images</td>
<td>✓ Achievable but labor-intensive</td>
</tr>
</tbody>
</table>
<p><strong>Data-Centric AI Principles:</strong> 1. <strong>Quality &gt; Quantity:</strong> 500 clean labels &gt;&gt; 5,000 noisy labels 2. <strong>Representative Sampling:</strong> Cover all Philippine ecosystems (lowland, upland, coastal) 3. <strong>Class Balance:</strong> Equal samples per class (or weighted loss) 4. <strong>Validation Split:</strong> Hold out 20% for unbiased evaluation</p>
<p><strong>Philippine Data Sources:</strong> - PhilSA Space+ Data Dashboard (satellite imagery) - NAMRIA Geoportal (reference maps) - DOST-ASTI DATOS (disaster imagery) - LiDAR Portal (elevation + canopy) - Field campaigns (GPS + photos)</p>
</section>
<section id="e.2-transfer-learning---training-on-limited-data" class="level4">
<h4 class="anchored" data-anchor-id="e.2-transfer-learning---training-on-limited-data">E.2: Transfer Learning - Training on Limited Data</h4>
<p><strong>Strategy:</strong> Start with model pre-trained on large dataset (ImageNet), fine-tune on Philippine data</p>
<p><strong>Benefits:</strong> - Need 10× less data - Train 5× faster - Better accuracy with small datasets</p>
<p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained ResNet</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> ResNet50(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze early layers (keep learned features)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[:<span class="dv">100</span>]:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add custom classification head</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(base_model.output)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> Dense(<span class="dv">8</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x)  <span class="co"># 8 Palawan classes</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>, outputs<span class="op">=</span>output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>When to Use:</strong> - Limited training data (&lt;5,000 samples) - Similar task to pre-training (natural images) - Need quick results</p>
<p><strong>Caution:</strong> ImageNet has RGB images. Sentinel-2 has 10+ bands. Adaptation strategies needed (Session 4).</p>
</section>
<section id="e.3-data-augmentation---artificially-expanding-training-data" class="level4">
<h4 class="anchored" data-anchor-id="e.3-data-augmentation---artificially-expanding-training-data">E.3: Data Augmentation - Artificially Expanding Training Data</h4>
<p><strong>Geometric Transformations:</strong> - <strong>Rotation:</strong> 90°, 180°, 270° (satellites view from any angle) - <strong>Horizontal/Vertical Flip:</strong> Valid for overhead imagery - <strong>Zoom/Scale:</strong> Simulate different resolutions - <strong>Translation:</strong> Small shifts</p>
<p><strong>Spectral Augmentations:</strong> - <strong>Brightness/Contrast:</strong> Simulate atmospheric conditions - <strong>Gaussian Noise:</strong> Simulate sensor noise - <strong>Band Dropout:</strong> Improve robustness</p>
<p><strong>Philippine Context:</strong> - ✓ Use rotation/flips for land cover (no preferential orientation) - ✗ Avoid rotation for infrastructure (roads have direction) - ✓ Augment brightness for cloud variations</p>
<p><strong>Implementation (Session 4):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>augmentation <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">90</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    vertical_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    brightness_range<span class="op">=</span>[<span class="fl">0.8</span>, <span class="fl">1.2</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="e.4-computational-requirements" class="level4">
<h4 class="anchored" data-anchor-id="e.4-computational-requirements">E.4: Computational Requirements</h4>
<p><strong>Training Resources:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Training Time*</th>
<th>GPU Memory</th>
<th>Cost (Colab Pro)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN</td>
<td>30 min</td>
<td>4 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="even">
<td>ResNet-50 (fine-tune)</td>
<td>2-4 hours</td>
<td>8 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="odd">
<td>U-Net (segmentation)</td>
<td>4-8 hours</td>
<td>12 GB</td>
<td>Pro needed</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>24+ hours</td>
<td>16 GB</td>
<td>Pro+ needed</td>
</tr>
</tbody>
</table>
<p>*1,000 training images, 50 epochs, V100 GPU</p>
<p><strong>Philippine Context:</strong> - PhilSA uses on-premise GPU servers (8× NVIDIA A100) - Universities: Limited GPU access (submit jobs) - Practitioners: Google Colab Pro ($10/month) recommended</p>
<p><strong>Optimization Strategies (Session 4):</strong> - Use mixed precision training (FP16) - Reduce batch size if memory limited - Train on smaller image chips (128×128 instead of 256×256) - Use gradient checkpointing for large models</p>
</section>
<section id="e.5-model-interpretability---understanding-cnn-decisions" class="level4">
<h4 class="anchored" data-anchor-id="e.5-model-interpretability---understanding-cnn-decisions">E.5: Model Interpretability - Understanding CNN Decisions</h4>
<p><strong>Why It Matters:</strong> - DENR reports need explanations (“Why was this classified as deforested?”) - Debugging poor performance - Building stakeholder trust</p>
<p><strong>Techniques (Session 4):</strong> 1. <strong>Activation Visualization:</strong> See what filters learned 2. <strong>Saliency Maps:</strong> Which pixels influenced decision? 3. <strong>Class Activation Maps (CAM):</strong> Highlight relevant regions 4. <strong>Filter Visualization:</strong> What patterns do filters detect?</p>
<p><strong>Philippine Application Example:</strong> - <strong>Question:</strong> Why did model classify mangroves as agriculture? - <strong>CAM Analysis:</strong> Model focused on water proximity, not canopy structure - <strong>Solution:</strong> Add texture features or more mangrove training samples</p>
<hr>
</section>
</section>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<section id="automatic-feature-learning" class="level3">
<h3 class="anchored" data-anchor-id="automatic-feature-learning">Automatic Feature Learning</h3>
<p><strong>What is it?</strong> CNNs learn optimal features directly from raw pixel data, eliminating manual feature engineering.</p>
<p><strong>How it works:</strong> - <strong>Layer 1:</strong> Learns edges (horizontal, vertical, diagonal) - like Sobel filters you created manually - <strong>Layer 2:</strong> Combines edges into textures and simple shapes - <strong>Layer 3:</strong> Combines shapes into complex patterns (canopy structure, urban grid) - <strong>Layer 4+:</strong> High-level semantic features (forest type, settlement pattern)</p>
<p><strong>Comparison to Session 2:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 50%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Random Forest (Session 2)</th>
<th>CNN (Session 3-4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Features</strong></td>
<td>Manual (NDVI, GLCM)</td>
<td>Learned automatically</td>
</tr>
<tr class="even">
<td><strong>Spatial Context</strong></td>
<td>Limited (within feature)</td>
<td>Fully exploited (receptive field)</td>
</tr>
<tr class="odd">
<td><strong>Data Needed</strong></td>
<td>500-1,000 samples</td>
<td>5,000-10,000 samples</td>
</tr>
<tr class="even">
<td><strong>Training Time</strong></td>
<td>Minutes</td>
<td>Hours</td>
</tr>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>80-85% typical</td>
<td>90-95% possible</td>
</tr>
</tbody>
</table>
</section>
<section id="receptive-field" class="level3">
<h3 class="anchored" data-anchor-id="receptive-field">Receptive Field</h3>
<p><strong>Definition:</strong> The region of input image that influences a particular neuron’s activation.</p>
<p><strong>Example:</strong> - A neuron in Layer 1 sees a 3×3 pixel region (30m × 30m for Sentinel-2) - A neuron in Layer 3 sees a 15×15 pixel region (150m × 150m) - Output neuron “sees” entire image chip</p>
<p><strong>Why it matters:</strong> - Small objects (buildings): Need fewer layers - Large objects (agricultural fields): Need deeper networks - Contextual classification: Large receptive field captures neighborhood</p>
<p><strong>Philippine Example:</strong> Classifying a pixel as “mangrove” requires seeing water proximity (large receptive field) AND canopy texture (small receptive field). Multi-scale processing essential!</p>
</section>
<section id="translation-invariance" class="level3">
<h3 class="anchored" data-anchor-id="translation-invariance">Translation Invariance</h3>
<p><strong>What is it?</strong> CNN can recognize patterns regardless of position in image.</p>
<p><strong>How achieved:</strong> 1. <strong>Parameter sharing:</strong> Same filter applied everywhere 2. <strong>Pooling:</strong> Abstracts exact position</p>
<p><strong>EO Benefit:</strong> Forest is forest whether in top-left or bottom-right of image. Train once, apply anywhere in Philippines!</p>
<p><strong>Contrast with Position:</strong> For some tasks, position DOES matter (e.g., urban always near coasts in Philippines). Advanced architectures can encode position.</p>
</section>
<section id="gradient-descent-and-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</h3>
<p><strong>Intuitive Explanation:</strong></p>
<p>Imagine hiking down a foggy mountain (error surface) to reach valley (minimum loss): - <strong>Gradient:</strong> Direction of steepest descent - <strong>Learning rate:</strong> Step size - <strong>Backpropagation:</strong> Efficiently calculates gradients for all weights</p>
<p><strong>Training Process:</strong> 1. Forward pass: Image → Predictions 2. Calculate loss: How wrong were predictions? 3. Backward pass: Compute ∂Loss/∂Weight for every parameter 4. Update weights: Move “downhill” toward better performance</p>
<p><strong>Common Issues:</strong> - <strong>Learning rate too high:</strong> Jump over minimum (unstable) - <strong>Learning rate too low:</strong> Painfully slow convergence - <strong>Local minima:</strong> Stuck in suboptimal solution (less common with large networks)</p>
<p><strong>Session 4:</strong> Implement Adam optimizer (adaptive learning rates)</p>
<hr>
</section>
</section>
<section id="interactive-demonstrations" class="level2">
<h2 class="anchored" data-anchor-id="interactive-demonstrations">Interactive Demonstrations</h2>
<section id="demo-1-perceptron-playground" class="level3">
<h3 class="anchored" data-anchor-id="demo-1-perceptron-playground">Demo 1: Perceptron Playground</h3>
<p><strong>Objective:</strong> Build intuition for how perceptrons learn decision boundaries</p>
<p><strong>Activity:</strong> 1. Load 2D dataset (NDVI vs NDWI for water classification) 2. Initialize random weights 3. Visualize decision boundary 4. Update weights iteratively 5. Watch boundary align with data</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Expected Outcome:</strong> Understand that neural networks find separating hyperplanes through gradient descent.</p>
</section>
<section id="demo-2-activation-function-gallery" class="level3">
<h3 class="anchored" data-anchor-id="demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</h3>
<p><strong>Objective:</strong> Visualize how different activation functions transform data</p>
<p><strong>Activity:</strong> 1. Plot sigmoid, ReLU, tanh, Leaky ReLU 2. Apply to Sentinel-2 reflectance values 3. Compare output distributions 4. See why ReLU is most popular</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 2)</p>
<p><strong>Key Insight:</strong> ReLU is simple, fast, and sparse (many zeros = efficient).</p>
</section>
<section id="demo-3-manual-convolution-on-sentinel-2" class="level3">
<h3 class="anchored" data-anchor-id="demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</h3>
<p><strong>Objective:</strong> Understand convolution as a sliding filter operation</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 NIR band (Palawan forest patch) 2. Define edge detection filter (3×3 Sobel) 3. Manually compute convolution (NumPy) 4. Visualize feature map 5. Try different filters (blur, sharpen, Gaussian)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Aha Moment:</strong> “Edge detection filter highlights forest boundaries—exactly what CNN learns automatically!”</p>
</section>
<section id="demo-4-pooling-demonstration" class="level3">
<h3 class="anchored" data-anchor-id="demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</h3>
<p><strong>Objective:</strong> Understand downsampling and translation invariance</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 image chip (256×256) 2. Apply max pooling (2×2, stride 2) 3. Compare original vs pooled (128×128) 4. Shift image by 1 pixel, repeat 5. See that pooled output is nearly identical (translation invariance)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 3)</p>
<p><strong>Takeaway:</strong> Pooling reduces dimensionality while preserving important features.</p>
</section>
<section id="demo-5-architecture-exploration" class="level3">
<h3 class="anchored" data-anchor-id="demo-5-architecture-exploration">Demo 5: Architecture Exploration</h3>
<p><strong>Objective:</strong> Compare CNN architectures visually</p>
<p><strong>Activity:</strong> 1. Visualize LeNet-5, VGG-16, ResNet-50, U-Net architectures 2. Count parameters for each 3. Trace receptive field growth 4. Discuss trade-offs (accuracy vs speed)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 4)</p>
<p><strong>Connection to Session 4:</strong> Choose architecture based on task (classification → ResNet, segmentation → U-Net).</p>
<hr>
</section>
</section>
<section id="philippine-eo-applications" class="level2">
<h2 class="anchored" data-anchor-id="philippine-eo-applications">Philippine EO Applications</h2>
<section id="philsa-space-dashboard" class="level3">
<h3 class="anchored" data-anchor-id="philsa-space-dashboard">PhilSA Space+ Dashboard</h3>
<p><strong>Current CNN Applications:</strong></p>
<ol type="1">
<li><strong>Automated Cloud Masking</strong>
<ul>
<li><strong>Model:</strong> U-Net trained on 5,000 Sentinel-2 scenes</li>
<li><strong>Performance:</strong> 95% accuracy, 2 min per scene</li>
<li><strong>Impact:</strong> Enables rapid mosaic generation</li>
</ul></li>
<li><strong>Land Cover Classification (National)</strong>
<ul>
<li><strong>Model:</strong> ResNet-50 fine-tuned on Philippine landscape</li>
<li><strong>Classes:</strong> 10 (following FAO LCCS)</li>
<li><strong>Coverage:</strong> Entire Philippines, quarterly updates</li>
<li><strong>Users:</strong> DENR, DAR, NEDA</li>
</ul></li>
<li><strong>Disaster Rapid Mapping</strong>
<ul>
<li><strong>Flood Detection:</strong> Sentinel-1 + U-Net → 6-hour response</li>
<li><strong>Damage Assessment:</strong> High-res + object detection → building damage maps</li>
<li><strong>Integration:</strong> NDRRMC operations dashboard</li>
</ul></li>
</ol>
</section>
<section id="denr-forest-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="denr-forest-monitoring">DENR Forest Monitoring</h3>
<p><strong>CNN Use Cases:</strong></p>
<ul>
<li><strong>Protected Area Surveillance:</strong> Monthly Sentinel-2 analysis (ResNet classifier)</li>
<li><strong>Illegal Logging Detection:</strong> Change detection CNN on multi-temporal stacks</li>
<li><strong>Biodiversity Hotspot Mapping:</strong> Fine-grained forest type classification</li>
<li><strong>REDD+ MRV:</strong> Automated forest cover change reporting</li>
</ul>
<p><strong>Data Pipeline:</strong></p>
<pre><code>Sentinel-2 (PhilSA) → Preprocessing (cloud mask) →
CNN Classification → Change Detection →
Alert Generation → Field Validation</code></pre>
</section>
<section id="lgu-applications-session-4-focus" class="level3">
<h3 class="anchored" data-anchor-id="lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</h3>
<p><strong>Accessible CNN Tools for Local Governments:</strong></p>
<ol type="1">
<li><strong>ASTI SkAI-Pinas:</strong> Pre-trained models for common PH tasks</li>
<li><strong>Google Earth Engine:</strong> CNN inference on cloud platform</li>
<li><strong>Colab Notebooks:</strong> Low-cost GPU training (this training!)</li>
</ol>
<p><strong>Example Workflow (Session 4):</strong> - LGU staff collects 500 training labels (Palawan land cover) - Fine-tunes ResNet-50 using Session 4 notebook - Deploys model for quarterly monitoring - Integrates into local land use planning</p>
<hr>
</section>
</section>
<section id="expected-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="expected-outcomes">Expected Outcomes</h2>
<section id="conceptual-understanding" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h3>
<p>By the end of Session 3, you should be able to:</p>
<p>✅ <strong>Explain to a colleague</strong> why CNNs are better than Random Forest for complex spatial patterns ✅ <strong>Sketch</strong> a simple CNN architecture and label components (Conv, Pool, FC) ✅ <strong>Describe</strong> what happens during forward and backward propagation ✅ <strong>Identify</strong> appropriate architectures for classification vs segmentation ✅ <strong>Discuss</strong> data requirements and computational constraints</p>
</section>
<section id="technical-skills" class="level3">
<h3 class="anchored" data-anchor-id="technical-skills">Technical Skills</h3>
<p>✅ <strong>Build</strong> a simple perceptron from scratch using NumPy ✅ <strong>Implement</strong> activation functions and visualize their behavior ✅ <strong>Perform</strong> manual convolution on satellite imagery ✅ <strong>Apply</strong> classic edge detection filters (Sobel, Gaussian) ✅ <strong>Visualize</strong> feature maps and pooling operations</p>
</section>
<section id="practical-readiness-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="practical-readiness-for-session-4">Practical Readiness for Session 4</h3>
<p>✅ <strong>Understand</strong> why we’ll use TensorFlow/Keras (vs building from scratch) ✅ <strong>Anticipate</strong> challenges with Sentinel-2 multi-band data ✅ <strong>Recognize</strong> data preparation needs (chips, labels, augmentation) ✅ <strong>Set expectations</strong> for training time and resource requirements</p>
<hr>
</section>
</section>
<section id="hands-on-notebooks" class="level2">
<h2 class="anchored" data-anchor-id="hands-on-notebooks">Hands-On Notebooks</h2>
<section id="access-the-interactive-materials" class="level3">
<h3 class="anchored" data-anchor-id="access-the-interactive-materials">Access the Interactive Materials</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>📓 Jupyter Notebooks (Theory + Interactive)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two comprehensive notebooks guide you through neural network and CNN fundamentals:</p>
<p><strong>Notebook 1: Neural Network Theory</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Build perceptron from scratch - Implement activation functions - Train 2-layer network on spectral data - Explore learning rate effects - Visualize decision boundaries</p>
<p><strong>Duration:</strong> 45 minutes</p>
<hr>
<p><strong>Notebook 2: CNN Operations</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Manual convolution on Sentinel-2 imagery - Edge detection filters (Sobel, Gaussian, Laplacian) - Max pooling demonstration - Architecture comparison (LeNet, VGG, ResNet, U-Net) - Feature map visualization</p>
<p><strong>Duration:</strong> 55 minutes</p>
<hr>
<p><strong>Google Colab:</strong> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open Theory Notebook"></a> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open CNN Notebook"></a></p>
</div>
</div>
</section>
<section id="supporting-documentation" class="level3">
<h3 class="anchored" data-anchor-id="supporting-documentation">Supporting Documentation</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>📚 Reference Materials
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>CNN Architectures Deep Dive:</strong> <a href="../../resources/cheatsheets.html"><code>Cheatsheets &amp; References</code></a></p>
<p>Detailed explanations of LeNet-5, VGG-16, ResNet-50, U-Net, and modern variants (EfficientNet, Vision Transformers). Includes architecture diagrams, parameter counts, and EO applications.</p>
<hr>
<p><strong>EO Applications Guide:</strong> <a href="../../resources/glossary.html"><code>Glossary &amp; EO Resources</code></a></p>
<p>Comprehensive guide to CNN applications in Earth observation: - Scene classification examples - Semantic segmentation workflows - Object detection case studies - Change detection methods - Philippine-specific use cases</p>
<hr>
<p><strong>Session Overview:</strong> <a href="../../day2/index.html"><code>Day 2 Overview</code></a></p>
<p>Quick reference with session objectives, structure, and prerequisites.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="troubleshooting" class="level2">
<h2 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h2>
<section id="common-conceptual-questions" class="level3">
<h3 class="anchored" data-anchor-id="common-conceptual-questions">Common Conceptual Questions</h3>
<p><strong>“Why does CNN need so much more data than Random Forest?”</strong></p>
<p>Random Forest learns from features YOU engineered (NDVI, GLCM). It needs to learn relationships between ~10-20 features.</p>
<p>CNNs learn from raw pixels (~10,000 per image chip). It needs to learn what features to extract PLUS how to classify. Much harder optimization problem = more data needed.</p>
<p><strong>Mitigation:</strong> Transfer learning (Session 4) dramatically reduces data needs.</p>
<hr>
<p><strong>“Can I use CNNs with small datasets (&lt;500 samples)?”</strong></p>
<p>Technically yes, but results will be poor if training from scratch.</p>
<p><strong>Better approach:</strong> 1. Use pre-trained models (ImageNet weights) 2. Aggressive data augmentation 3. Use simpler architectures (fewer layers) 4. Consider traditional ML if data is truly limited</p>
<hr>
<p><strong>“Why are my manual convolutions so slow in the notebook?”</strong></p>
<p>NumPy convolutions (for learning) are intentionally simple. Production CNNs use highly optimized libraries (cuDNN) on GPUs—1000× faster!</p>
<p>Session 4 will use TensorFlow/PyTorch with GPU acceleration.</p>
<hr>
<p><strong>“How do I know which architecture to use?”</strong></p>
<p><strong>Simple decision tree:</strong> - <strong>Classification task</strong> (one label per image) → ResNet, EfficientNet - <strong>Segmentation task</strong> (label every pixel) → U-Net, DeepLabv3+ - <strong>Object detection</strong> (find + localize) → YOLO, Faster R-CNN - <strong>Limited data</strong> → Simpler architecture + transfer learning - <strong>Real-time inference needed</strong> → MobileNet, EfficientNet-Lite</p>
<p>Session 4 will implement ResNet (classification) and U-Net (segmentation).</p>
<hr>
</section>
<section id="technical-issues" class="level3">
<h3 class="anchored" data-anchor-id="technical-issues">Technical Issues</h3>
<p><strong>“Notebook cells won’t execute in Colab”</strong></p>
<ol type="1">
<li>Check GPU is enabled: Runtime → Change runtime type → GPU</li>
<li>Restart runtime: Runtime → Restart runtime</li>
<li>Verify installations: <code>!pip list | grep numpy</code></li>
</ol>
<hr>
<p><strong>“Out of memory error when running convolutions”</strong></p>
<ul>
<li>Use smaller image chips (128×128 instead of 256×256)</li>
<li>Process one image at a time (don’t load entire dataset)</li>
<li>Restart Colab runtime to clear memory</li>
</ul>
<hr>
<p><strong>“Visualizations aren’t displaying”</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add to beginning of notebook</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
<p><strong>“Can’t access Sentinel-2 data in notebook”</strong></p>
<p>Session 3 uses pre-downloaded Sentinel-2 samples (included in notebook). No GEE authentication needed.</p>
<p>Session 4 will integrate GEE for larger-scale data loading.</p>
<hr>
</section>
<section id="getting-help" class="level3">
<h3 class="anchored" data-anchor-id="getting-help">Getting Help</h3>
<ul>
<li>📖 <a href="https://developers.google.com/machine-learning/crash-course/image-classification">CNN Tutorial (Google ML Crash Course)</a></li>
<li>📖 <a href="http://cs231n.stanford.edu/">CS231n Stanford Course</a> - Best CNN educational resource</li>
<li>💬 <a href="mailto:training@copphil.org">Instructor support</a> - Questions during lab hours</li>
<li>📧 <a href="https://data.philsa.gov.ph/support">PhilSA Data Support</a> - Access issues</li>
</ul>
<hr>
</section>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<section id="foundational-learning" class="level3">
<h3 class="anchored" data-anchor-id="foundational-learning">Foundational Learning</h3>
<p><strong>Neural Networks:</strong> - <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Network Series</a> - Best intuitive explanation (visual) - <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Free Book)</a> - <a href="https://www.coursera.org/specializations/deep-learning">Andrew Ng’s Deep Learning Specialization</a> (Coursera)</p>
<p><strong>CNNs Specifically:</strong> - <a href="http://cs231n.stanford.edu/schedule.html">Stanford CS231n Lectures</a> - Comprehensive (free) - <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer (Interactive)</a> - Visualize CNNs in browser - <a href="https://distill.pub/">Distill.pub Articles</a> - Beautiful visual explanations</p>
</section>
<section id="earth-observation-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="earth-observation-deep-learning">Earth Observation Deep Learning</h3>
<p><strong>Papers:</strong> - Zhu et al.&nbsp;(2017). “Deep Learning in Remote Sensing: A Review.” <em>IEEE GRSM</em> - Ma et al.&nbsp;(2019). “Deep learning in remote sensing applications: A meta-analysis and review.” <em>ISPRS</em> - Rußwurm &amp; Körner (2020). “Self-attention for raw optical satellite time series classification.” <em>ISPRS</em></p>
<p><strong>Tutorials:</strong> - <a href="https://eo-college.org/resources/deep-learning/">Deep Learning for Earth Observation (ESA)</a> - <a href="https://rastervision.io/">Raster Vision</a> - Framework for geospatial ML - <a href="https://torchgeo.readthedocs.io/">TorchGeo</a> - PyTorch library for geospatial data</p>
<p><strong>Datasets:</strong> - <a href="https://github.com/phelber/EuroSAT">EuroSAT</a> - Sentinel-2 scene classification (10 classes) - <a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">UC Merced Land Use</a> - High-res classification - <a href="http://deepglobe.org/">DeepGlobe</a> - Segmentation challenges - <a href="https://spacenet.ai/">SpaceNet</a> - Building detection</p>
</section>
<section id="philippine-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-context">Philippine Context</h3>
<ul>
<li><a href="https://philsa.gov.ph/publications/">PhilSA Research Publications</a> - CNN applications in Philippines</li>
<li><a href="https://skai.dost.gov.ph/docs">ASTI SkAI-Pinas Documentation</a> - Pre-trained PH models</li>
<li><a href="https://dimer.asti.dost.gov.ph/">DIMER Database</a> - Philippine disaster imagery</li>
</ul>
<hr>
</section>
</section>
<section id="assessment" class="level2">
<h2 class="anchored" data-anchor-id="assessment">Assessment</h2>
<section id="formative-assessment-during-session" class="level3">
<h3 class="anchored" data-anchor-id="formative-assessment-during-session">Formative Assessment (During Session)</h3>
<p><strong>Self-Check Questions:</strong></p>
<ol type="1">
<li>✓ Can you explain the difference between a perceptron and a multi-layer network?</li>
<li>✓ Why does ReLU work better than sigmoid in hidden layers?</li>
<li>✓ What does a convolutional filter “learn” during training?</li>
<li>✓ How does pooling provide translation invariance?</li>
<li>✓ When would you use U-Net instead of ResNet?</li>
</ol>
<p><strong>Interactive Exercises:</strong> - ✓ Complete all TODO cells in theory notebook - ✓ Implement manual convolution from scratch - ✓ Visualize at least 3 different activation functions - ✓ Compare filter responses on forest vs urban areas</p>
</section>
<section id="summative-assessment-end-of-session" class="level3">
<h3 class="anchored" data-anchor-id="summative-assessment-end-of-session">Summative Assessment (End of Session)</h3>
<p><strong>Knowledge Check (10 questions, multiple choice):</strong> - Neural network components - CNN architecture understanding - Application selection (classification vs segmentation) - Data requirements and constraints</p>
<p><strong>Practical Demonstration:</strong> - Explain CNN decision-making process for a sample image - Sketch appropriate architecture for given EO task - Estimate data requirements for Philippine use case</p>
<p><strong>Readiness for Session 4:</strong> - ✓ Understand TensorFlow/Keras workflow conceptually - ✓ Recognize data preparation needs - ✓ Anticipate training challenges</p>
<hr>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>After Session 3
</div>
</div>
<div class="callout-body-container callout-body">
<p>You now understand CNN theory and operations! Session 4 puts this knowledge into practice with hands-on implementation.</p>
<p><strong>Session 4: Hands-On CNN Implementation</strong></p>
<p>You’ll build and train: 1. <strong>ResNet Classifier:</strong> Palawan land cover (8 classes) 2. <strong>U-Net Segmentation:</strong> Flood mapping (Central Luzon)</p>
<p>Using TensorFlow/Keras with real Sentinel-2 data.</p>
<p><a href="../../day2/sessions/session4.html" class="btn btn-primary">Continue to Session 4 →</a></p>
</div>
</div>
<section id="recommended-pre-work-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</h3>
<p>Before Session 4, please:</p>
<ol type="1">
<li>✅ <strong>Review notebook exercises</strong> - Ensure you understand convolution and activation functions</li>
<li>✅ <strong>Read U-Net paper</strong> - <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;2015</a> (15 min)</li>
<li>✅ <strong>Check GPU access</strong> - Enable GPU in Colab (Settings → Hardware Accelerator → GPU)</li>
<li>✅ <strong>Install TensorFlow</strong> - <code>!pip install tensorflow==2.15</code> (will do in Session 4, but test now)</li>
</ol>
</section>
<section id="extended-learning-paths" class="level3">
<h3 class="anchored" data-anchor-id="extended-learning-paths">Extended Learning Paths</h3>
<p><strong>Path 1: Deep Dive into Theory</strong> - Complete CS231n Stanford course - Implement backpropagation from scratch - Study optimization algorithms (Adam, RMSprop)</p>
<p><strong>Path 2: Explore Advanced Architectures</strong> - Attention mechanisms (Transformers for EO) - Vision Transformers (ViT) - Self-supervised learning (SimCLR, MoCo)</p>
<p><strong>Path 3: Philippine EO Applications</strong> - Contribute training data to PhilSA Space+ - Develop CNN model for local area - Publish results in Philippine GIS conference</p>
<hr>
</section>
</section>
<section id="quick-links" class="level2">
<h2 class="anchored" data-anchor-id="quick-links">Quick Links</h2>
<ul>
<li><strong>Part A (ML→DL):</strong> 15 min - Keep conceptual, avoid getting bogged down in math</li>
<li><strong>Part B (NN Fundamentals):</strong> 25 min - Live code perceptron, let students modify</li>
<li><strong>Part C (CNNs):</strong> 30 min - Most critical section, use lots of visuals</li>
<li><strong>Part D (EO Applications):</strong> 25 min - Show real PhilSA examples if possible</li>
<li><strong>Part E (Practical):</strong> 15 min - Set realistic expectations for Session 4</li>
</ul>
<p><strong>Total:</strong> 110 min (2.5 hours with 20 min buffer for questions)</p>
<hr>
<p><strong>Common Student Challenges:</strong></p>
<ol type="1">
<li><p><strong>“I don’t understand backpropagation”</strong> → Focus on intuition (gradient descent down error surface), not calculus. Session 4 uses libraries that handle this automatically.</p></li>
<li><p><strong>“Why are we doing manual convolutions in NumPy?”</strong> → Emphasize this is for learning. Session 4 uses optimized libraries (1000× faster). Like learning to drive with manual transmission—helps understand what’s happening under the hood.</p></li>
<li><p><strong>“Will my laptop be fast enough for Session 4?”</strong> → No local installation needed! Google Colab provides free GPUs. Sessions 4 notebooks are optimized for Colab free tier.</p></li>
<li><p><strong>“Can CNNs use Sentinel-2’s 10+ bands?”</strong> → YES! Unlike ImageNet RGB pre-training. Session 4 shows adaptation strategies. This is a huge advantage of CNNs for EO.</p></li>
</ol>
<hr>
<p><strong>Teaching Tips:</strong></p>
<ul>
<li><strong>Start with Familiar:</strong> Connect to Session 2 Random Forest throughout</li>
<li><strong>Visual Heavy:</strong> Show lots of images/diagrams (CNNs are visual learners!)</li>
<li><strong>Interactive:</strong> Have students modify filter values in real-time</li>
<li><strong>Philippine Examples:</strong> Use Palawan imagery from Session 2 for continuity</li>
<li><strong>Manage Expectations:</strong> Be honest about data/compute requirements</li>
<li><strong>Celebrate Progress:</strong> “You now understand CNNs better than 90% of GIS professionals!”</li>
</ul>
<hr>
<p><strong>Demonstration Best Practices:</strong></p>
<ol type="1">
<li><strong>Perceptron Demo:</strong> Use simple 2D data first (NDVI vs NDWI), visualize decision boundary updating in real-time</li>
<li><strong>Convolution Demo:</strong> Pick dramatic example (forest edge) where edge detection is obvious</li>
<li><strong>Architecture Comparison:</strong> Show parameter counts to emphasize efficiency gains</li>
<li><strong>Philippine Focus:</strong> Always end each section with “How does this help DENR/PhilSA/LGUs?”</li>
</ol>
<hr>
<p><strong>Assessment Rubric:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Excellent (5)</th>
<th>Good (4)</th>
<th>Adequate (3)</th>
<th>Needs Improvement (2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Conceptual</strong></td>
<td>Explains CNN advantages with examples</td>
<td>States CNN benefits</td>
<td>Lists CNN components</td>
<td>Confuses CNN with traditional ML</td>
</tr>
<tr class="even">
<td><strong>Technical</strong></td>
<td>Implements convolution from scratch</td>
<td>Completes all notebook exercises</td>
<td>Completes &gt;70% exercises</td>
<td>Struggles with NumPy operations</td>
</tr>
<tr class="odd">
<td><strong>Application</strong></td>
<td>Proposes appropriate architecture for novel task</td>
<td>Identifies correct architecture for standard tasks</td>
<td>Distinguishes classification vs segmentation</td>
<td>Cannot select architecture</td>
</tr>
<tr class="even">
<td><strong>Readiness</strong></td>
<td>Articulates Session 4 workflow</td>
<td>Understands TensorFlow role</td>
<td>Knows Session 4 is hands-on</td>
<td>Unclear on next steps</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Session 4 Transition:</strong></p>
<p>End with excitement: <em>“You’ve learned the theory. Tomorrow you’ll build a production-ready land cover classifier and flood mapper using TensorFlow. Bring your questions, bring your data ideas, and get ready to train some CNNs!”</em></p>
<p><strong>Preview Session 4 Outcomes:</strong> - ResNet model achieving &gt;85% accuracy on Palawan - U-Net generating flood maps in 2 minutes - Exportable models for operational use</p>
<hr>
<p><strong>Backup Activities (if ahead of schedule):</strong></p>
<ol type="1">
<li><strong>Live Code Challenge:</strong> “Build a 3-layer network for Palawan classification” (10 min)</li>
<li><strong>Architecture Design:</strong> “Sketch CNN for building detection task” (5 min)</li>
<li><strong>Group Discussion:</strong> “What EO problem in YOUR organization could use CNNs?” (10 min)</li>
</ol>
<hr>
<p><strong>Resource Check (Before Session):</strong></p>
<ul>
<li>✓ Test both Colab notebooks execute without errors</li>
<li>✓ Verify Sentinel-2 sample data loads correctly</li>
<li>✓ Pre-download datasets to Google Drive (backup)</li>
<li>✓ Have architecture diagrams ready (slides or drawn)</li>
<li>✓ Queue up CNN Explainer website for demos</li>
<li>✓ Test video/screen sharing for visualizations :::</li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/DimitrisKasabalis\.github\.io\/cophil-training-v1\.0");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day2/sessions/session2.html" class="pagination-link" aria-label="Session 2: Advanced Palawan Land Cover Lab">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Session 2: Advanced Palawan Land Cover Lab</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day2/sessions/session4.html" class="pagination-link" aria-label="Session 4: CNN Hands-on Lab">
        <span class="nav-page-text">Session 4: CNN Hands-on Lab</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Session 3: Introduction to Deep Learning and CNNs"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Neural Networks and Convolutional Architectures for Earth Observation"</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "CoPhil Advanced Training Program"</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">nav</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb"</span><span class="ot"> aria-label</span><span class="op">=</span><span class="st">"Breadcrumb"</span><span class="dt">&gt;</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../../index.html"</span><span class="dt">&gt;</span>Home<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../index.html"</span><span class="dt">&gt;</span>Day 2<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-current"</span><span class="dt">&gt;</span>Session 3<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">nav</span><span class="dt">&gt;</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>::: {.hero}</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Session 3: Introduction to Deep Learning and CNNs</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neural Networks and Convolutional Architectures for Earth Observation</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>Transitioning from feature engineering to feature learning</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session Overview</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>**Duration:** 2.5 hours | **Type:** Theory + Interactive Demonstrations | **Difficulty:** Intermediate</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>This pivotal session bridges traditional machine learning (Sessions 1-2) and modern deep learning approaches. You'll understand the fundamental shift from manual feature engineering to automatic feature learning through neural networks, with specific focus on Convolutional Neural Networks (CNNs) for Earth observation applications.</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## Presentation Slides</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> src</span><span class="op">=</span><span class="st">"../presentations/session3_deep_learning.html"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"100%"</span><span class="ot"> height</span><span class="op">=</span><span class="st">"600"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"border: 1px solid #ccc; border-radius: 4px;"</span><span class="dt">&gt;&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## Prerequisites</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Complete Sessions 1-2 (Random Forest classification)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Understanding of classification concepts (accuracy, confusion matrix)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Basic Python and NumPy familiarity</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Colab environment with GPU runtime enabled</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Conceptual understanding of matrix operations</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="fu">## What You'll Learn</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>After completing this session, you will be able to:</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Understand the ML → DL Transition**</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Recognize when to use traditional ML vs deep learning</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Explain the automatic feature learning paradigm</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Identify computational requirements and trade-offs</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understand data requirements for deep learning success</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Master Neural Network Fundamentals**</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Build simple perceptrons from scratch using NumPy</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Implement activation functions (ReLU, sigmoid, softmax)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understand forward propagation and backpropagation</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Visualize decision boundaries and learning dynamics</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Comprehend Convolutional Neural Networks**</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Explain convolution operations and their purpose</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understand pooling, padding, and stride concepts</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Visualize filter responses on satellite imagery</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compare CNN architectures (LeNet, VGG, ResNet, U-Net)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Apply CNNs to Earth Observation Tasks**</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Identify appropriate CNN architectures for EO problems</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understand scene classification vs semantic segmentation</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Recognize object detection and change detection approaches</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Connect CNN capabilities to Philippine EO applications</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Navigate Practical Considerations**</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Address data-centric AI principles for EO</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Handle limited training data scenarios</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understand transfer learning and pre-trained models</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Recognize computational constraints and optimization strategies</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session Structure</span></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part A: From Machine Learning to Deep Learning (15 minutes)</span></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>Understanding the paradigm shift from manual to automatic feature learning.</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>::: {.feature-grid}</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>**🔧 Traditional ML (Sessions 1-2)**</span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>**What you did:**</span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manually calculated NDVI, NDWI, NDBI</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Engineered GLCM texture features</span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extracted temporal statistics</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combined features thoughtfully</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>**Pros:** Interpretable, works with small datasets</span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>**Cons:** Requires domain expertise, limited by imagination</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>**🧠 Deep Learning (Sessions 3-4)**</span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>**What CNNs do:**</span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn features automatically from raw pixels</span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Discover hidden patterns humans miss</span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Build hierarchical representations</span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimize end-to-end</span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>**Pros:** No feature engineering, state-of-the-art accuracy</span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>**Cons:** Needs large datasets, computationally intensive</span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a>**⚖️ When to Use Which?**</span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>**Use Random Forest when:**</span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limited training data (&lt;1000 samples)</span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Need interpretability (DENR reports)</span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Have domain features (indices)</span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fast prototyping needed</span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>**Use CNNs when:**</span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large labeled datasets (&gt;10,000 samples)</span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex spatial patterns</span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Maximum accuracy required</span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>GPU resources available</span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>**🇵🇭 Philippine EO Context**</span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>**PhilSA Applications:**</span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene classification (land cover)</span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cloud detection (Sentinel-2)</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building footprint extraction</span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood extent mapping</span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>**Why CNNs?**</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>Handle complex tropical landscapes, monsoon cloud patterns, informal settlements</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>**Key Insight:**</span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>*"In Sessions 1-2, you manually engineered features. CNNs will learn these features automatically—and discover new ones you never imagined!"*</span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part B: Neural Network Fundamentals (25 minutes)</span></span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>Building intuition from the ground up using interactive Jupyter notebooks.</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a><span class="fu">#### B.1: The Perceptron - Simplest Neural Unit</span></span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>Understanding the building block of all neural networks.</span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>**Mathematical Foundation:**</span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a>y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)</span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_i$: Input features (e.g., pixel values, NDVI)</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$w_i$: Learned weights</span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$b$: Bias term</span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f$: Activation function</span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y$: Output prediction</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>**Hands-On:** Build a perceptron from scratch to classify "Water vs Non-Water" using NDWI.</span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple perceptron implementation</span></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim):</span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim)</span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a>        linear_output <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(linear_output)</span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-190"><a href="#cb9-190" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, z):</span>
<span id="cb9-191"><a href="#cb9-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>  <span class="co"># Step function</span></span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a><span class="fu">#### B.2: Activation Functions - Adding Non-Linearity</span></span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a>Why neural networks need activation functions to solve complex problems.</span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a>::: {.feature-grid}</span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a>**Sigmoid**</span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a>$$\sigma(z) = \frac{1}{1 + e^{-z}}$$</span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a>**Range:** (0, 1)</span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a>**Use:** Binary classification output</span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a>**EO Example:** Cloud probability</span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a>**ReLU (Rectified Linear Unit)**</span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a>$$\text{ReLU}(z) = \max(0, z)$$</span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a>**Range:** [0, ∞)</span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a>**Use:** Hidden layers (most popular)</span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a>**Why:** Fast, sparse activation</span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a>**Softmax**</span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a>$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$</span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-221"><a href="#cb9-221" aria-hidden="true" tabindex="-1"></a>**Range:** (0, 1), sums to 1</span>
<span id="cb9-222"><a href="#cb9-222" aria-hidden="true" tabindex="-1"></a>**Use:** Multi-class classification</span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a>**EO Example:** Land cover classes</span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a>**Tanh**</span>
<span id="cb9-228"><a href="#cb9-228" aria-hidden="true" tabindex="-1"></a>$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$</span>
<span id="cb9-229"><a href="#cb9-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a>**Range:** (-1, 1)</span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a>**Use:** When centered data needed</span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a>**Note:** Less common than ReLU</span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-235"><a href="#cb9-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-236"><a href="#cb9-236" aria-hidden="true" tabindex="-1"></a>**Interactive Demo:** Visualize how each activation function transforms Sentinel-2 spectral values.</span>
<span id="cb9-237"><a href="#cb9-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-238"><a href="#cb9-238" aria-hidden="true" tabindex="-1"></a><span class="fu">#### B.3: Multi-Layer Networks - Learning Complex Patterns</span></span>
<span id="cb9-239"><a href="#cb9-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-240"><a href="#cb9-240" aria-hidden="true" tabindex="-1"></a>Stacking layers to learn hierarchical representations.</span>
<span id="cb9-241"><a href="#cb9-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-242"><a href="#cb9-242" aria-hidden="true" tabindex="-1"></a>**Architecture:**</span>
<span id="cb9-243"><a href="#cb9-243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-244"><a href="#cb9-244" aria-hidden="true" tabindex="-1"></a><span class="in">Input Layer (e.g., Sentinel-2 bands)</span></span>
<span id="cb9-245"><a href="#cb9-245" aria-hidden="true" tabindex="-1"></a><span class="in">    ↓</span></span>
<span id="cb9-246"><a href="#cb9-246" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 1 (64 neurons, ReLU)</span></span>
<span id="cb9-247"><a href="#cb9-247" aria-hidden="true" tabindex="-1"></a><span class="in">    ↓</span></span>
<span id="cb9-248"><a href="#cb9-248" aria-hidden="true" tabindex="-1"></a><span class="in">Hidden Layer 2 (32 neurons, ReLU)</span></span>
<span id="cb9-249"><a href="#cb9-249" aria-hidden="true" tabindex="-1"></a><span class="in">    ↓</span></span>
<span id="cb9-250"><a href="#cb9-250" aria-hidden="true" tabindex="-1"></a><span class="in">Output Layer (8 classes, Softmax)</span></span>
<span id="cb9-251"><a href="#cb9-251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-252"><a href="#cb9-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-253"><a href="#cb9-253" aria-hidden="true" tabindex="-1"></a>**What Each Layer Learns:**</span>
<span id="cb9-254"><a href="#cb9-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 1:** Low-level features (edges, textures)</span>
<span id="cb9-255"><a href="#cb9-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 2:** Mid-level features (shapes, patterns)</span>
<span id="cb9-256"><a href="#cb9-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 3:** High-level features (objects, scenes)</span>
<span id="cb9-257"><a href="#cb9-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output:** Class probabilities</span>
<span id="cb9-258"><a href="#cb9-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-259"><a href="#cb9-259" aria-hidden="true" tabindex="-1"></a>**Hands-On:** Train a 2-layer network to classify Palawan land cover using spectral features (from Session 2).</span>
<span id="cb9-260"><a href="#cb9-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-261"><a href="#cb9-261" aria-hidden="true" tabindex="-1"></a><span class="fu">#### B.4: Training Process - Learning from Data</span></span>
<span id="cb9-262"><a href="#cb9-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-263"><a href="#cb9-263" aria-hidden="true" tabindex="-1"></a>Understanding gradient descent and backpropagation intuitively.</span>
<span id="cb9-264"><a href="#cb9-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-265"><a href="#cb9-265" aria-hidden="true" tabindex="-1"></a>**Training Loop:**</span>
<span id="cb9-266"><a href="#cb9-266" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Forward Pass:** Input → Hidden → Output → Prediction</span>
<span id="cb9-267"><a href="#cb9-267" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Calculate Loss:** Compare prediction to true label</span>
<span id="cb9-268"><a href="#cb9-268" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Backward Pass:** Compute gradients (how much each weight contributed to error)</span>
<span id="cb9-269"><a href="#cb9-269" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Update Weights:** $w_{new} = w_{old} - \alpha \cdot \frac{\partial L}{\partial w}$</span>
<span id="cb9-270"><a href="#cb9-270" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Repeat:** Until loss converges</span>
<span id="cb9-271"><a href="#cb9-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-272"><a href="#cb9-272" aria-hidden="true" tabindex="-1"></a>**Key Hyperparameters:**</span>
<span id="cb9-273"><a href="#cb9-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learning Rate ($\alpha$):** Step size for weight updates (0.001 - 0.1)</span>
<span id="cb9-274"><a href="#cb9-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Batch Size:** Number of samples per gradient update (32, 64, 128)</span>
<span id="cb9-275"><a href="#cb9-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Epochs:** Complete passes through training data (10-100)</span>
<span id="cb9-276"><a href="#cb9-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-277"><a href="#cb9-277" aria-hidden="true" tabindex="-1"></a>**Interactive Exploration:** Experiment with learning rates to see overfitting vs underfitting.</span>
<span id="cb9-278"><a href="#cb9-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-279"><a href="#cb9-279" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part C: Convolutional Neural Networks (30 minutes)</span></span>
<span id="cb9-280"><a href="#cb9-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-281"><a href="#cb9-281" aria-hidden="true" tabindex="-1"></a>Deep dive into the architecture that revolutionized computer vision and Earth observation.</span>
<span id="cb9-282"><a href="#cb9-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-283"><a href="#cb9-283" aria-hidden="true" tabindex="-1"></a><span class="fu">#### C.1: Why CNNs for Images?</span></span>
<span id="cb9-284"><a href="#cb9-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-285"><a href="#cb9-285" aria-hidden="true" tabindex="-1"></a>**Problem with Regular Neural Networks:**</span>
<span id="cb9-286"><a href="#cb9-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A 10m Sentinel-2 image chip (256×256×10 bands) = 655,360 parameters just for first layer!</span>
<span id="cb9-287"><a href="#cb9-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No spatial awareness (treats nearby pixels same as distant ones)</span>
<span id="cb9-288"><a href="#cb9-288" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally infeasible</span>
<span id="cb9-289"><a href="#cb9-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-290"><a href="#cb9-290" aria-hidden="true" tabindex="-1"></a>**CNN Solutions:**</span>
<span id="cb9-291"><a href="#cb9-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Local Connectivity:** Each neuron connects to small spatial region</span>
<span id="cb9-292"><a href="#cb9-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter Sharing:** Same filter applied across entire image</span>
<span id="cb9-293"><a href="#cb9-293" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Translation Invariance:** Detect features anywhere in image</span>
<span id="cb9-294"><a href="#cb9-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-295"><a href="#cb9-295" aria-hidden="true" tabindex="-1"></a>**Result:** Millions fewer parameters, spatially-aware learning!</span>
<span id="cb9-296"><a href="#cb9-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-297"><a href="#cb9-297" aria-hidden="true" tabindex="-1"></a><span class="fu">#### C.2: Convolution Operation - The Heart of CNNs</span></span>
<span id="cb9-298"><a href="#cb9-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-299"><a href="#cb9-299" aria-hidden="true" tabindex="-1"></a>Understanding how convolutions extract features from satellite imagery.</span>
<span id="cb9-300"><a href="#cb9-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-301"><a href="#cb9-301" aria-hidden="true" tabindex="-1"></a>**Mathematical Definition:**</span>
<span id="cb9-302"><a href="#cb9-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-303"><a href="#cb9-303" aria-hidden="true" tabindex="-1"></a>(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m,n)</span>
<span id="cb9-304"><a href="#cb9-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-305"><a href="#cb9-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-306"><a href="#cb9-306" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb9-307"><a href="#cb9-307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$I$: Input image (e.g., Sentinel-2 NIR band)</span>
<span id="cb9-308"><a href="#cb9-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$K$: Filter/kernel (e.g., 3×3 edge detector)</span>
<span id="cb9-309"><a href="#cb9-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$*$: Convolution operator</span>
<span id="cb9-310"><a href="#cb9-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-311"><a href="#cb9-311" aria-hidden="true" tabindex="-1"></a>**Visual Example:**</span>
<span id="cb9-312"><a href="#cb9-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-313"><a href="#cb9-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-314"><a href="#cb9-314" aria-hidden="true" tabindex="-1"></a><span class="in">Sentinel-2 NIR Image (5×5)    Edge Detection Filter (3×3)</span></span>
<span id="cb9-315"><a href="#cb9-315" aria-hidden="true" tabindex="-1"></a><span class="in">┌─────────────────┐           ┌─────────┐</span></span>
<span id="cb9-316"><a href="#cb9-316" aria-hidden="true" tabindex="-1"></a><span class="in">│ 120 115 118 122 │           │ -1  -1  -1 │</span></span>
<span id="cb9-317"><a href="#cb9-317" aria-hidden="true" tabindex="-1"></a><span class="in">│ 118 245 242 125 │     *     │  0   0   0 │</span></span>
<span id="cb9-318"><a href="#cb9-318" aria-hidden="true" tabindex="-1"></a><span class="in">│ 119 248 244 121 │           │  1   1   1 │</span></span>
<span id="cb9-319"><a href="#cb9-319" aria-hidden="true" tabindex="-1"></a><span class="in">│ 121 116 119 123 │           └─────────┘</span></span>
<span id="cb9-320"><a href="#cb9-320" aria-hidden="true" tabindex="-1"></a><span class="in">└─────────────────┘</span></span>
<span id="cb9-321"><a href="#cb9-321" aria-hidden="true" tabindex="-1"></a><span class="in">                ↓</span></span>
<span id="cb9-322"><a href="#cb9-322" aria-hidden="true" tabindex="-1"></a><span class="in">    Feature Map (detects water edges)</span></span>
<span id="cb9-323"><a href="#cb9-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-324"><a href="#cb9-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-325"><a href="#cb9-325" aria-hidden="true" tabindex="-1"></a>**Hands-On:**</span>
<span id="cb9-326"><a href="#cb9-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply manual convolutions to Sentinel-2 patches</span>
<span id="cb9-327"><a href="#cb9-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize classic filters (edge detection, blur, sharpen)</span>
<span id="cb9-328"><a href="#cb9-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>See how filters respond to forests, water, urban areas</span>
<span id="cb9-329"><a href="#cb9-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-330"><a href="#cb9-330" aria-hidden="true" tabindex="-1"></a><span class="fu">#### C.3: CNN Building Blocks</span></span>
<span id="cb9-331"><a href="#cb9-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-332"><a href="#cb9-332" aria-hidden="true" tabindex="-1"></a>**1. Convolutional Layer**</span>
<span id="cb9-333"><a href="#cb9-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applies multiple filters to input</span>
<span id="cb9-334"><a href="#cb9-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each filter learns to detect different feature</span>
<span id="cb9-335"><a href="#cb9-335" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Feature maps (activations)</span>
<span id="cb9-336"><a href="#cb9-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-337"><a href="#cb9-337" aria-hidden="true" tabindex="-1"></a>**Parameters:**</span>
<span id="cb9-338"><a href="#cb9-338" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`filters`</span>: Number of filters (32, 64, 128...)</span>
<span id="cb9-339"><a href="#cb9-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`kernel_size`</span>: Filter dimensions (3×3, 5×5)</span>
<span id="cb9-340"><a href="#cb9-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`stride`</span>: Step size for filter movement (usually 1)</span>
<span id="cb9-341"><a href="#cb9-341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`padding`</span>: Border handling ('same' or 'valid')</span>
<span id="cb9-342"><a href="#cb9-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-343"><a href="#cb9-343" aria-hidden="true" tabindex="-1"></a>**2. Pooling Layer**</span>
<span id="cb9-344"><a href="#cb9-344" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduces spatial dimensions (downsampling)</span>
<span id="cb9-345"><a href="#cb9-345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provides translation invariance</span>
<span id="cb9-346"><a href="#cb9-346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Most common: Max pooling</span>
<span id="cb9-347"><a href="#cb9-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-348"><a href="#cb9-348" aria-hidden="true" tabindex="-1"></a>**Max Pooling (2×2):**</span>
<span id="cb9-349"><a href="#cb9-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-350"><a href="#cb9-350" aria-hidden="true" tabindex="-1"></a><span class="in">Input (4×4)                Output (2×2)</span></span>
<span id="cb9-351"><a href="#cb9-351" aria-hidden="true" tabindex="-1"></a><span class="in">┌────────────┐            ┌──────┐</span></span>
<span id="cb9-352"><a href="#cb9-352" aria-hidden="true" tabindex="-1"></a><span class="in">│ 1  3  2  4 │            │ 3  4 │</span></span>
<span id="cb9-353"><a href="#cb9-353" aria-hidden="true" tabindex="-1"></a><span class="in">│ 2  3  1  2 │    →       │ 7  9 │</span></span>
<span id="cb9-354"><a href="#cb9-354" aria-hidden="true" tabindex="-1"></a><span class="in">│ 5  7  8  9 │            └──────┘</span></span>
<span id="cb9-355"><a href="#cb9-355" aria-hidden="true" tabindex="-1"></a><span class="in">│ 1  2  3  4 │</span></span>
<span id="cb9-356"><a href="#cb9-356" aria-hidden="true" tabindex="-1"></a><span class="in">└────────────┘</span></span>
<span id="cb9-357"><a href="#cb9-357" aria-hidden="true" tabindex="-1"></a><span class="in">Takes maximum in each 2×2 window</span></span>
<span id="cb9-358"><a href="#cb9-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-359"><a href="#cb9-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-360"><a href="#cb9-360" aria-hidden="true" tabindex="-1"></a>**3. Fully Connected Layer**</span>
<span id="cb9-361"><a href="#cb9-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Traditional neural network layer</span>
<span id="cb9-362"><a href="#cb9-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Connects all features to output classes</span>
<span id="cb9-363"><a href="#cb9-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Usually at end of network</span>
<span id="cb9-364"><a href="#cb9-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-365"><a href="#cb9-365" aria-hidden="true" tabindex="-1"></a>**4. Dropout Layer**</span>
<span id="cb9-366"><a href="#cb9-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Randomly deactivates neurons during training</span>
<span id="cb9-367"><a href="#cb9-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prevents overfitting</span>
<span id="cb9-368"><a href="#cb9-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Common rate: 0.3-0.5</span>
<span id="cb9-369"><a href="#cb9-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-370"><a href="#cb9-370" aria-hidden="true" tabindex="-1"></a><span class="fu">#### C.4: Classic CNN Architectures</span></span>
<span id="cb9-371"><a href="#cb9-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-372"><a href="#cb9-372" aria-hidden="true" tabindex="-1"></a>Understanding architectures used in EO applications.</span>
<span id="cb9-373"><a href="#cb9-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-374"><a href="#cb9-374" aria-hidden="true" tabindex="-1"></a>::: {.feature-grid}</span>
<span id="cb9-375"><a href="#cb9-375" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-376"><a href="#cb9-376" aria-hidden="true" tabindex="-1"></a>**LeNet-5 (1998)**</span>
<span id="cb9-377"><a href="#cb9-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-378"><a href="#cb9-378" aria-hidden="true" tabindex="-1"></a>**Structure:**</span>
<span id="cb9-379"><a href="#cb9-379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conv → Pool → Conv → Pool → FC</span>
<span id="cb9-380"><a href="#cb9-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>60K parameters</span>
<span id="cb9-381"><a href="#cb9-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Original: Handwritten digits</span>
<span id="cb9-382"><a href="#cb9-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-383"><a href="#cb9-383" aria-hidden="true" tabindex="-1"></a>**EO Use:**</span>
<span id="cb9-384"><a href="#cb9-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simple scene classification</span>
<span id="cb9-385"><a href="#cb9-385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Educational examples</span>
<span id="cb9-386"><a href="#cb9-386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Quick prototypes</span>
<span id="cb9-387"><a href="#cb9-387" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-388"><a href="#cb9-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-389"><a href="#cb9-389" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-390"><a href="#cb9-390" aria-hidden="true" tabindex="-1"></a>**VGG-16 (2014)**</span>
<span id="cb9-391"><a href="#cb9-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-392"><a href="#cb9-392" aria-hidden="true" tabindex="-1"></a>**Structure:**</span>
<span id="cb9-393"><a href="#cb9-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>13 Conv layers + 3 FC</span>
<span id="cb9-394"><a href="#cb9-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>138M parameters</span>
<span id="cb9-395"><a href="#cb9-395" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small 3×3 filters stacked</span>
<span id="cb9-396"><a href="#cb9-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-397"><a href="#cb9-397" aria-hidden="true" tabindex="-1"></a>**EO Use:**</span>
<span id="cb9-398"><a href="#cb9-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene classification</span>
<span id="cb9-399"><a href="#cb9-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained on ImageNet</span>
<span id="cb9-400"><a href="#cb9-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transfer learning baseline</span>
<span id="cb9-401"><a href="#cb9-401" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-402"><a href="#cb9-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-403"><a href="#cb9-403" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-404"><a href="#cb9-404" aria-hidden="true" tabindex="-1"></a>**ResNet-50 (2015)**</span>
<span id="cb9-405"><a href="#cb9-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-406"><a href="#cb9-406" aria-hidden="true" tabindex="-1"></a>**Innovation:**</span>
<span id="cb9-407"><a href="#cb9-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Skip connections (residual blocks)</span>
<span id="cb9-408"><a href="#cb9-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Solves vanishing gradient</span>
<span id="cb9-409"><a href="#cb9-409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>50 layers deep</span>
<span id="cb9-410"><a href="#cb9-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-411"><a href="#cb9-411" aria-hidden="true" tabindex="-1"></a>**EO Use:**</span>
<span id="cb9-412"><a href="#cb9-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-accuracy classification</span>
<span id="cb9-413"><a href="#cb9-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature extraction</span>
<span id="cb9-414"><a href="#cb9-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilSA scene classifier</span>
<span id="cb9-415"><a href="#cb9-415" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-416"><a href="#cb9-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-417"><a href="#cb9-417" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-418"><a href="#cb9-418" aria-hidden="true" tabindex="-1"></a>**U-Net (2015)**</span>
<span id="cb9-419"><a href="#cb9-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-420"><a href="#cb9-420" aria-hidden="true" tabindex="-1"></a>**Innovation:**</span>
<span id="cb9-421"><a href="#cb9-421" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encoder-decoder architecture</span>
<span id="cb9-422"><a href="#cb9-422" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Skip connections preserve detail</span>
<span id="cb9-423"><a href="#cb9-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Outputs same-size segmentation</span>
<span id="cb9-424"><a href="#cb9-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-425"><a href="#cb9-425" aria-hidden="true" tabindex="-1"></a>**EO Use:**</span>
<span id="cb9-426"><a href="#cb9-426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic segmentation</span>
<span id="cb9-427"><a href="#cb9-427" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood mapping</span>
<span id="cb9-428"><a href="#cb9-428" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building extraction</span>
<span id="cb9-429"><a href="#cb9-429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Session 4 focus!**</span>
<span id="cb9-430"><a href="#cb9-430" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-431"><a href="#cb9-431" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-432"><a href="#cb9-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-433"><a href="#cb9-433" aria-hidden="true" tabindex="-1"></a>**Interactive Visualization:** Explore how different architectures process Sentinel-2 imagery.</span>
<span id="cb9-434"><a href="#cb9-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-435"><a href="#cb9-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part D: CNNs for Earth Observation (25 minutes)</span></span>
<span id="cb9-436"><a href="#cb9-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-437"><a href="#cb9-437" aria-hidden="true" tabindex="-1"></a>Connecting CNN capabilities to Philippine EO operational needs.</span>
<span id="cb9-438"><a href="#cb9-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-439"><a href="#cb9-439" aria-hidden="true" tabindex="-1"></a><span class="fu">#### D.1: Scene Classification</span></span>
<span id="cb9-440"><a href="#cb9-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-441"><a href="#cb9-441" aria-hidden="true" tabindex="-1"></a>**Task:** Assign single label to entire image patch</span>
<span id="cb9-442"><a href="#cb9-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-443"><a href="#cb9-443" aria-hidden="true" tabindex="-1"></a>**Architecture:** ResNet, VGG, EfficientNet (classification head)</span>
<span id="cb9-444"><a href="#cb9-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-445"><a href="#cb9-445" aria-hidden="true" tabindex="-1"></a>**Philippine Applications:**</span>
<span id="cb9-446"><a href="#cb9-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-447"><a href="#cb9-447" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Application <span class="pp">|</span> Classes <span class="pp">|</span> Dataset <span class="pp">|</span> Stakeholder <span class="pp">|</span></span>
<span id="cb9-448"><a href="#cb9-448" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------------|---------|---------|-------------|</span></span>
<span id="cb9-449"><a href="#cb9-449" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Land Cover** <span class="pp">|</span> Forest, Urban, Agriculture, Water, Bare <span class="pp">|</span> PhilSA Sentinel-2 <span class="pp">|</span> DENR, LGUs <span class="pp">|</span></span>
<span id="cb9-450"><a href="#cb9-450" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Cloud Detection** <span class="pp">|</span> Clear, Thin Cloud, Thick Cloud, Shadow <span class="pp">|</span> Sentinel-2 Level-1C <span class="pp">|</span> PhilSA (preprocessing) <span class="pp">|</span></span>
<span id="cb9-451"><a href="#cb9-451" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Rice Field Stage** <span class="pp">|</span> Land Prep, Transplanting, Vegetative, Harvest <span class="pp">|</span> PlanetScope + field data <span class="pp">|</span> DA, PhilRice <span class="pp">|</span></span>
<span id="cb9-452"><a href="#cb9-452" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Disaster Assessment** <span class="pp">|</span> Damaged, Undamaged, Debris <span class="pp">|</span> Drones + Sentinel-2 <span class="pp">|</span> NDRRMC, PAGASA <span class="pp">|</span></span>
<span id="cb9-453"><a href="#cb9-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-454"><a href="#cb9-454" aria-hidden="true" tabindex="-1"></a>**Data Requirements:**</span>
<span id="cb9-455"><a href="#cb9-455" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Typical: 1,000-10,000 labeled image chips per class</span>
<span id="cb9-456"><a href="#cb9-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilSA strategy: Start with 500/class, augment with rotations/flips</span>
<span id="cb9-457"><a href="#cb9-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-458"><a href="#cb9-458" aria-hidden="true" tabindex="-1"></a>**Hands-On (Session 4):** Build ResNet-based classifier for Palawan land cover</span>
<span id="cb9-459"><a href="#cb9-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-460"><a href="#cb9-460" aria-hidden="true" tabindex="-1"></a><span class="fu">#### D.2: Semantic Segmentation</span></span>
<span id="cb9-461"><a href="#cb9-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-462"><a href="#cb9-462" aria-hidden="true" tabindex="-1"></a>**Task:** Classify every pixel in image (pixel-wise labels)</span>
<span id="cb9-463"><a href="#cb9-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-464"><a href="#cb9-464" aria-hidden="true" tabindex="-1"></a>**Architecture:** U-Net, DeepLabv3+, SegNet</span>
<span id="cb9-465"><a href="#cb9-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-466"><a href="#cb9-466" aria-hidden="true" tabindex="-1"></a>**Philippine Applications:**</span>
<span id="cb9-467"><a href="#cb9-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-468"><a href="#cb9-468" aria-hidden="true" tabindex="-1"></a>::: {.feature-grid}</span>
<span id="cb9-469"><a href="#cb9-469" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-470"><a href="#cb9-470" aria-hidden="true" tabindex="-1"></a>**🌊 Flood Mapping**</span>
<span id="cb9-471"><a href="#cb9-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-472"><a href="#cb9-472" aria-hidden="true" tabindex="-1"></a>**Challenge:**</span>
<span id="cb9-473"><a href="#cb9-473" aria-hidden="true" tabindex="-1"></a>Rapid post-disaster assessment</span>
<span id="cb9-474"><a href="#cb9-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-475"><a href="#cb9-475" aria-hidden="true" tabindex="-1"></a>**CNN Solution:**</span>
<span id="cb9-476"><a href="#cb9-476" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Sentinel-1 SAR (pre + post event)</span>
<span id="cb9-477"><a href="#cb9-477" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Flooded/Non-flooded pixel map</span>
<span id="cb9-478"><a href="#cb9-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Architecture: U-Net</span>
<span id="cb9-479"><a href="#cb9-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-480"><a href="#cb9-480" aria-hidden="true" tabindex="-1"></a>**PhilSA Use Case:**</span>
<span id="cb9-481"><a href="#cb9-481" aria-hidden="true" tabindex="-1"></a>Pampanga flood 2023 - 6-hour processing time (vs 2 days manual)</span>
<span id="cb9-482"><a href="#cb9-482" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-483"><a href="#cb9-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-484"><a href="#cb9-484" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-485"><a href="#cb9-485" aria-hidden="true" tabindex="-1"></a>**🏘️ Informal Settlements**</span>
<span id="cb9-486"><a href="#cb9-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-487"><a href="#cb9-487" aria-hidden="true" tabindex="-1"></a>**Challenge:**</span>
<span id="cb9-488"><a href="#cb9-488" aria-hidden="true" tabindex="-1"></a>Map slums for disaster planning</span>
<span id="cb9-489"><a href="#cb9-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-490"><a href="#cb9-490" aria-hidden="true" tabindex="-1"></a>**CNN Solution:**</span>
<span id="cb9-491"><a href="#cb9-491" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: High-res imagery (PlanetScope, drones)</span>
<span id="cb9-492"><a href="#cb9-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Building footprints</span>
<span id="cb9-493"><a href="#cb9-493" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Architecture: U-Net + post-processing</span>
<span id="cb9-494"><a href="#cb9-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-495"><a href="#cb9-495" aria-hidden="true" tabindex="-1"></a>**NEDA Application:**</span>
<span id="cb9-496"><a href="#cb9-496" aria-hidden="true" tabindex="-1"></a>Metro Manila vulnerability assessment</span>
<span id="cb9-497"><a href="#cb9-497" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-498"><a href="#cb9-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-499"><a href="#cb9-499" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-500"><a href="#cb9-500" aria-hidden="true" tabindex="-1"></a>**🌳 Forest Degradation**</span>
<span id="cb9-501"><a href="#cb9-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-502"><a href="#cb9-502" aria-hidden="true" tabindex="-1"></a>**Challenge:**</span>
<span id="cb9-503"><a href="#cb9-503" aria-hidden="true" tabindex="-1"></a>Detect selective logging</span>
<span id="cb9-504"><a href="#cb9-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-505"><a href="#cb9-505" aria-hidden="true" tabindex="-1"></a>**CNN Solution:**</span>
<span id="cb9-506"><a href="#cb9-506" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Multi-temporal Sentinel-2</span>
<span id="cb9-507"><a href="#cb9-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Degraded forest pixels</span>
<span id="cb9-508"><a href="#cb9-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Architecture: U-Net + LSTM</span>
<span id="cb9-509"><a href="#cb9-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-510"><a href="#cb9-510" aria-hidden="true" tabindex="-1"></a>**DENR Use:**</span>
<span id="cb9-511"><a href="#cb9-511" aria-hidden="true" tabindex="-1"></a>Protected area monitoring</span>
<span id="cb9-512"><a href="#cb9-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-513"><a href="#cb9-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-514"><a href="#cb9-514" aria-hidden="true" tabindex="-1"></a>::: {.feature-card}</span>
<span id="cb9-515"><a href="#cb9-515" aria-hidden="true" tabindex="-1"></a>**⛏️ Mining Activity**</span>
<span id="cb9-516"><a href="#cb9-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-517"><a href="#cb9-517" aria-hidden="true" tabindex="-1"></a>**Challenge:**</span>
<span id="cb9-518"><a href="#cb9-518" aria-hidden="true" tabindex="-1"></a>Illegal mining detection</span>
<span id="cb9-519"><a href="#cb9-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-520"><a href="#cb9-520" aria-hidden="true" tabindex="-1"></a>**CNN Solution:**</span>
<span id="cb9-521"><a href="#cb9-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Sentinel-2 + Sentinel-1</span>
<span id="cb9-522"><a href="#cb9-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Mining site polygons</span>
<span id="cb9-523"><a href="#cb9-523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Post-process: Vectorize</span>
<span id="cb9-524"><a href="#cb9-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-525"><a href="#cb9-525" aria-hidden="true" tabindex="-1"></a>**MGB Application:**</span>
<span id="cb9-526"><a href="#cb9-526" aria-hidden="true" tabindex="-1"></a>Permit compliance checking</span>
<span id="cb9-527"><a href="#cb9-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-528"><a href="#cb9-528" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-529"><a href="#cb9-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-530"><a href="#cb9-530" aria-hidden="true" tabindex="-1"></a>**Data Requirements:**</span>
<span id="cb9-531"><a href="#cb9-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Annotation intensive: Need pixel-level labels</span>
<span id="cb9-532"><a href="#cb9-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Typical: 100-500 labeled images (256×256 chips)</span>
<span id="cb9-533"><a href="#cb9-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tools: QGIS, LabelMe, CVAT</span>
<span id="cb9-534"><a href="#cb9-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-535"><a href="#cb9-535" aria-hidden="true" tabindex="-1"></a>**Hands-On (Session 4):** Implement U-Net for flood mapping in Central Luzon</span>
<span id="cb9-536"><a href="#cb9-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-537"><a href="#cb9-537" aria-hidden="true" tabindex="-1"></a><span class="fu">#### D.3: Object Detection</span></span>
<span id="cb9-538"><a href="#cb9-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-539"><a href="#cb9-539" aria-hidden="true" tabindex="-1"></a>**Task:** Find and localize objects with bounding boxes</span>
<span id="cb9-540"><a href="#cb9-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-541"><a href="#cb9-541" aria-hidden="true" tabindex="-1"></a>**Architecture:** Faster R-CNN, YOLO, RetinaNet</span>
<span id="cb9-542"><a href="#cb9-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-543"><a href="#cb9-543" aria-hidden="true" tabindex="-1"></a>**Philippine Applications:**</span>
<span id="cb9-544"><a href="#cb9-544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ship Detection:** Illegal fishing monitoring (Sentinel-1)</span>
<span id="cb9-545"><a href="#cb9-545" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Building Detection:** Infrastructure mapping (high-res)</span>
<span id="cb9-546"><a href="#cb9-546" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tree Counting:** Forest inventory (drone imagery)</span>
<span id="cb9-547"><a href="#cb9-547" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vehicle Detection:** Traffic monitoring (PlanetScope)</span>
<span id="cb9-548"><a href="#cb9-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-549"><a href="#cb9-549" aria-hidden="true" tabindex="-1"></a>**Data Format:** Bounding boxes + class labels (COCO, PASCAL VOC formats)</span>
<span id="cb9-550"><a href="#cb9-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-551"><a href="#cb9-551" aria-hidden="true" tabindex="-1"></a>**Computational Note:** More complex than classification, requires anchor boxes and region proposals</span>
<span id="cb9-552"><a href="#cb9-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-553"><a href="#cb9-553" aria-hidden="true" tabindex="-1"></a><span class="fu">#### D.4: Change Detection</span></span>
<span id="cb9-554"><a href="#cb9-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-555"><a href="#cb9-555" aria-hidden="true" tabindex="-1"></a>**Task:** Identify what changed between two time points</span>
<span id="cb9-556"><a href="#cb9-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-557"><a href="#cb9-557" aria-hidden="true" tabindex="-1"></a>**CNN Approaches:**</span>
<span id="cb9-558"><a href="#cb9-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-559"><a href="#cb9-559" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Siamese Networks:** Compare two images with shared weights</span>
<span id="cb9-560"><a href="#cb9-560" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Early Fusion:** Stack temporal images as input channels</span>
<span id="cb9-561"><a href="#cb9-561" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Late Fusion:** Separate encoders + change decoder</span>
<span id="cb9-562"><a href="#cb9-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-563"><a href="#cb9-563" aria-hidden="true" tabindex="-1"></a>**Philippine Applications:**</span>
<span id="cb9-564"><a href="#cb9-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deforestation (Palawan, Mindanao)</span>
<span id="cb9-565"><a href="#cb9-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Urban expansion (Metro Manila, Cebu)</span>
<span id="cb9-566"><a href="#cb9-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Post-disaster damage (typhoon impacts)</span>
<span id="cb9-567"><a href="#cb9-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Agricultural change (conversion detection)</span>
<span id="cb9-568"><a href="#cb9-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-569"><a href="#cb9-569" aria-hidden="true" tabindex="-1"></a>**Challenge:** Need paired labeled change data (before + after + change mask)</span>
<span id="cb9-570"><a href="#cb9-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-571"><a href="#cb9-571" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part E: Practical Considerations for EO Deep Learning (15 minutes)</span></span>
<span id="cb9-572"><a href="#cb9-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-573"><a href="#cb9-573" aria-hidden="true" tabindex="-1"></a>Real-world challenges and solutions for Philippine EO practitioners.</span>
<span id="cb9-574"><a href="#cb9-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-575"><a href="#cb9-575" aria-hidden="true" tabindex="-1"></a><span class="fu">#### E.1: The Data Challenge</span></span>
<span id="cb9-576"><a href="#cb9-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-577"><a href="#cb9-577" aria-hidden="true" tabindex="-1"></a>**How Much Data Do You Need?**</span>
<span id="cb9-578"><a href="#cb9-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-579"><a href="#cb9-579" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Model Complexity <span class="pp">|</span> Typical Requirement <span class="pp">|</span> Philippine Reality <span class="pp">|</span></span>
<span id="cb9-580"><a href="#cb9-580" aria-hidden="true" tabindex="-1"></a><span class="pp">|------------------|---------------------|-------------------|</span></span>
<span id="cb9-581"><a href="#cb9-581" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Simple CNN (5 layers) <span class="pp">|</span> 5,000-10,000 samples <span class="pp">|</span> ✓ Achievable <span class="pp">|</span></span>
<span id="cb9-582"><a href="#cb9-582" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ResNet-50 (from scratch) <span class="pp">|</span> 100,000+ samples <span class="pp">|</span> ✗ Rarely available <span class="pp">|</span></span>
<span id="cb9-583"><a href="#cb9-583" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ResNet-50 (fine-tuned) <span class="pp">|</span> 1,000-5,000 samples <span class="pp">|</span> ✓ Achievable with augmentation <span class="pp">|</span></span>
<span id="cb9-584"><a href="#cb9-584" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> U-Net (segmentation) <span class="pp">|</span> 100-500 images <span class="pp">|</span> ✓ Achievable but labor-intensive <span class="pp">|</span></span>
<span id="cb9-585"><a href="#cb9-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-586"><a href="#cb9-586" aria-hidden="true" tabindex="-1"></a>**Data-Centric AI Principles:**</span>
<span id="cb9-587"><a href="#cb9-587" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Quality &gt; Quantity:** 500 clean labels &gt;&gt; 5,000 noisy labels</span>
<span id="cb9-588"><a href="#cb9-588" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Representative Sampling:** Cover all Philippine ecosystems (lowland, upland, coastal)</span>
<span id="cb9-589"><a href="#cb9-589" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Class Balance:** Equal samples per class (or weighted loss)</span>
<span id="cb9-590"><a href="#cb9-590" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Validation Split:** Hold out 20% for unbiased evaluation</span>
<span id="cb9-591"><a href="#cb9-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-592"><a href="#cb9-592" aria-hidden="true" tabindex="-1"></a>**Philippine Data Sources:**</span>
<span id="cb9-593"><a href="#cb9-593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilSA Space+ Data Dashboard (satellite imagery)</span>
<span id="cb9-594"><a href="#cb9-594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>NAMRIA Geoportal (reference maps)</span>
<span id="cb9-595"><a href="#cb9-595" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DOST-ASTI DATOS (disaster imagery)</span>
<span id="cb9-596"><a href="#cb9-596" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LiDAR Portal (elevation + canopy)</span>
<span id="cb9-597"><a href="#cb9-597" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Field campaigns (GPS + photos)</span>
<span id="cb9-598"><a href="#cb9-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-599"><a href="#cb9-599" aria-hidden="true" tabindex="-1"></a><span class="fu">#### E.2: Transfer Learning - Training on Limited Data</span></span>
<span id="cb9-600"><a href="#cb9-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-601"><a href="#cb9-601" aria-hidden="true" tabindex="-1"></a>**Strategy:** Start with model pre-trained on large dataset (ImageNet), fine-tune on Philippine data</span>
<span id="cb9-602"><a href="#cb9-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-603"><a href="#cb9-603" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb9-604"><a href="#cb9-604" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Need 10× less data</span>
<span id="cb9-605"><a href="#cb9-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train 5× faster</span>
<span id="cb9-606"><a href="#cb9-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better accuracy with small datasets</span>
<span id="cb9-607"><a href="#cb9-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-608"><a href="#cb9-608" aria-hidden="true" tabindex="-1"></a>**Implementation:**</span>
<span id="cb9-609"><a href="#cb9-609" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb9-610"><a href="#cb9-610" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained ResNet</span></span>
<span id="cb9-611"><a href="#cb9-611" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> ResNet50(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-612"><a href="#cb9-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-613"><a href="#cb9-613" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze early layers (keep learned features)</span></span>
<span id="cb9-614"><a href="#cb9-614" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[:<span class="dv">100</span>]:</span>
<span id="cb9-615"><a href="#cb9-615" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-616"><a href="#cb9-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-617"><a href="#cb9-617" aria-hidden="true" tabindex="-1"></a><span class="co"># Add custom classification head</span></span>
<span id="cb9-618"><a href="#cb9-618" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(base_model.output)</span>
<span id="cb9-619"><a href="#cb9-619" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb9-620"><a href="#cb9-620" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> Dense(<span class="dv">8</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x)  <span class="co"># 8 Palawan classes</span></span>
<span id="cb9-621"><a href="#cb9-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-622"><a href="#cb9-622" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>, outputs<span class="op">=</span>output)</span>
<span id="cb9-623"><a href="#cb9-623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-624"><a href="#cb9-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-625"><a href="#cb9-625" aria-hidden="true" tabindex="-1"></a>**When to Use:**</span>
<span id="cb9-626"><a href="#cb9-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limited training data (&lt;5,000 samples)</span>
<span id="cb9-627"><a href="#cb9-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Similar task to pre-training (natural images)</span>
<span id="cb9-628"><a href="#cb9-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Need quick results</span>
<span id="cb9-629"><a href="#cb9-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-630"><a href="#cb9-630" aria-hidden="true" tabindex="-1"></a>**Caution:** ImageNet has RGB images. Sentinel-2 has 10+ bands. Adaptation strategies needed (Session 4).</span>
<span id="cb9-631"><a href="#cb9-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-632"><a href="#cb9-632" aria-hidden="true" tabindex="-1"></a><span class="fu">#### E.3: Data Augmentation - Artificially Expanding Training Data</span></span>
<span id="cb9-633"><a href="#cb9-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-634"><a href="#cb9-634" aria-hidden="true" tabindex="-1"></a>**Geometric Transformations:**</span>
<span id="cb9-635"><a href="#cb9-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation:** 90°, 180°, 270° (satellites view from any angle)</span>
<span id="cb9-636"><a href="#cb9-636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Horizontal/Vertical Flip:** Valid for overhead imagery</span>
<span id="cb9-637"><a href="#cb9-637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zoom/Scale:** Simulate different resolutions</span>
<span id="cb9-638"><a href="#cb9-638" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Translation:** Small shifts</span>
<span id="cb9-639"><a href="#cb9-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-640"><a href="#cb9-640" aria-hidden="true" tabindex="-1"></a>**Spectral Augmentations:**</span>
<span id="cb9-641"><a href="#cb9-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Brightness/Contrast:** Simulate atmospheric conditions</span>
<span id="cb9-642"><a href="#cb9-642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gaussian Noise:** Simulate sensor noise</span>
<span id="cb9-643"><a href="#cb9-643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Band Dropout:** Improve robustness</span>
<span id="cb9-644"><a href="#cb9-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-645"><a href="#cb9-645" aria-hidden="true" tabindex="-1"></a>**Philippine Context:**</span>
<span id="cb9-646"><a href="#cb9-646" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Use rotation/flips for land cover (no preferential orientation)</span>
<span id="cb9-647"><a href="#cb9-647" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Avoid rotation for infrastructure (roads have direction)</span>
<span id="cb9-648"><a href="#cb9-648" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Augment brightness for cloud variations</span>
<span id="cb9-649"><a href="#cb9-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-650"><a href="#cb9-650" aria-hidden="true" tabindex="-1"></a>**Implementation (Session 4):**</span>
<span id="cb9-651"><a href="#cb9-651" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb9-652"><a href="#cb9-652" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb9-653"><a href="#cb9-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-654"><a href="#cb9-654" aria-hidden="true" tabindex="-1"></a>augmentation <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb9-655"><a href="#cb9-655" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">90</span>,</span>
<span id="cb9-656"><a href="#cb9-656" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-657"><a href="#cb9-657" aria-hidden="true" tabindex="-1"></a>    vertical_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-658"><a href="#cb9-658" aria-hidden="true" tabindex="-1"></a>    brightness_range<span class="op">=</span>[<span class="fl">0.8</span>, <span class="fl">1.2</span>],</span>
<span id="cb9-659"><a href="#cb9-659" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb9-660"><a href="#cb9-660" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-661"><a href="#cb9-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-662"><a href="#cb9-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-663"><a href="#cb9-663" aria-hidden="true" tabindex="-1"></a><span class="fu">#### E.4: Computational Requirements</span></span>
<span id="cb9-664"><a href="#cb9-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-665"><a href="#cb9-665" aria-hidden="true" tabindex="-1"></a>**Training Resources:**</span>
<span id="cb9-666"><a href="#cb9-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-667"><a href="#cb9-667" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Model <span class="pp">|</span> Training Time* <span class="pp">|</span> GPU Memory <span class="pp">|</span> Cost (Colab Pro) <span class="pp">|</span></span>
<span id="cb9-668"><a href="#cb9-668" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------|----------------|------------|------------------|</span></span>
<span id="cb9-669"><a href="#cb9-669" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Simple CNN <span class="pp">|</span> 30 min <span class="pp">|</span> 4 GB <span class="pp">|</span> Free tier OK <span class="pp">|</span></span>
<span id="cb9-670"><a href="#cb9-670" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ResNet-50 (fine-tune) <span class="pp">|</span> 2-4 hours <span class="pp">|</span> 8 GB <span class="pp">|</span> Free tier OK <span class="pp">|</span></span>
<span id="cb9-671"><a href="#cb9-671" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> U-Net (segmentation) <span class="pp">|</span> 4-8 hours <span class="pp">|</span> 12 GB <span class="pp">|</span> Pro needed <span class="pp">|</span></span>
<span id="cb9-672"><a href="#cb9-672" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ResNet-50 (from scratch) <span class="pp">|</span> 24+ hours <span class="pp">|</span> 16 GB <span class="pp">|</span> Pro+ needed <span class="pp">|</span></span>
<span id="cb9-673"><a href="#cb9-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-674"><a href="#cb9-674" aria-hidden="true" tabindex="-1"></a>*1,000 training images, 50 epochs, V100 GPU</span>
<span id="cb9-675"><a href="#cb9-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-676"><a href="#cb9-676" aria-hidden="true" tabindex="-1"></a>**Philippine Context:**</span>
<span id="cb9-677"><a href="#cb9-677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PhilSA uses on-premise GPU servers (8× NVIDIA A100)</span>
<span id="cb9-678"><a href="#cb9-678" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Universities: Limited GPU access (submit jobs)</span>
<span id="cb9-679"><a href="#cb9-679" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Practitioners: Google Colab Pro ($10/month) recommended</span>
<span id="cb9-680"><a href="#cb9-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-681"><a href="#cb9-681" aria-hidden="true" tabindex="-1"></a>**Optimization Strategies (Session 4):**</span>
<span id="cb9-682"><a href="#cb9-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use mixed precision training (FP16)</span>
<span id="cb9-683"><a href="#cb9-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduce batch size if memory limited</span>
<span id="cb9-684"><a href="#cb9-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train on smaller image chips (128×128 instead of 256×256)</span>
<span id="cb9-685"><a href="#cb9-685" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use gradient checkpointing for large models</span>
<span id="cb9-686"><a href="#cb9-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-687"><a href="#cb9-687" aria-hidden="true" tabindex="-1"></a><span class="fu">#### E.5: Model Interpretability - Understanding CNN Decisions</span></span>
<span id="cb9-688"><a href="#cb9-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-689"><a href="#cb9-689" aria-hidden="true" tabindex="-1"></a>**Why It Matters:**</span>
<span id="cb9-690"><a href="#cb9-690" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DENR reports need explanations ("Why was this classified as deforested?")</span>
<span id="cb9-691"><a href="#cb9-691" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Debugging poor performance</span>
<span id="cb9-692"><a href="#cb9-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building stakeholder trust</span>
<span id="cb9-693"><a href="#cb9-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-694"><a href="#cb9-694" aria-hidden="true" tabindex="-1"></a>**Techniques (Session 4):**</span>
<span id="cb9-695"><a href="#cb9-695" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Activation Visualization:** See what filters learned</span>
<span id="cb9-696"><a href="#cb9-696" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Saliency Maps:** Which pixels influenced decision?</span>
<span id="cb9-697"><a href="#cb9-697" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Class Activation Maps (CAM):** Highlight relevant regions</span>
<span id="cb9-698"><a href="#cb9-698" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Filter Visualization:** What patterns do filters detect?</span>
<span id="cb9-699"><a href="#cb9-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-700"><a href="#cb9-700" aria-hidden="true" tabindex="-1"></a>**Philippine Application Example:**</span>
<span id="cb9-701"><a href="#cb9-701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Question:** Why did model classify mangroves as agriculture?</span>
<span id="cb9-702"><a href="#cb9-702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CAM Analysis:** Model focused on water proximity, not canopy structure</span>
<span id="cb9-703"><a href="#cb9-703" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Solution:** Add texture features or more mangrove training samples</span>
<span id="cb9-704"><a href="#cb9-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-705"><a href="#cb9-705" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-706"><a href="#cb9-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-707"><a href="#cb9-707" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Concepts</span></span>
<span id="cb9-708"><a href="#cb9-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-709"><a href="#cb9-709" aria-hidden="true" tabindex="-1"></a><span class="fu">### Automatic Feature Learning</span></span>
<span id="cb9-710"><a href="#cb9-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-711"><a href="#cb9-711" aria-hidden="true" tabindex="-1"></a>**What is it?**</span>
<span id="cb9-712"><a href="#cb9-712" aria-hidden="true" tabindex="-1"></a>CNNs learn optimal features directly from raw pixel data, eliminating manual feature engineering.</span>
<span id="cb9-713"><a href="#cb9-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-714"><a href="#cb9-714" aria-hidden="true" tabindex="-1"></a>**How it works:**</span>
<span id="cb9-715"><a href="#cb9-715" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 1:** Learns edges (horizontal, vertical, diagonal) - like Sobel filters you created manually</span>
<span id="cb9-716"><a href="#cb9-716" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 2:** Combines edges into textures and simple shapes</span>
<span id="cb9-717"><a href="#cb9-717" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 3:** Combines shapes into complex patterns (canopy structure, urban grid)</span>
<span id="cb9-718"><a href="#cb9-718" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 4+:** High-level semantic features (forest type, settlement pattern)</span>
<span id="cb9-719"><a href="#cb9-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-720"><a href="#cb9-720" aria-hidden="true" tabindex="-1"></a>**Comparison to Session 2:**</span>
<span id="cb9-721"><a href="#cb9-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-722"><a href="#cb9-722" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> Random Forest (Session 2) <span class="pp">|</span> CNN (Session 3-4) <span class="pp">|</span></span>
<span id="cb9-723"><a href="#cb9-723" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|---------------------------|-------------------|</span></span>
<span id="cb9-724"><a href="#cb9-724" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Features** <span class="pp">|</span> Manual (NDVI, GLCM) <span class="pp">|</span> Learned automatically <span class="pp">|</span></span>
<span id="cb9-725"><a href="#cb9-725" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Spatial Context** <span class="pp">|</span> Limited (within feature) <span class="pp">|</span> Fully exploited (receptive field) <span class="pp">|</span></span>
<span id="cb9-726"><a href="#cb9-726" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Data Needed** <span class="pp">|</span> 500-1,000 samples <span class="pp">|</span> 5,000-10,000 samples <span class="pp">|</span></span>
<span id="cb9-727"><a href="#cb9-727" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Training Time** <span class="pp">|</span> Minutes <span class="pp">|</span> Hours <span class="pp">|</span></span>
<span id="cb9-728"><a href="#cb9-728" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Accuracy** <span class="pp">|</span> 80-85% typical <span class="pp">|</span> 90-95% possible <span class="pp">|</span></span>
<span id="cb9-729"><a href="#cb9-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-730"><a href="#cb9-730" aria-hidden="true" tabindex="-1"></a><span class="fu">### Receptive Field</span></span>
<span id="cb9-731"><a href="#cb9-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-732"><a href="#cb9-732" aria-hidden="true" tabindex="-1"></a>**Definition:** The region of input image that influences a particular neuron's activation.</span>
<span id="cb9-733"><a href="#cb9-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-734"><a href="#cb9-734" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb9-735"><a href="#cb9-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A neuron in Layer 1 sees a 3×3 pixel region (30m × 30m for Sentinel-2)</span>
<span id="cb9-736"><a href="#cb9-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A neuron in Layer 3 sees a 15×15 pixel region (150m × 150m)</span>
<span id="cb9-737"><a href="#cb9-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output neuron "sees" entire image chip</span>
<span id="cb9-738"><a href="#cb9-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-739"><a href="#cb9-739" aria-hidden="true" tabindex="-1"></a>**Why it matters:**</span>
<span id="cb9-740"><a href="#cb9-740" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small objects (buildings): Need fewer layers</span>
<span id="cb9-741"><a href="#cb9-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large objects (agricultural fields): Need deeper networks</span>
<span id="cb9-742"><a href="#cb9-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Contextual classification: Large receptive field captures neighborhood</span>
<span id="cb9-743"><a href="#cb9-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-744"><a href="#cb9-744" aria-hidden="true" tabindex="-1"></a>**Philippine Example:**</span>
<span id="cb9-745"><a href="#cb9-745" aria-hidden="true" tabindex="-1"></a>Classifying a pixel as "mangrove" requires seeing water proximity (large receptive field) AND canopy texture (small receptive field). Multi-scale processing essential!</span>
<span id="cb9-746"><a href="#cb9-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-747"><a href="#cb9-747" aria-hidden="true" tabindex="-1"></a><span class="fu">### Translation Invariance</span></span>
<span id="cb9-748"><a href="#cb9-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-749"><a href="#cb9-749" aria-hidden="true" tabindex="-1"></a>**What is it?**</span>
<span id="cb9-750"><a href="#cb9-750" aria-hidden="true" tabindex="-1"></a>CNN can recognize patterns regardless of position in image.</span>
<span id="cb9-751"><a href="#cb9-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-752"><a href="#cb9-752" aria-hidden="true" tabindex="-1"></a>**How achieved:**</span>
<span id="cb9-753"><a href="#cb9-753" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Parameter sharing:** Same filter applied everywhere</span>
<span id="cb9-754"><a href="#cb9-754" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Pooling:** Abstracts exact position</span>
<span id="cb9-755"><a href="#cb9-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-756"><a href="#cb9-756" aria-hidden="true" tabindex="-1"></a>**EO Benefit:**</span>
<span id="cb9-757"><a href="#cb9-757" aria-hidden="true" tabindex="-1"></a>Forest is forest whether in top-left or bottom-right of image. Train once, apply anywhere in Philippines!</span>
<span id="cb9-758"><a href="#cb9-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-759"><a href="#cb9-759" aria-hidden="true" tabindex="-1"></a>**Contrast with Position:**</span>
<span id="cb9-760"><a href="#cb9-760" aria-hidden="true" tabindex="-1"></a>For some tasks, position DOES matter (e.g., urban always near coasts in Philippines). Advanced architectures can encode position.</span>
<span id="cb9-761"><a href="#cb9-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-762"><a href="#cb9-762" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent and Backpropagation</span></span>
<span id="cb9-763"><a href="#cb9-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-764"><a href="#cb9-764" aria-hidden="true" tabindex="-1"></a>**Intuitive Explanation:**</span>
<span id="cb9-765"><a href="#cb9-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-766"><a href="#cb9-766" aria-hidden="true" tabindex="-1"></a>Imagine hiking down a foggy mountain (error surface) to reach valley (minimum loss):</span>
<span id="cb9-767"><a href="#cb9-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gradient:** Direction of steepest descent</span>
<span id="cb9-768"><a href="#cb9-768" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learning rate:** Step size</span>
<span id="cb9-769"><a href="#cb9-769" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Backpropagation:** Efficiently calculates gradients for all weights</span>
<span id="cb9-770"><a href="#cb9-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-771"><a href="#cb9-771" aria-hidden="true" tabindex="-1"></a>**Training Process:**</span>
<span id="cb9-772"><a href="#cb9-772" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Forward pass: Image → Predictions</span>
<span id="cb9-773"><a href="#cb9-773" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate loss: How wrong were predictions?</span>
<span id="cb9-774"><a href="#cb9-774" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Backward pass: Compute ∂Loss/∂Weight for every parameter</span>
<span id="cb9-775"><a href="#cb9-775" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Update weights: Move "downhill" toward better performance</span>
<span id="cb9-776"><a href="#cb9-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-777"><a href="#cb9-777" aria-hidden="true" tabindex="-1"></a>**Common Issues:**</span>
<span id="cb9-778"><a href="#cb9-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learning rate too high:** Jump over minimum (unstable)</span>
<span id="cb9-779"><a href="#cb9-779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learning rate too low:** Painfully slow convergence</span>
<span id="cb9-780"><a href="#cb9-780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Local minima:** Stuck in suboptimal solution (less common with large networks)</span>
<span id="cb9-781"><a href="#cb9-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-782"><a href="#cb9-782" aria-hidden="true" tabindex="-1"></a>**Session 4:** Implement Adam optimizer (adaptive learning rates)</span>
<span id="cb9-783"><a href="#cb9-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-784"><a href="#cb9-784" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-785"><a href="#cb9-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-786"><a href="#cb9-786" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interactive Demonstrations</span></span>
<span id="cb9-787"><a href="#cb9-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-788"><a href="#cb9-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 1: Perceptron Playground</span></span>
<span id="cb9-789"><a href="#cb9-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-790"><a href="#cb9-790" aria-hidden="true" tabindex="-1"></a>**Objective:** Build intuition for how perceptrons learn decision boundaries</span>
<span id="cb9-791"><a href="#cb9-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-792"><a href="#cb9-792" aria-hidden="true" tabindex="-1"></a>**Activity:**</span>
<span id="cb9-793"><a href="#cb9-793" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load 2D dataset (NDVI vs NDWI for water classification)</span>
<span id="cb9-794"><a href="#cb9-794" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Initialize random weights</span>
<span id="cb9-795"><a href="#cb9-795" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Visualize decision boundary</span>
<span id="cb9-796"><a href="#cb9-796" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Update weights iteratively</span>
<span id="cb9-797"><a href="#cb9-797" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Watch boundary align with data</span>
<span id="cb9-798"><a href="#cb9-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-799"><a href="#cb9-799" aria-hidden="true" tabindex="-1"></a>**Notebook:** <span class="in">`session3_theory_STUDENT.ipynb`</span> (Part 1)</span>
<span id="cb9-800"><a href="#cb9-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-801"><a href="#cb9-801" aria-hidden="true" tabindex="-1"></a>**Expected Outcome:** Understand that neural networks find separating hyperplanes through gradient descent.</span>
<span id="cb9-802"><a href="#cb9-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-803"><a href="#cb9-803" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 2: Activation Function Gallery</span></span>
<span id="cb9-804"><a href="#cb9-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-805"><a href="#cb9-805" aria-hidden="true" tabindex="-1"></a>**Objective:** Visualize how different activation functions transform data</span>
<span id="cb9-806"><a href="#cb9-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-807"><a href="#cb9-807" aria-hidden="true" tabindex="-1"></a>**Activity:**</span>
<span id="cb9-808"><a href="#cb9-808" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Plot sigmoid, ReLU, tanh, Leaky ReLU</span>
<span id="cb9-809"><a href="#cb9-809" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Apply to Sentinel-2 reflectance values</span>
<span id="cb9-810"><a href="#cb9-810" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compare output distributions</span>
<span id="cb9-811"><a href="#cb9-811" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>See why ReLU is most popular</span>
<span id="cb9-812"><a href="#cb9-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-813"><a href="#cb9-813" aria-hidden="true" tabindex="-1"></a>**Notebook:** <span class="in">`session3_theory_STUDENT.ipynb`</span> (Part 2)</span>
<span id="cb9-814"><a href="#cb9-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-815"><a href="#cb9-815" aria-hidden="true" tabindex="-1"></a>**Key Insight:** ReLU is simple, fast, and sparse (many zeros = efficient).</span>
<span id="cb9-816"><a href="#cb9-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-817"><a href="#cb9-817" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 3: Manual Convolution on Sentinel-2</span></span>
<span id="cb9-818"><a href="#cb9-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-819"><a href="#cb9-819" aria-hidden="true" tabindex="-1"></a>**Objective:** Understand convolution as a sliding filter operation</span>
<span id="cb9-820"><a href="#cb9-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-821"><a href="#cb9-821" aria-hidden="true" tabindex="-1"></a>**Activity:**</span>
<span id="cb9-822"><a href="#cb9-822" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load Sentinel-2 NIR band (Palawan forest patch)</span>
<span id="cb9-823"><a href="#cb9-823" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Define edge detection filter (3×3 Sobel)</span>
<span id="cb9-824"><a href="#cb9-824" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Manually compute convolution (NumPy)</span>
<span id="cb9-825"><a href="#cb9-825" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Visualize feature map</span>
<span id="cb9-826"><a href="#cb9-826" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Try different filters (blur, sharpen, Gaussian)</span>
<span id="cb9-827"><a href="#cb9-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-828"><a href="#cb9-828" aria-hidden="true" tabindex="-1"></a>**Notebook:** <span class="in">`session3_cnn_operations_STUDENT.ipynb`</span> (Part 1)</span>
<span id="cb9-829"><a href="#cb9-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-830"><a href="#cb9-830" aria-hidden="true" tabindex="-1"></a>**Aha Moment:** "Edge detection filter highlights forest boundaries—exactly what CNN learns automatically!"</span>
<span id="cb9-831"><a href="#cb9-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-832"><a href="#cb9-832" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 4: Pooling Demonstration</span></span>
<span id="cb9-833"><a href="#cb9-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-834"><a href="#cb9-834" aria-hidden="true" tabindex="-1"></a>**Objective:** Understand downsampling and translation invariance</span>
<span id="cb9-835"><a href="#cb9-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-836"><a href="#cb9-836" aria-hidden="true" tabindex="-1"></a>**Activity:**</span>
<span id="cb9-837"><a href="#cb9-837" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load Sentinel-2 image chip (256×256)</span>
<span id="cb9-838"><a href="#cb9-838" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Apply max pooling (2×2, stride 2)</span>
<span id="cb9-839"><a href="#cb9-839" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compare original vs pooled (128×128)</span>
<span id="cb9-840"><a href="#cb9-840" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Shift image by 1 pixel, repeat</span>
<span id="cb9-841"><a href="#cb9-841" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>See that pooled output is nearly identical (translation invariance)</span>
<span id="cb9-842"><a href="#cb9-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-843"><a href="#cb9-843" aria-hidden="true" tabindex="-1"></a>**Notebook:** <span class="in">`session3_cnn_operations_STUDENT.ipynb`</span> (Part 3)</span>
<span id="cb9-844"><a href="#cb9-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-845"><a href="#cb9-845" aria-hidden="true" tabindex="-1"></a>**Takeaway:** Pooling reduces dimensionality while preserving important features.</span>
<span id="cb9-846"><a href="#cb9-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-847"><a href="#cb9-847" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 5: Architecture Exploration</span></span>
<span id="cb9-848"><a href="#cb9-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-849"><a href="#cb9-849" aria-hidden="true" tabindex="-1"></a>**Objective:** Compare CNN architectures visually</span>
<span id="cb9-850"><a href="#cb9-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-851"><a href="#cb9-851" aria-hidden="true" tabindex="-1"></a>**Activity:**</span>
<span id="cb9-852"><a href="#cb9-852" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Visualize LeNet-5, VGG-16, ResNet-50, U-Net architectures</span>
<span id="cb9-853"><a href="#cb9-853" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Count parameters for each</span>
<span id="cb9-854"><a href="#cb9-854" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Trace receptive field growth</span>
<span id="cb9-855"><a href="#cb9-855" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Discuss trade-offs (accuracy vs speed)</span>
<span id="cb9-856"><a href="#cb9-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-857"><a href="#cb9-857" aria-hidden="true" tabindex="-1"></a>**Notebook:** <span class="in">`session3_cnn_operations_STUDENT.ipynb`</span> (Part 4)</span>
<span id="cb9-858"><a href="#cb9-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-859"><a href="#cb9-859" aria-hidden="true" tabindex="-1"></a>**Connection to Session 4:** Choose architecture based on task (classification → ResNet, segmentation → U-Net).</span>
<span id="cb9-860"><a href="#cb9-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-861"><a href="#cb9-861" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-862"><a href="#cb9-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-863"><a href="#cb9-863" aria-hidden="true" tabindex="-1"></a><span class="fu">## Philippine EO Applications</span></span>
<span id="cb9-864"><a href="#cb9-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-865"><a href="#cb9-865" aria-hidden="true" tabindex="-1"></a><span class="fu">### PhilSA Space+ Dashboard</span></span>
<span id="cb9-866"><a href="#cb9-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-867"><a href="#cb9-867" aria-hidden="true" tabindex="-1"></a>**Current CNN Applications:**</span>
<span id="cb9-868"><a href="#cb9-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-869"><a href="#cb9-869" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Automated Cloud Masking**</span>
<span id="cb9-870"><a href="#cb9-870" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Model:** U-Net trained on 5,000 Sentinel-2 scenes</span>
<span id="cb9-871"><a href="#cb9-871" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Performance:** 95% accuracy, 2 min per scene</span>
<span id="cb9-872"><a href="#cb9-872" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Impact:** Enables rapid mosaic generation</span>
<span id="cb9-873"><a href="#cb9-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-874"><a href="#cb9-874" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Land Cover Classification (National)**</span>
<span id="cb9-875"><a href="#cb9-875" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Model:** ResNet-50 fine-tuned on Philippine landscape</span>
<span id="cb9-876"><a href="#cb9-876" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Classes:** 10 (following FAO LCCS)</span>
<span id="cb9-877"><a href="#cb9-877" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Coverage:** Entire Philippines, quarterly updates</span>
<span id="cb9-878"><a href="#cb9-878" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Users:** DENR, DAR, NEDA</span>
<span id="cb9-879"><a href="#cb9-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-880"><a href="#cb9-880" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Disaster Rapid Mapping**</span>
<span id="cb9-881"><a href="#cb9-881" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Flood Detection:** Sentinel-1 + U-Net → 6-hour response</span>
<span id="cb9-882"><a href="#cb9-882" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Damage Assessment:** High-res + object detection → building damage maps</span>
<span id="cb9-883"><a href="#cb9-883" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Integration:** NDRRMC operations dashboard</span>
<span id="cb9-884"><a href="#cb9-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-885"><a href="#cb9-885" aria-hidden="true" tabindex="-1"></a><span class="fu">### DENR Forest Monitoring</span></span>
<span id="cb9-886"><a href="#cb9-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-887"><a href="#cb9-887" aria-hidden="true" tabindex="-1"></a>**CNN Use Cases:**</span>
<span id="cb9-888"><a href="#cb9-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-889"><a href="#cb9-889" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Protected Area Surveillance:** Monthly Sentinel-2 analysis (ResNet classifier)</span>
<span id="cb9-890"><a href="#cb9-890" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Illegal Logging Detection:** Change detection CNN on multi-temporal stacks</span>
<span id="cb9-891"><a href="#cb9-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Biodiversity Hotspot Mapping:** Fine-grained forest type classification</span>
<span id="cb9-892"><a href="#cb9-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**REDD+ MRV:** Automated forest cover change reporting</span>
<span id="cb9-893"><a href="#cb9-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-894"><a href="#cb9-894" aria-hidden="true" tabindex="-1"></a>**Data Pipeline:**</span>
<span id="cb9-895"><a href="#cb9-895" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-896"><a href="#cb9-896" aria-hidden="true" tabindex="-1"></a><span class="in">Sentinel-2 (PhilSA) → Preprocessing (cloud mask) →</span></span>
<span id="cb9-897"><a href="#cb9-897" aria-hidden="true" tabindex="-1"></a><span class="in">CNN Classification → Change Detection →</span></span>
<span id="cb9-898"><a href="#cb9-898" aria-hidden="true" tabindex="-1"></a><span class="in">Alert Generation → Field Validation</span></span>
<span id="cb9-899"><a href="#cb9-899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-900"><a href="#cb9-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-901"><a href="#cb9-901" aria-hidden="true" tabindex="-1"></a><span class="fu">### LGU Applications (Session 4 Focus)</span></span>
<span id="cb9-902"><a href="#cb9-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-903"><a href="#cb9-903" aria-hidden="true" tabindex="-1"></a>**Accessible CNN Tools for Local Governments:**</span>
<span id="cb9-904"><a href="#cb9-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-905"><a href="#cb9-905" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**ASTI SkAI-Pinas:** Pre-trained models for common PH tasks</span>
<span id="cb9-906"><a href="#cb9-906" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Google Earth Engine:** CNN inference on cloud platform</span>
<span id="cb9-907"><a href="#cb9-907" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Colab Notebooks:** Low-cost GPU training (this training!)</span>
<span id="cb9-908"><a href="#cb9-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-909"><a href="#cb9-909" aria-hidden="true" tabindex="-1"></a>**Example Workflow (Session 4):**</span>
<span id="cb9-910"><a href="#cb9-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LGU staff collects 500 training labels (Palawan land cover)</span>
<span id="cb9-911"><a href="#cb9-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fine-tunes ResNet-50 using Session 4 notebook</span>
<span id="cb9-912"><a href="#cb9-912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deploys model for quarterly monitoring</span>
<span id="cb9-913"><a href="#cb9-913" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integrates into local land use planning</span>
<span id="cb9-914"><a href="#cb9-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-915"><a href="#cb9-915" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-916"><a href="#cb9-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-917"><a href="#cb9-917" aria-hidden="true" tabindex="-1"></a><span class="fu">## Expected Outcomes</span></span>
<span id="cb9-918"><a href="#cb9-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-919"><a href="#cb9-919" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conceptual Understanding</span></span>
<span id="cb9-920"><a href="#cb9-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-921"><a href="#cb9-921" aria-hidden="true" tabindex="-1"></a>By the end of Session 3, you should be able to:</span>
<span id="cb9-922"><a href="#cb9-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-923"><a href="#cb9-923" aria-hidden="true" tabindex="-1"></a>✅ **Explain to a colleague** why CNNs are better than Random Forest for complex spatial patterns</span>
<span id="cb9-924"><a href="#cb9-924" aria-hidden="true" tabindex="-1"></a>✅ **Sketch** a simple CNN architecture and label components (Conv, Pool, FC)</span>
<span id="cb9-925"><a href="#cb9-925" aria-hidden="true" tabindex="-1"></a>✅ **Describe** what happens during forward and backward propagation</span>
<span id="cb9-926"><a href="#cb9-926" aria-hidden="true" tabindex="-1"></a>✅ **Identify** appropriate architectures for classification vs segmentation</span>
<span id="cb9-927"><a href="#cb9-927" aria-hidden="true" tabindex="-1"></a>✅ **Discuss** data requirements and computational constraints</span>
<span id="cb9-928"><a href="#cb9-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-929"><a href="#cb9-929" aria-hidden="true" tabindex="-1"></a><span class="fu">### Technical Skills</span></span>
<span id="cb9-930"><a href="#cb9-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-931"><a href="#cb9-931" aria-hidden="true" tabindex="-1"></a>✅ **Build** a simple perceptron from scratch using NumPy</span>
<span id="cb9-932"><a href="#cb9-932" aria-hidden="true" tabindex="-1"></a>✅ **Implement** activation functions and visualize their behavior</span>
<span id="cb9-933"><a href="#cb9-933" aria-hidden="true" tabindex="-1"></a>✅ **Perform** manual convolution on satellite imagery</span>
<span id="cb9-934"><a href="#cb9-934" aria-hidden="true" tabindex="-1"></a>✅ **Apply** classic edge detection filters (Sobel, Gaussian)</span>
<span id="cb9-935"><a href="#cb9-935" aria-hidden="true" tabindex="-1"></a>✅ **Visualize** feature maps and pooling operations</span>
<span id="cb9-936"><a href="#cb9-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-937"><a href="#cb9-937" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Readiness for Session 4</span></span>
<span id="cb9-938"><a href="#cb9-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-939"><a href="#cb9-939" aria-hidden="true" tabindex="-1"></a>✅ **Understand** why we'll use TensorFlow/Keras (vs building from scratch)</span>
<span id="cb9-940"><a href="#cb9-940" aria-hidden="true" tabindex="-1"></a>✅ **Anticipate** challenges with Sentinel-2 multi-band data</span>
<span id="cb9-941"><a href="#cb9-941" aria-hidden="true" tabindex="-1"></a>✅ **Recognize** data preparation needs (chips, labels, augmentation)</span>
<span id="cb9-942"><a href="#cb9-942" aria-hidden="true" tabindex="-1"></a>✅ **Set expectations** for training time and resource requirements</span>
<span id="cb9-943"><a href="#cb9-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-944"><a href="#cb9-944" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-945"><a href="#cb9-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-946"><a href="#cb9-946" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hands-On Notebooks</span></span>
<span id="cb9-947"><a href="#cb9-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-948"><a href="#cb9-948" aria-hidden="true" tabindex="-1"></a><span class="fu">### Access the Interactive Materials</span></span>
<span id="cb9-949"><a href="#cb9-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-950"><a href="#cb9-950" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb9-951"><a href="#cb9-951" aria-hidden="true" tabindex="-1"></a><span class="fu">## 📓 Jupyter Notebooks (Theory + Interactive)</span></span>
<span id="cb9-952"><a href="#cb9-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-953"><a href="#cb9-953" aria-hidden="true" tabindex="-1"></a>Two comprehensive notebooks guide you through neural network and CNN fundamentals:</span>
<span id="cb9-954"><a href="#cb9-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-955"><a href="#cb9-955" aria-hidden="true" tabindex="-1"></a>**Notebook 1: Neural Network Theory**</span>
<span id="cb9-956"><a href="#cb9-956" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`session3_theory_interactive.ipynb`</span><span class="co">](../notebooks/session3_theory_interactive.ipynb)</span></span>
<span id="cb9-957"><a href="#cb9-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-958"><a href="#cb9-958" aria-hidden="true" tabindex="-1"></a>**Contents:**</span>
<span id="cb9-959"><a href="#cb9-959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Build perceptron from scratch</span>
<span id="cb9-960"><a href="#cb9-960" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement activation functions</span>
<span id="cb9-961"><a href="#cb9-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train 2-layer network on spectral data</span>
<span id="cb9-962"><a href="#cb9-962" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explore learning rate effects</span>
<span id="cb9-963"><a href="#cb9-963" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize decision boundaries</span>
<span id="cb9-964"><a href="#cb9-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-965"><a href="#cb9-965" aria-hidden="true" tabindex="-1"></a>**Duration:** 45 minutes</span>
<span id="cb9-966"><a href="#cb9-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-967"><a href="#cb9-967" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-968"><a href="#cb9-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-969"><a href="#cb9-969" aria-hidden="true" tabindex="-1"></a>**Notebook 2: CNN Operations**</span>
<span id="cb9-970"><a href="#cb9-970" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`session3_theory_interactive.ipynb`</span><span class="co">](../notebooks/session3_theory_interactive.ipynb)</span></span>
<span id="cb9-971"><a href="#cb9-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-972"><a href="#cb9-972" aria-hidden="true" tabindex="-1"></a>**Contents:**</span>
<span id="cb9-973"><a href="#cb9-973" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manual convolution on Sentinel-2 imagery</span>
<span id="cb9-974"><a href="#cb9-974" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Edge detection filters (Sobel, Gaussian, Laplacian)</span>
<span id="cb9-975"><a href="#cb9-975" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Max pooling demonstration</span>
<span id="cb9-976"><a href="#cb9-976" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Architecture comparison (LeNet, VGG, ResNet, U-Net)</span>
<span id="cb9-977"><a href="#cb9-977" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature map visualization</span>
<span id="cb9-978"><a href="#cb9-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-979"><a href="#cb9-979" aria-hidden="true" tabindex="-1"></a>**Duration:** 55 minutes</span>
<span id="cb9-980"><a href="#cb9-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-981"><a href="#cb9-981" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-982"><a href="#cb9-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-983"><a href="#cb9-983" aria-hidden="true" tabindex="-1"></a>**Google Colab:**</span>
<span id="cb9-984"><a href="#cb9-984" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="al">![Open Theory Notebook](https://colab.research.google.com/assets/colab-badge.svg)</span><span class="co">](https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb)</span></span>
<span id="cb9-985"><a href="#cb9-985" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="al">![Open CNN Notebook](https://colab.research.google.com/assets/colab-badge.svg)</span><span class="co">](https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb)</span></span>
<span id="cb9-986"><a href="#cb9-986" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-987"><a href="#cb9-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-988"><a href="#cb9-988" aria-hidden="true" tabindex="-1"></a><span class="fu">### Supporting Documentation</span></span>
<span id="cb9-989"><a href="#cb9-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-990"><a href="#cb9-990" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb9-991"><a href="#cb9-991" aria-hidden="true" tabindex="-1"></a><span class="fu">## 📚 Reference Materials</span></span>
<span id="cb9-992"><a href="#cb9-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-993"><a href="#cb9-993" aria-hidden="true" tabindex="-1"></a>**CNN Architectures Deep Dive:**</span>
<span id="cb9-994"><a href="#cb9-994" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`Cheatsheets &amp; References`</span><span class="co">](../../resources/cheatsheets.qmd)</span></span>
<span id="cb9-995"><a href="#cb9-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-996"><a href="#cb9-996" aria-hidden="true" tabindex="-1"></a>Detailed explanations of LeNet-5, VGG-16, ResNet-50, U-Net, and modern variants (EfficientNet, Vision Transformers). Includes architecture diagrams, parameter counts, and EO applications.</span>
<span id="cb9-997"><a href="#cb9-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-998"><a href="#cb9-998" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-999"><a href="#cb9-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1000"><a href="#cb9-1000" aria-hidden="true" tabindex="-1"></a>**EO Applications Guide:**</span>
<span id="cb9-1001"><a href="#cb9-1001" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`Glossary &amp; EO Resources`</span><span class="co">](../../resources/glossary.qmd)</span></span>
<span id="cb9-1002"><a href="#cb9-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1003"><a href="#cb9-1003" aria-hidden="true" tabindex="-1"></a>Comprehensive guide to CNN applications in Earth observation:</span>
<span id="cb9-1004"><a href="#cb9-1004" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene classification examples</span>
<span id="cb9-1005"><a href="#cb9-1005" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic segmentation workflows</span>
<span id="cb9-1006"><a href="#cb9-1006" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object detection case studies</span>
<span id="cb9-1007"><a href="#cb9-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Change detection methods</span>
<span id="cb9-1008"><a href="#cb9-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Philippine-specific use cases</span>
<span id="cb9-1009"><a href="#cb9-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1010"><a href="#cb9-1010" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1011"><a href="#cb9-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1012"><a href="#cb9-1012" aria-hidden="true" tabindex="-1"></a>**Session Overview:**</span>
<span id="cb9-1013"><a href="#cb9-1013" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`Day 2 Overview`</span><span class="co">](../index.qmd)</span></span>
<span id="cb9-1014"><a href="#cb9-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1015"><a href="#cb9-1015" aria-hidden="true" tabindex="-1"></a>Quick reference with session objectives, structure, and prerequisites.</span>
<span id="cb9-1016"><a href="#cb9-1016" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-1017"><a href="#cb9-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1018"><a href="#cb9-1018" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1019"><a href="#cb9-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1020"><a href="#cb9-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">## Troubleshooting</span></span>
<span id="cb9-1021"><a href="#cb9-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1022"><a href="#cb9-1022" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Conceptual Questions</span></span>
<span id="cb9-1023"><a href="#cb9-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1024"><a href="#cb9-1024" aria-hidden="true" tabindex="-1"></a>**"Why does CNN need so much more data than Random Forest?"**</span>
<span id="cb9-1025"><a href="#cb9-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1026"><a href="#cb9-1026" aria-hidden="true" tabindex="-1"></a>Random Forest learns from features YOU engineered (NDVI, GLCM). It needs to learn relationships between ~10-20 features.</span>
<span id="cb9-1027"><a href="#cb9-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1028"><a href="#cb9-1028" aria-hidden="true" tabindex="-1"></a>CNNs learn from raw pixels (~10,000 per image chip). It needs to learn what features to extract PLUS how to classify. Much harder optimization problem = more data needed.</span>
<span id="cb9-1029"><a href="#cb9-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1030"><a href="#cb9-1030" aria-hidden="true" tabindex="-1"></a>**Mitigation:** Transfer learning (Session 4) dramatically reduces data needs.</span>
<span id="cb9-1031"><a href="#cb9-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1032"><a href="#cb9-1032" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1033"><a href="#cb9-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1034"><a href="#cb9-1034" aria-hidden="true" tabindex="-1"></a>**"Can I use CNNs with small datasets (&lt;500 samples)?"**</span>
<span id="cb9-1035"><a href="#cb9-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1036"><a href="#cb9-1036" aria-hidden="true" tabindex="-1"></a>Technically yes, but results will be poor if training from scratch.</span>
<span id="cb9-1037"><a href="#cb9-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1038"><a href="#cb9-1038" aria-hidden="true" tabindex="-1"></a>**Better approach:**</span>
<span id="cb9-1039"><a href="#cb9-1039" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use pre-trained models (ImageNet weights)</span>
<span id="cb9-1040"><a href="#cb9-1040" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Aggressive data augmentation</span>
<span id="cb9-1041"><a href="#cb9-1041" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Use simpler architectures (fewer layers)</span>
<span id="cb9-1042"><a href="#cb9-1042" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Consider traditional ML if data is truly limited</span>
<span id="cb9-1043"><a href="#cb9-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1044"><a href="#cb9-1044" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1045"><a href="#cb9-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1046"><a href="#cb9-1046" aria-hidden="true" tabindex="-1"></a>**"Why are my manual convolutions so slow in the notebook?"**</span>
<span id="cb9-1047"><a href="#cb9-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1048"><a href="#cb9-1048" aria-hidden="true" tabindex="-1"></a>NumPy convolutions (for learning) are intentionally simple. Production CNNs use highly optimized libraries (cuDNN) on GPUs—1000× faster!</span>
<span id="cb9-1049"><a href="#cb9-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1050"><a href="#cb9-1050" aria-hidden="true" tabindex="-1"></a>Session 4 will use TensorFlow/PyTorch with GPU acceleration.</span>
<span id="cb9-1051"><a href="#cb9-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1052"><a href="#cb9-1052" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1053"><a href="#cb9-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1054"><a href="#cb9-1054" aria-hidden="true" tabindex="-1"></a>**"How do I know which architecture to use?"**</span>
<span id="cb9-1055"><a href="#cb9-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1056"><a href="#cb9-1056" aria-hidden="true" tabindex="-1"></a>**Simple decision tree:**</span>
<span id="cb9-1057"><a href="#cb9-1057" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification task** (one label per image) → ResNet, EfficientNet</span>
<span id="cb9-1058"><a href="#cb9-1058" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segmentation task** (label every pixel) → U-Net, DeepLabv3+</span>
<span id="cb9-1059"><a href="#cb9-1059" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Object detection** (find + localize) → YOLO, Faster R-CNN</span>
<span id="cb9-1060"><a href="#cb9-1060" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Limited data** → Simpler architecture + transfer learning</span>
<span id="cb9-1061"><a href="#cb9-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Real-time inference needed** → MobileNet, EfficientNet-Lite</span>
<span id="cb9-1062"><a href="#cb9-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1063"><a href="#cb9-1063" aria-hidden="true" tabindex="-1"></a>Session 4 will implement ResNet (classification) and U-Net (segmentation).</span>
<span id="cb9-1064"><a href="#cb9-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1065"><a href="#cb9-1065" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1066"><a href="#cb9-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1067"><a href="#cb9-1067" aria-hidden="true" tabindex="-1"></a><span class="fu">### Technical Issues</span></span>
<span id="cb9-1068"><a href="#cb9-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1069"><a href="#cb9-1069" aria-hidden="true" tabindex="-1"></a>**"Notebook cells won't execute in Colab"**</span>
<span id="cb9-1070"><a href="#cb9-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1071"><a href="#cb9-1071" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Check GPU is enabled: Runtime → Change runtime type → GPU</span>
<span id="cb9-1072"><a href="#cb9-1072" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Restart runtime: Runtime → Restart runtime</span>
<span id="cb9-1073"><a href="#cb9-1073" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Verify installations: <span class="in">`!pip list | grep numpy`</span></span>
<span id="cb9-1074"><a href="#cb9-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1075"><a href="#cb9-1075" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1076"><a href="#cb9-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1077"><a href="#cb9-1077" aria-hidden="true" tabindex="-1"></a>**"Out of memory error when running convolutions"**</span>
<span id="cb9-1078"><a href="#cb9-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1079"><a href="#cb9-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use smaller image chips (128×128 instead of 256×256)</span>
<span id="cb9-1080"><a href="#cb9-1080" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Process one image at a time (don't load entire dataset)</span>
<span id="cb9-1081"><a href="#cb9-1081" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Restart Colab runtime to clear memory</span>
<span id="cb9-1082"><a href="#cb9-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1083"><a href="#cb9-1083" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1084"><a href="#cb9-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1085"><a href="#cb9-1085" aria-hidden="true" tabindex="-1"></a>**"Visualizations aren't displaying"**</span>
<span id="cb9-1086"><a href="#cb9-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1087"><a href="#cb9-1087" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb9-1088"><a href="#cb9-1088" aria-hidden="true" tabindex="-1"></a><span class="co"># Add to beginning of notebook</span></span>
<span id="cb9-1089"><a href="#cb9-1089" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb9-1090"><a href="#cb9-1090" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-1091"><a href="#cb9-1091" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb9-1092"><a href="#cb9-1092" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-1093"><a href="#cb9-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1094"><a href="#cb9-1094" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1095"><a href="#cb9-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1096"><a href="#cb9-1096" aria-hidden="true" tabindex="-1"></a>**"Can't access Sentinel-2 data in notebook"**</span>
<span id="cb9-1097"><a href="#cb9-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1098"><a href="#cb9-1098" aria-hidden="true" tabindex="-1"></a>Session 3 uses pre-downloaded Sentinel-2 samples (included in notebook). No GEE authentication needed.</span>
<span id="cb9-1099"><a href="#cb9-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1100"><a href="#cb9-1100" aria-hidden="true" tabindex="-1"></a>Session 4 will integrate GEE for larger-scale data loading.</span>
<span id="cb9-1101"><a href="#cb9-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1102"><a href="#cb9-1102" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1103"><a href="#cb9-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1104"><a href="#cb9-1104" aria-hidden="true" tabindex="-1"></a><span class="fu">### Getting Help</span></span>
<span id="cb9-1105"><a href="#cb9-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1106"><a href="#cb9-1106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>📖 <span class="co">[</span><span class="ot">CNN Tutorial (Google ML Crash Course)</span><span class="co">](https://developers.google.com/machine-learning/crash-course/image-classification)</span></span>
<span id="cb9-1107"><a href="#cb9-1107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>📖 <span class="co">[</span><span class="ot">CS231n Stanford Course</span><span class="co">](http://cs231n.stanford.edu/)</span> - Best CNN educational resource</span>
<span id="cb9-1108"><a href="#cb9-1108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>💬 <span class="co">[</span><span class="ot">Instructor support</span><span class="co">](mailto:training@copphil.org)</span> - Questions during lab hours</span>
<span id="cb9-1109"><a href="#cb9-1109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>📧 <span class="co">[</span><span class="ot">PhilSA Data Support</span><span class="co">](https://data.philsa.gov.ph/support)</span> - Access issues</span>
<span id="cb9-1110"><a href="#cb9-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1111"><a href="#cb9-1111" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1112"><a href="#cb9-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1113"><a href="#cb9-1113" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb9-1114"><a href="#cb9-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1115"><a href="#cb9-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">### Foundational Learning</span></span>
<span id="cb9-1116"><a href="#cb9-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1117"><a href="#cb9-1117" aria-hidden="true" tabindex="-1"></a>**Neural Networks:**</span>
<span id="cb9-1118"><a href="#cb9-1118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">3Blue1Brown Neural Network Series</span><span class="co">](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)</span> - Best intuitive explanation (visual)</span>
<span id="cb9-1119"><a href="#cb9-1119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Neural Networks and Deep Learning (Free Book)</span><span class="co">](http://neuralnetworksanddeeplearning.com/)</span></span>
<span id="cb9-1120"><a href="#cb9-1120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Andrew Ng's Deep Learning Specialization</span><span class="co">](https://www.coursera.org/specializations/deep-learning)</span> (Coursera)</span>
<span id="cb9-1121"><a href="#cb9-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1122"><a href="#cb9-1122" aria-hidden="true" tabindex="-1"></a>**CNNs Specifically:**</span>
<span id="cb9-1123"><a href="#cb9-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Stanford CS231n Lectures</span><span class="co">](http://cs231n.stanford.edu/schedule.html)</span> - Comprehensive (free)</span>
<span id="cb9-1124"><a href="#cb9-1124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">CNN Explainer (Interactive)</span><span class="co">](https://poloclub.github.io/cnn-explainer/)</span> - Visualize CNNs in browser</span>
<span id="cb9-1125"><a href="#cb9-1125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Distill.pub Articles</span><span class="co">](https://distill.pub/)</span> - Beautiful visual explanations</span>
<span id="cb9-1126"><a href="#cb9-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1127"><a href="#cb9-1127" aria-hidden="true" tabindex="-1"></a><span class="fu">### Earth Observation Deep Learning</span></span>
<span id="cb9-1128"><a href="#cb9-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1129"><a href="#cb9-1129" aria-hidden="true" tabindex="-1"></a>**Papers:**</span>
<span id="cb9-1130"><a href="#cb9-1130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Zhu et al. (2017). "Deep Learning in Remote Sensing: A Review." *IEEE GRSM*</span>
<span id="cb9-1131"><a href="#cb9-1131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ma et al. (2019). "Deep learning in remote sensing applications: A meta-analysis and review." *ISPRS*</span>
<span id="cb9-1132"><a href="#cb9-1132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rußwurm &amp; Körner (2020). "Self-attention for raw optical satellite time series classification." *ISPRS*</span>
<span id="cb9-1133"><a href="#cb9-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1134"><a href="#cb9-1134" aria-hidden="true" tabindex="-1"></a>**Tutorials:**</span>
<span id="cb9-1135"><a href="#cb9-1135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Deep Learning for Earth Observation (ESA)</span><span class="co">](https://eo-college.org/resources/deep-learning/)</span></span>
<span id="cb9-1136"><a href="#cb9-1136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Raster Vision</span><span class="co">](https://rastervision.io/)</span> - Framework for geospatial ML</span>
<span id="cb9-1137"><a href="#cb9-1137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">TorchGeo</span><span class="co">](https://torchgeo.readthedocs.io/)</span> - PyTorch library for geospatial data</span>
<span id="cb9-1138"><a href="#cb9-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1139"><a href="#cb9-1139" aria-hidden="true" tabindex="-1"></a>**Datasets:**</span>
<span id="cb9-1140"><a href="#cb9-1140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">EuroSAT</span><span class="co">](https://github.com/phelber/EuroSAT)</span> - Sentinel-2 scene classification (10 classes)</span>
<span id="cb9-1141"><a href="#cb9-1141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">UC Merced Land Use</span><span class="co">](http://weegee.vision.ucmerced.edu/datasets/landuse.html)</span> - High-res classification</span>
<span id="cb9-1142"><a href="#cb9-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DeepGlobe</span><span class="co">](http://deepglobe.org/)</span> - Segmentation challenges</span>
<span id="cb9-1143"><a href="#cb9-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SpaceNet</span><span class="co">](https://spacenet.ai/)</span> - Building detection</span>
<span id="cb9-1144"><a href="#cb9-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1145"><a href="#cb9-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine Context</span></span>
<span id="cb9-1146"><a href="#cb9-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1147"><a href="#cb9-1147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PhilSA Research Publications</span><span class="co">](https://philsa.gov.ph/publications/)</span> - CNN applications in Philippines</span>
<span id="cb9-1148"><a href="#cb9-1148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">ASTI SkAI-Pinas Documentation</span><span class="co">](https://skai.dost.gov.ph/docs)</span> - Pre-trained PH models</span>
<span id="cb9-1149"><a href="#cb9-1149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DIMER Database</span><span class="co">](https://dimer.asti.dost.gov.ph/)</span> - Philippine disaster imagery</span>
<span id="cb9-1150"><a href="#cb9-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1151"><a href="#cb9-1151" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1152"><a href="#cb9-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1153"><a href="#cb9-1153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Assessment</span></span>
<span id="cb9-1154"><a href="#cb9-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1155"><a href="#cb9-1155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Formative Assessment (During Session)</span></span>
<span id="cb9-1156"><a href="#cb9-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1157"><a href="#cb9-1157" aria-hidden="true" tabindex="-1"></a>**Self-Check Questions:**</span>
<span id="cb9-1158"><a href="#cb9-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1159"><a href="#cb9-1159" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>✓ Can you explain the difference between a perceptron and a multi-layer network?</span>
<span id="cb9-1160"><a href="#cb9-1160" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>✓ Why does ReLU work better than sigmoid in hidden layers?</span>
<span id="cb9-1161"><a href="#cb9-1161" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>✓ What does a convolutional filter "learn" during training?</span>
<span id="cb9-1162"><a href="#cb9-1162" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>✓ How does pooling provide translation invariance?</span>
<span id="cb9-1163"><a href="#cb9-1163" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>✓ When would you use U-Net instead of ResNet?</span>
<span id="cb9-1164"><a href="#cb9-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1165"><a href="#cb9-1165" aria-hidden="true" tabindex="-1"></a>**Interactive Exercises:**</span>
<span id="cb9-1166"><a href="#cb9-1166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Complete all TODO cells in theory notebook</span>
<span id="cb9-1167"><a href="#cb9-1167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Implement manual convolution from scratch</span>
<span id="cb9-1168"><a href="#cb9-1168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Visualize at least 3 different activation functions</span>
<span id="cb9-1169"><a href="#cb9-1169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Compare filter responses on forest vs urban areas</span>
<span id="cb9-1170"><a href="#cb9-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1171"><a href="#cb9-1171" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summative Assessment (End of Session)</span></span>
<span id="cb9-1172"><a href="#cb9-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1173"><a href="#cb9-1173" aria-hidden="true" tabindex="-1"></a>**Knowledge Check (10 questions, multiple choice):**</span>
<span id="cb9-1174"><a href="#cb9-1174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Neural network components</span>
<span id="cb9-1175"><a href="#cb9-1175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CNN architecture understanding</span>
<span id="cb9-1176"><a href="#cb9-1176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Application selection (classification vs segmentation)</span>
<span id="cb9-1177"><a href="#cb9-1177" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data requirements and constraints</span>
<span id="cb9-1178"><a href="#cb9-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1179"><a href="#cb9-1179" aria-hidden="true" tabindex="-1"></a>**Practical Demonstration:**</span>
<span id="cb9-1180"><a href="#cb9-1180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain CNN decision-making process for a sample image</span>
<span id="cb9-1181"><a href="#cb9-1181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sketch appropriate architecture for given EO task</span>
<span id="cb9-1182"><a href="#cb9-1182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimate data requirements for Philippine use case</span>
<span id="cb9-1183"><a href="#cb9-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1184"><a href="#cb9-1184" aria-hidden="true" tabindex="-1"></a>**Readiness for Session 4:**</span>
<span id="cb9-1185"><a href="#cb9-1185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Understand TensorFlow/Keras workflow conceptually</span>
<span id="cb9-1186"><a href="#cb9-1186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Recognize data preparation needs</span>
<span id="cb9-1187"><a href="#cb9-1187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Anticipate training challenges</span>
<span id="cb9-1188"><a href="#cb9-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1189"><a href="#cb9-1189" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1190"><a href="#cb9-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1191"><a href="#cb9-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next Steps</span></span>
<span id="cb9-1192"><a href="#cb9-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1193"><a href="#cb9-1193" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb9-1194"><a href="#cb9-1194" aria-hidden="true" tabindex="-1"></a><span class="fu">## After Session 3</span></span>
<span id="cb9-1195"><a href="#cb9-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1196"><a href="#cb9-1196" aria-hidden="true" tabindex="-1"></a>You now understand CNN theory and operations! Session 4 puts this knowledge into practice with hands-on implementation.</span>
<span id="cb9-1197"><a href="#cb9-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1198"><a href="#cb9-1198" aria-hidden="true" tabindex="-1"></a>**Session 4: Hands-On CNN Implementation**</span>
<span id="cb9-1199"><a href="#cb9-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1200"><a href="#cb9-1200" aria-hidden="true" tabindex="-1"></a>You'll build and train:</span>
<span id="cb9-1201"><a href="#cb9-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**ResNet Classifier:** Palawan land cover (8 classes)</span>
<span id="cb9-1202"><a href="#cb9-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**U-Net Segmentation:** Flood mapping (Central Luzon)</span>
<span id="cb9-1203"><a href="#cb9-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1204"><a href="#cb9-1204" aria-hidden="true" tabindex="-1"></a>Using TensorFlow/Keras with real Sentinel-2 data.</span>
<span id="cb9-1205"><a href="#cb9-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1206"><a href="#cb9-1206" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Continue to Session 4 →</span><span class="co">](session4.qmd)</span>{.btn .btn-primary}</span>
<span id="cb9-1207"><a href="#cb9-1207" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-1208"><a href="#cb9-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1209"><a href="#cb9-1209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recommended Pre-Work for Session 4</span></span>
<span id="cb9-1210"><a href="#cb9-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1211"><a href="#cb9-1211" aria-hidden="true" tabindex="-1"></a>Before Session 4, please:</span>
<span id="cb9-1212"><a href="#cb9-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1213"><a href="#cb9-1213" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>✅ **Review notebook exercises** - Ensure you understand convolution and activation functions</span>
<span id="cb9-1214"><a href="#cb9-1214" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>✅ **Read U-Net paper** - <span class="co">[</span><span class="ot">Ronneberger et al. 2015</span><span class="co">](https://arxiv.org/abs/1505.04597)</span> (15 min)</span>
<span id="cb9-1215"><a href="#cb9-1215" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>✅ **Check GPU access** - Enable GPU in Colab (Settings → Hardware Accelerator → GPU)</span>
<span id="cb9-1216"><a href="#cb9-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>✅ **Install TensorFlow** - <span class="in">`!pip install tensorflow==2.15`</span> (will do in Session 4, but test now)</span>
<span id="cb9-1217"><a href="#cb9-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1218"><a href="#cb9-1218" aria-hidden="true" tabindex="-1"></a><span class="fu">### Extended Learning Paths</span></span>
<span id="cb9-1219"><a href="#cb9-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1220"><a href="#cb9-1220" aria-hidden="true" tabindex="-1"></a>**Path 1: Deep Dive into Theory**</span>
<span id="cb9-1221"><a href="#cb9-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complete CS231n Stanford course</span>
<span id="cb9-1222"><a href="#cb9-1222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement backpropagation from scratch</span>
<span id="cb9-1223"><a href="#cb9-1223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Study optimization algorithms (Adam, RMSprop)</span>
<span id="cb9-1224"><a href="#cb9-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1225"><a href="#cb9-1225" aria-hidden="true" tabindex="-1"></a>**Path 2: Explore Advanced Architectures**</span>
<span id="cb9-1226"><a href="#cb9-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention mechanisms (Transformers for EO)</span>
<span id="cb9-1227"><a href="#cb9-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vision Transformers (ViT)</span>
<span id="cb9-1228"><a href="#cb9-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Self-supervised learning (SimCLR, MoCo)</span>
<span id="cb9-1229"><a href="#cb9-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1230"><a href="#cb9-1230" aria-hidden="true" tabindex="-1"></a>**Path 3: Philippine EO Applications**</span>
<span id="cb9-1231"><a href="#cb9-1231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Contribute training data to PhilSA Space+</span>
<span id="cb9-1232"><a href="#cb9-1232" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Develop CNN model for local area</span>
<span id="cb9-1233"><a href="#cb9-1233" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publish results in Philippine GIS conference</span>
<span id="cb9-1234"><a href="#cb9-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1235"><a href="#cb9-1235" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1236"><a href="#cb9-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1237"><a href="#cb9-1237" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quick Links</span></span>
<span id="cb9-1238"><a href="#cb9-1238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Part A (ML→DL):** 15 min - Keep conceptual, avoid getting bogged down in math</span>
<span id="cb9-1239"><a href="#cb9-1239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Part B (NN Fundamentals):** 25 min - Live code perceptron, let students modify</span>
<span id="cb9-1240"><a href="#cb9-1240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Part C (CNNs):** 30 min - Most critical section, use lots of visuals</span>
<span id="cb9-1241"><a href="#cb9-1241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Part D (EO Applications):** 25 min - Show real PhilSA examples if possible</span>
<span id="cb9-1242"><a href="#cb9-1242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Part E (Practical):** 15 min - Set realistic expectations for Session 4</span>
<span id="cb9-1243"><a href="#cb9-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1244"><a href="#cb9-1244" aria-hidden="true" tabindex="-1"></a>**Total:** 110 min (2.5 hours with 20 min buffer for questions)</span>
<span id="cb9-1245"><a href="#cb9-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1246"><a href="#cb9-1246" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1247"><a href="#cb9-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1248"><a href="#cb9-1248" aria-hidden="true" tabindex="-1"></a>**Common Student Challenges:**</span>
<span id="cb9-1249"><a href="#cb9-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1250"><a href="#cb9-1250" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**"I don't understand backpropagation"**</span>
<span id="cb9-1251"><a href="#cb9-1251" aria-hidden="true" tabindex="-1"></a>   → Focus on intuition (gradient descent down error surface), not calculus. Session 4 uses libraries that handle this automatically.</span>
<span id="cb9-1252"><a href="#cb9-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1253"><a href="#cb9-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**"Why are we doing manual convolutions in NumPy?"**</span>
<span id="cb9-1254"><a href="#cb9-1254" aria-hidden="true" tabindex="-1"></a>   → Emphasize this is for learning. Session 4 uses optimized libraries (1000× faster). Like learning to drive with manual transmission—helps understand what's happening under the hood.</span>
<span id="cb9-1255"><a href="#cb9-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1256"><a href="#cb9-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**"Will my laptop be fast enough for Session 4?"**</span>
<span id="cb9-1257"><a href="#cb9-1257" aria-hidden="true" tabindex="-1"></a>   → No local installation needed! Google Colab provides free GPUs. Sessions 4 notebooks are optimized for Colab free tier.</span>
<span id="cb9-1258"><a href="#cb9-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1259"><a href="#cb9-1259" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**"Can CNNs use Sentinel-2's 10+ bands?"**</span>
<span id="cb9-1260"><a href="#cb9-1260" aria-hidden="true" tabindex="-1"></a>   → YES! Unlike ImageNet RGB pre-training. Session 4 shows adaptation strategies. This is a huge advantage of CNNs for EO.</span>
<span id="cb9-1261"><a href="#cb9-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1262"><a href="#cb9-1262" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1263"><a href="#cb9-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1264"><a href="#cb9-1264" aria-hidden="true" tabindex="-1"></a>**Teaching Tips:**</span>
<span id="cb9-1265"><a href="#cb9-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1266"><a href="#cb9-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Start with Familiar:** Connect to Session 2 Random Forest throughout</span>
<span id="cb9-1267"><a href="#cb9-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Visual Heavy:** Show lots of images/diagrams (CNNs are visual learners!)</span>
<span id="cb9-1268"><a href="#cb9-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interactive:** Have students modify filter values in real-time</span>
<span id="cb9-1269"><a href="#cb9-1269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Philippine Examples:** Use Palawan imagery from Session 2 for continuity</span>
<span id="cb9-1270"><a href="#cb9-1270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Manage Expectations:** Be honest about data/compute requirements</span>
<span id="cb9-1271"><a href="#cb9-1271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Celebrate Progress:** "You now understand CNNs better than 90% of GIS professionals!"</span>
<span id="cb9-1272"><a href="#cb9-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1273"><a href="#cb9-1273" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1274"><a href="#cb9-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1275"><a href="#cb9-1275" aria-hidden="true" tabindex="-1"></a>**Demonstration Best Practices:**</span>
<span id="cb9-1276"><a href="#cb9-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1277"><a href="#cb9-1277" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Perceptron Demo:** Use simple 2D data first (NDVI vs NDWI), visualize decision boundary updating in real-time</span>
<span id="cb9-1278"><a href="#cb9-1278" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Convolution Demo:** Pick dramatic example (forest edge) where edge detection is obvious</span>
<span id="cb9-1279"><a href="#cb9-1279" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Architecture Comparison:** Show parameter counts to emphasize efficiency gains</span>
<span id="cb9-1280"><a href="#cb9-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Philippine Focus:** Always end each section with "How does this help DENR/PhilSA/LGUs?"</span>
<span id="cb9-1281"><a href="#cb9-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1282"><a href="#cb9-1282" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1283"><a href="#cb9-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1284"><a href="#cb9-1284" aria-hidden="true" tabindex="-1"></a>**Assessment Rubric:**</span>
<span id="cb9-1285"><a href="#cb9-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1286"><a href="#cb9-1286" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Criteria <span class="pp">|</span> Excellent (5) <span class="pp">|</span> Good (4) <span class="pp">|</span> Adequate (3) <span class="pp">|</span> Needs Improvement (2) <span class="pp">|</span></span>
<span id="cb9-1287"><a href="#cb9-1287" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|---------------|----------|--------------|----------------------|</span></span>
<span id="cb9-1288"><a href="#cb9-1288" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Conceptual** <span class="pp">|</span> Explains CNN advantages with examples <span class="pp">|</span> States CNN benefits <span class="pp">|</span> Lists CNN components <span class="pp">|</span> Confuses CNN with traditional ML <span class="pp">|</span></span>
<span id="cb9-1289"><a href="#cb9-1289" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Technical** <span class="pp">|</span> Implements convolution from scratch <span class="pp">|</span> Completes all notebook exercises <span class="pp">|</span> Completes &gt;70% exercises <span class="pp">|</span> Struggles with NumPy operations <span class="pp">|</span></span>
<span id="cb9-1290"><a href="#cb9-1290" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Application** <span class="pp">|</span> Proposes appropriate architecture for novel task <span class="pp">|</span> Identifies correct architecture for standard tasks <span class="pp">|</span> Distinguishes classification vs segmentation <span class="pp">|</span> Cannot select architecture <span class="pp">|</span></span>
<span id="cb9-1291"><a href="#cb9-1291" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Readiness** <span class="pp">|</span> Articulates Session 4 workflow <span class="pp">|</span> Understands TensorFlow role <span class="pp">|</span> Knows Session 4 is hands-on <span class="pp">|</span> Unclear on next steps <span class="pp">|</span></span>
<span id="cb9-1292"><a href="#cb9-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1293"><a href="#cb9-1293" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1294"><a href="#cb9-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1295"><a href="#cb9-1295" aria-hidden="true" tabindex="-1"></a>**Session 4 Transition:**</span>
<span id="cb9-1296"><a href="#cb9-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1297"><a href="#cb9-1297" aria-hidden="true" tabindex="-1"></a>End with excitement:</span>
<span id="cb9-1298"><a href="#cb9-1298" aria-hidden="true" tabindex="-1"></a>*"You've learned the theory. Tomorrow you'll build a production-ready land cover classifier and flood mapper using TensorFlow. Bring your questions, bring your data ideas, and get ready to train some CNNs!"*</span>
<span id="cb9-1299"><a href="#cb9-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1300"><a href="#cb9-1300" aria-hidden="true" tabindex="-1"></a>**Preview Session 4 Outcomes:**</span>
<span id="cb9-1301"><a href="#cb9-1301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ResNet model achieving &gt;85% accuracy on Palawan</span>
<span id="cb9-1302"><a href="#cb9-1302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net generating flood maps in 2 minutes</span>
<span id="cb9-1303"><a href="#cb9-1303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Exportable models for operational use</span>
<span id="cb9-1304"><a href="#cb9-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1305"><a href="#cb9-1305" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1306"><a href="#cb9-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1307"><a href="#cb9-1307" aria-hidden="true" tabindex="-1"></a>**Backup Activities (if ahead of schedule):**</span>
<span id="cb9-1308"><a href="#cb9-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1309"><a href="#cb9-1309" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Live Code Challenge:** "Build a 3-layer network for Palawan classification" (10 min)</span>
<span id="cb9-1310"><a href="#cb9-1310" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Architecture Design:** "Sketch CNN for building detection task" (5 min)</span>
<span id="cb9-1311"><a href="#cb9-1311" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Group Discussion:** "What EO problem in YOUR organization could use CNNs?" (10 min)</span>
<span id="cb9-1312"><a href="#cb9-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1313"><a href="#cb9-1313" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb9-1314"><a href="#cb9-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1315"><a href="#cb9-1315" aria-hidden="true" tabindex="-1"></a>**Resource Check (Before Session):**</span>
<span id="cb9-1316"><a href="#cb9-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-1317"><a href="#cb9-1317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Test both Colab notebooks execute without errors</span>
<span id="cb9-1318"><a href="#cb9-1318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Verify Sentinel-2 sample data loads correctly</span>
<span id="cb9-1319"><a href="#cb9-1319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Pre-download datasets to Google Drive (backup)</span>
<span id="cb9-1320"><a href="#cb9-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Have architecture diagrams ready (slides or drawn)</span>
<span id="cb9-1321"><a href="#cb9-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Queue up CNN Explainer website for demos</span>
<span id="cb9-1322"><a href="#cb9-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Test video/screen sharing for visualizations</span>
<span id="cb9-1323"><a href="#cb9-1323" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:info@philsa.gov.ph">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>