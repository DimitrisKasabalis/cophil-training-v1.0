{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 Theory: Understanding Random Forest for Earth Observation\n",
    "\n",
    "**CoPhil 4-Day Advanced Online Training**  \n",
    "**DAY 2 - Session 1: Supervised Machine Learning - Part 1**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand Decision Trees**: Explain how a single decision tree makes predictions through recursive splitting\n",
    "2. **Grasp Ensemble Learning**: Describe how Random Forest combines multiple trees through bootstrap sampling and random feature selection\n",
    "3. **Interpret Feature Importance**: Analyze which spectral bands or derived indices contribute most to classification\n",
    "4. **Evaluate Model Performance**: Read and interpret confusion matrices to assess classification accuracy\n",
    "5. **Apply to EO Context**: Connect these concepts to satellite image classification tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Why Random Forest for Earth Observation?\n",
    "\n",
    "Random Forest is one of the most popular algorithms for land cover classification because:\n",
    "\n",
    "- **Handles high-dimensional data**: Works well with many spectral bands (Sentinel-2 has 13 bands)\n",
    "- **Robust to overfitting**: Ensemble approach reduces variance\n",
    "- **Feature importance**: Reveals which bands are most informative\n",
    "- **No feature scaling required**: Unlike neural networks\n",
    "- **Fast training**: Efficient even with large datasets\n",
    "- **Interpretable**: Can visualize decision rules\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated Time**: 70 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Introduction and Setup (5 minutes)\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for machine learning\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"colorblind\")  # Color-blind friendly palette\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "print(f\"‚úì Random state set to: {RANDOM_STATE}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")\n",
    "print(f\"‚úì Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## B. Decision Trees Interactive Demo (15 minutes)\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a supervised learning algorithm that makes predictions by learning a series of if-then-else decision rules from data. Think of it like a flowchart:\n",
    "\n",
    "```\n",
    "Is NDVI > 0.3?\n",
    "‚îú‚îÄ Yes: Is NIR > 0.5?\n",
    "‚îÇ  ‚îú‚îÄ Yes: Forest\n",
    "‚îÇ  ‚îî‚îÄ No: Grassland\n",
    "‚îî‚îÄ No: Is SWIR < 0.2?\n",
    "   ‚îú‚îÄ Yes: Water\n",
    "   ‚îî‚îÄ No: Urban\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Root Node**: The first decision point (top of the tree)\n",
    "- **Internal Nodes**: Intermediate decision points\n",
    "- **Leaf Nodes**: Final predictions (bottom of the tree)\n",
    "- **Splitting**: How the algorithm decides which feature and threshold to use\n",
    "- **Depth**: Number of levels in the tree (deeper = more complex)\n",
    "\n",
    "### Let's Build a Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D classification dataset\n",
    "# This simulates two spectral bands (e.g., NIR and Red)\n",
    "X, y = make_moons(n_samples=200, noise=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "# Add feature names for EO context\n",
    "feature_names = ['NIR Reflectance', 'Red Reflectance']\n",
    "class_names = ['Water/Urban', 'Vegetation']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                     s=50, alpha=0.7, edgecolors='k', linewidth=0.5)\n",
    "plt.xlabel(feature_names[0], fontsize=12)\n",
    "plt.ylabel(feature_names[1], fontsize=12)\n",
    "plt.title('Training Data: Two Spectral Bands', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Class', ticks=[0, 1])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TIP: In real EO applications, each point would represent a pixel with its spectral reflectance values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Single Decision Tree\n",
    "\n",
    "Let's train a decision tree and visualize how it splits the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree with limited depth\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = tree.score(X, y)\n",
    "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Tree Depth: {tree.get_depth()}\")\n",
    "print(f\"Number of Leaves: {tree.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a 2D classification problem.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained classifier\n",
    "    X : array-like, shape (n_samples, 2)\n",
    "    y : array-like, shape (n_samples,)\n",
    "    title : str\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                         s=50, alpha=0.8, edgecolors='k', linewidth=0.5)\n",
    "    plt.xlabel(feature_names[0], fontsize=12)\n",
    "    plt.ylabel(feature_names[1], fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(scatter, label='Class', ticks=[0, 1])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(tree, X, y, \n",
    "                      title=\"Decision Tree: How It Splits the Feature Space\")\n",
    "\n",
    "print(\"\\nüí° TIP: Notice the rectangular decision boundaries. Trees can only make\")\n",
    "print(\"   axis-aligned splits (e.g., 'NIR > 0.5'), not diagonal lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Tree Structure\n",
    "\n",
    "Let's look inside the tree to see the actual decision rules it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree, \n",
    "         feature_names=feature_names,\n",
    "         class_names=class_names,\n",
    "         filled=True,\n",
    "         rounded=True,\n",
    "         fontsize=10)\n",
    "plt.title('Decision Tree Structure', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to Read This Tree:\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(\"‚Ä¢ Each box is a node with a decision rule (e.g., 'NIR <= 0.5')\")\n",
    "print(\"‚Ä¢ 'gini' measures impurity (0 = pure, 0.5 = mixed)\")\n",
    "print(\"‚Ä¢ 'samples' shows how many training points reach this node\")\n",
    "print(\"‚Ä¢ 'value' shows class distribution [class 0, class 1]\")\n",
    "print(\"‚Ä¢ Color intensity indicates class majority (darker = more confident)\")\n",
    "print(\"‚Ä¢ Leaf nodes (bottom) make the final prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Interactive Exercise: Effect of Tree Depth\n",
    "\n",
    "**Task**: Experiment with different `max_depth` values and observe how the decision boundary changes.\n",
    "\n",
    "**Questions to consider**:\n",
    "1. What happens with `max_depth=1` (a \"decision stump\")?\n",
    "2. What happens with `max_depth=10` (very deep tree)?\n",
    "3. Which depth seems to balance simplicity and accuracy?\n",
    "4. Can you identify overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different max_depth values\n",
    "# Try: max_depth = 1, 2, 5, 10, None (unlimited)\n",
    "\n",
    "max_depth_to_test = 1  # TODO: Change this value\n",
    "\n",
    "tree_experiment = DecisionTreeClassifier(max_depth=max_depth_to_test, \n",
    "                                        random_state=RANDOM_STATE)\n",
    "tree_experiment.fit(X, y)\n",
    "\n",
    "accuracy = tree_experiment.score(X, y)\n",
    "print(f\"Max Depth: {max_depth_to_test}\")\n",
    "print(f\"Training Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Actual Tree Depth: {tree_experiment.get_depth()}\")\n",
    "print(f\"Number of Leaves: {tree_experiment.get_n_leaves()}\")\n",
    "\n",
    "plot_decision_boundary(tree_experiment, X, y, \n",
    "                      title=f\"Decision Tree with max_depth={max_depth_to_test}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è COMMON MISTAKE: Setting max_depth=None can lead to overfitting!\")\n",
    "print(\"   The tree will memorize training data instead of learning patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## C. Random Forest Voting Mechanism (15 minutes)\n",
    "\n",
    "### The Power of Ensemble Learning\n",
    "\n",
    "A single decision tree can be unstable:\n",
    "- Small changes in data can lead to completely different trees\n",
    "- Prone to overfitting (memorizing training data)\n",
    "- High variance in predictions\n",
    "\n",
    "**Random Forest** solves this by combining many trees:\n",
    "\n",
    "1. **Bootstrap Sampling**: Each tree trains on a random subset of data (sampling with replacement)\n",
    "2. **Random Feature Selection**: Each split only considers a random subset of features\n",
    "3. **Majority Voting**: Final prediction is the class chosen by most trees\n",
    "\n",
    "**Analogy**: Instead of asking one expert (one tree), you ask a committee of experts (forest) and take a vote. This \"wisdom of the crowd\" is more robust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest with just 5 trees (for visualization)\n",
    "n_trees = 5\n",
    "rf_small = RandomForestClassifier(n_estimators=n_trees, \n",
    "                                 max_depth=3,\n",
    "                                 random_state=RANDOM_STATE)\n",
    "rf_small.fit(X, y)\n",
    "\n",
    "rf_accuracy = rf_small.score(X, y)\n",
    "print(f\"Random Forest Accuracy (5 trees): {rf_accuracy:.3f}\")\n",
    "print(f\"Single Tree Accuracy (from before): {train_accuracy:.3f}\")\n",
    "print(f\"\\nImprovement: {rf_accuracy - train_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Individual Trees in the Forest\n",
    "\n",
    "Let's see how each tree makes different decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for each individual tree\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot each individual tree\n",
    "for idx, tree in enumerate(rf_small.estimators_):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "              s=30, alpha=0.6, edgecolors='k', linewidth=0.3)\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "    ax.set_title(f'Tree {idx + 1}', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot the ensemble (Random Forest)\n",
    "ax = axes[5]\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = rf_small.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "          s=30, alpha=0.6, edgecolors='k', linewidth=0.3)\n",
    "ax.set_xlabel(feature_names[0])\n",
    "ax.set_ylabel(feature_names[1])\n",
    "ax.set_title('Random Forest (Ensemble)', fontweight='bold', color='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Individual Trees vs. Ensemble Decision', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TIP: Notice how each tree is slightly different due to bootstrap\")\n",
    "print(\"   sampling and random feature selection. The ensemble smooths out\")\n",
    "print(\"   individual errors and creates more stable boundaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Voting Confidence\n",
    "\n",
    "Random Forest can provide prediction probabilities based on the proportion of trees voting for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict probabilities for class 1 (Vegetation)\n",
    "Z_proba = rf_small.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z_proba = Z_proba.reshape(xx.shape)\n",
    "\n",
    "# Plot confidence\n",
    "plt.figure(figsize=(12, 7))\n",
    "contour = plt.contourf(xx, yy, Z_proba, levels=20, cmap='RdYlGn', alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "           s=50, alpha=0.7, edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(contour, label='Confidence for Vegetation Class')\n",
    "plt.xlabel(feature_names[0], fontsize=12)\n",
    "plt.ylabel(feature_names[1], fontsize=12)\n",
    "plt.title('Random Forest Prediction Confidence\\n(Based on Voting Proportions)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpreting Confidence:\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(\"‚Ä¢ Green (high values): Most trees vote for 'Vegetation'\")\n",
    "print(\"‚Ä¢ Red (low values): Most trees vote for 'Water/Urban'\")\n",
    "print(\"‚Ä¢ Yellow (middle values): Trees are uncertain (mixed votes)\")\n",
    "print(\"\\nüí° TIP: Low confidence regions often indicate:\")\n",
    "print(\"   - Class boundaries\")\n",
    "print(\"   - Mixed pixels (in EO context)\")\n",
    "print(\"   - Need for more training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Interactive Exercise: Effect of Number of Trees\n",
    "\n",
    "**Task**: Test how the number of trees affects model stability and accuracy.\n",
    "\n",
    "**Hypothesis**: More trees ‚Üí more stable predictions, but diminishing returns after a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different numbers of trees\n",
    "tree_counts = [1, 5, 10, 50, 100, 200]\n",
    "accuracies = []\n",
    "\n",
    "for n in tree_counts:\n",
    "    # TODO: Create and train a Random Forest with n trees\n",
    "    rf = RandomForestClassifier(n_estimators=n, \n",
    "                               max_depth=3,\n",
    "                               random_state=RANDOM_STATE)\n",
    "    rf.fit(X, y)\n",
    "    acc = rf.score(X, y)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"n_estimators={n:3d} ‚Üí Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot accuracy vs. number of trees\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tree_counts, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Trees', fontsize=12)\n",
    "plt.ylabel('Training Accuracy', fontsize=12)\n",
    "plt.title('Effect of Ensemble Size on Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observation: Accuracy stabilizes after ~50-100 trees.\")\n",
    "print(\"   In practice, 100-500 trees is common for EO applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## D. Feature Importance Analysis (10 minutes)\n",
    "\n",
    "### Why Feature Importance Matters in EO\n",
    "\n",
    "Feature importance tells us:\n",
    "- Which spectral bands contribute most to classification\n",
    "- Whether derived indices (NDVI, NDWI) are valuable\n",
    "- If certain features are redundant\n",
    "- How to optimize future data collection\n",
    "\n",
    "**How Random Forest Calculates Importance**:\n",
    "- Measures how much each feature decreases impurity (Gini or entropy)\n",
    "- Averaged across all trees in the forest\n",
    "- Higher values = more important for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset mimicking Sentinel-2 spectral bands\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Simulate 1000 pixels with 8 \"spectral bands\"\n",
    "n_samples = 1000\n",
    "n_features = 8\n",
    "\n",
    "# Feature names mimicking Sentinel-2 bands and indices\n",
    "eo_feature_names = [\n",
    "    'Blue (B2)',\n",
    "    'Green (B3)',\n",
    "    'Red (B4)',\n",
    "    'NIR (B8)',\n",
    "    'SWIR1 (B11)',\n",
    "    'SWIR2 (B12)',\n",
    "    'NDVI',\n",
    "    'NDWI'\n",
    "]\n",
    "\n",
    "# Create synthetic data with realistic patterns\n",
    "# Class 0: Water (low NIR, high Blue, high NDWI)\n",
    "# Class 1: Vegetation (high NIR, low Red, high NDVI)\n",
    "# Class 2: Urban (moderate all, low NDVI, low NDWI)\n",
    "\n",
    "X_eo = np.random.rand(n_samples, n_features)\n",
    "y_eo = np.random.choice([0, 1, 2], size=n_samples)\n",
    "\n",
    "# Add class-specific patterns\n",
    "for i in range(n_samples):\n",
    "    if y_eo[i] == 0:  # Water\n",
    "        X_eo[i, 0] += 0.3  # Higher Blue\n",
    "        X_eo[i, 3] -= 0.3  # Lower NIR\n",
    "        X_eo[i, 7] += 0.4  # Higher NDWI\n",
    "    elif y_eo[i] == 1:  # Vegetation\n",
    "        X_eo[i, 3] += 0.5  # Higher NIR\n",
    "        X_eo[i, 2] -= 0.2  # Lower Red\n",
    "        X_eo[i, 6] += 0.5  # Higher NDVI\n",
    "    else:  # Urban\n",
    "        X_eo[i, 4] += 0.2  # Higher SWIR1\n",
    "        X_eo[i, 5] += 0.2  # Higher SWIR2\n",
    "\n",
    "# Clip to [0, 1] range\n",
    "X_eo = np.clip(X_eo, 0, 1)\n",
    "\n",
    "print(f\"EO Dataset shape: {X_eo.shape}\")\n",
    "print(f\"Features: {eo_feature_names}\")\n",
    "print(f\"Classes: 0=Water, 1=Vegetation, 2=Urban\")\n",
    "print(f\"Class distribution: {np.bincount(y_eo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on EO-like data\n",
    "rf_eo = RandomForestClassifier(n_estimators=100, \n",
    "                              max_depth=10,\n",
    "                              random_state=RANDOM_STATE)\n",
    "rf_eo.fit(X_eo, y_eo)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf_eo.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort descending\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"{i+1}. {eo_feature_names[idx]:15s}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "plt.figure(figsize=(12, 7))\n",
    "bars = plt.barh(range(len(importances)), importances[indices], align='center')\n",
    "\n",
    "# Color bars by importance\n",
    "colors = plt.cm.viridis(importances[indices] / importances.max())\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "plt.yticks(range(len(importances)), [eo_feature_names[i] for i in indices])\n",
    "plt.xlabel('Importance (Mean Decrease in Impurity)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance for Land Cover Classification', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TIP: High importance doesn't always mean causation!\")\n",
    "print(\"   - NDVI is derived from NIR and Red, so they're correlated\")\n",
    "print(\"   - Consider domain knowledge alongside feature importance\")\n",
    "print(\"   - Importance can be unstable with correlated features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise: Interpret Feature Importance\n",
    "\n",
    "**Questions**:\n",
    "1. Which feature is most important? Why might this be?\n",
    "2. Are the derived indices (NDVI, NDWI) more or less important than raw bands?\n",
    "3. Which features could potentially be removed to simplify the model?\n",
    "4. How does this align with your knowledge of land cover spectral signatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers here** (double-click to edit):\n",
    "\n",
    "1. Most important feature:\n",
    "   - TODO: Write your observation\n",
    "\n",
    "2. Derived indices vs. raw bands:\n",
    "   - TODO: Write your analysis\n",
    "\n",
    "3. Features that could be removed:\n",
    "   - TODO: Write your suggestions\n",
    "\n",
    "4. Alignment with spectral signatures:\n",
    "   - TODO: Write your interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E. Confusion Matrix Interpretation (15 minutes)\n",
    "\n",
    "### Why Confusion Matrix?\n",
    "\n",
    "Overall accuracy can be misleading! Consider:\n",
    "- Dataset: 95% Forest, 5% Mangrove\n",
    "- Model: Predicts everything as Forest\n",
    "- Accuracy: 95% (sounds great!)\n",
    "- Problem: Completely missed mangroves!\n",
    "\n",
    "**Confusion Matrix** reveals:\n",
    "- Which classes are well-predicted\n",
    "- Which classes are confused with each other\n",
    "- Class-specific performance (precision, recall)\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "- **Precision (User's Accuracy)**: Of all pixels predicted as class X, how many are actually class X?\n",
    "  - Formula: TP / (TP + FP)\n",
    "  - Important when false positives are costly\n",
    "\n",
    "- **Recall (Producer's Accuracy)**: Of all actual class X pixels, how many did we correctly identify?\n",
    "  - Formula: TP / (TP + FN)\n",
    "  - Important when false negatives are costly\n",
    "\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "  - Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "  - Balances both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_eo, y_eo, test_size=0.3, random_state=RANDOM_STATE, stratify=y_eo\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "print(\"\\nüí° TIP: We use stratified split to maintain class proportions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_final = RandomForestClassifier(n_estimators=100, \n",
    "                                 max_depth=10,\n",
    "                                 random_state=RANDOM_STATE)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_final.predict(X_test)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Overall Test Accuracy: {overall_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "class_labels = ['Water', 'Vegetation', 'Urban']\n",
    "\n",
    "print(\"Confusion Matrix (raw counts):\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(cm)\n",
    "print(\"\\nRows = Actual class, Columns = Predicted class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_labels, \n",
    "           yticklabels=class_labels,\n",
    "           cbar_kws={'label': 'Number of Samples'},\n",
    "           linewidths=1, linecolor='gray')\n",
    "plt.xlabel('Predicted Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual Class', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix: Land Cover Classification', \n",
    "         fontsize=14, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to Read This Matrix:\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(\"‚Ä¢ Diagonal (top-left to bottom-right): Correct predictions\")\n",
    "print(\"‚Ä¢ Off-diagonal: Confusion between classes\")\n",
    "print(\"‚Ä¢ Dark blue cells indicate high counts\")\n",
    "print(\"\\nüí° TIP: Look for patterns in confusion:\")\n",
    "print(\"   - Are certain class pairs often confused?\")\n",
    "print(\"   - Do confusions make spectral sense?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized confusion matrix (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n",
    "           xticklabels=class_labels, \n",
    "           yticklabels=class_labels,\n",
    "           vmin=0, vmax=1,\n",
    "           cbar_kws={'label': 'Percentage'},\n",
    "           linewidths=1, linecolor='gray')\n",
    "plt.xlabel('Predicted Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual Class', fontsize=12, fontweight='bold')\n",
    "plt.title('Normalized Confusion Matrix (Row Percentages)', \n",
    "         fontsize=14, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TIP: Normalized matrix shows recall (producer's accuracy) for each class.\")\n",
    "print(\"   Diagonal values are the percentage correctly classified for each class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"‚îÅ\" * 80)\n",
    "report = classification_report(y_test, y_pred, \n",
    "                              target_names=class_labels,\n",
    "                              digits=3)\n",
    "print(report)\n",
    "\n",
    "print(\"\\nMetric Definitions:\")\n",
    "print(\"‚îÅ\" * 80)\n",
    "print(\"‚Ä¢ Precision (User's Accuracy): TP / (TP + FP)\")\n",
    "print(\"  ‚Üí Of predictions for this class, how many were correct?\")\n",
    "print(\"  ‚Üí Important when false alarms are costly\")\n",
    "print(\"\")\n",
    "print(\"‚Ä¢ Recall (Producer's Accuracy): TP / (TP + FN)\")\n",
    "print(\"  ‚Üí Of actual samples of this class, how many were found?\")\n",
    "print(\"  ‚Üí Important when missing instances is costly\")\n",
    "print(\"\")\n",
    "print(\"‚Ä¢ F1-Score: 2 √ó (Precision √ó Recall) / (Precision + Recall)\")\n",
    "print(\"  ‚Üí Harmonic mean balancing precision and recall\")\n",
    "print(\"\")\n",
    "print(\"‚Ä¢ Support: Number of actual samples in test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Create DataFrame for easier plotting\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "}, index=class_labels)\n",
    "\n",
    "# Plot\n",
    "ax = metrics_df.plot(kind='bar', figsize=(12, 7), width=0.8)\n",
    "plt.xlabel('Land Cover Class', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim([0, 1.05])\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TIP: In EO applications, different thresholds matter:\")\n",
    "print(\"   - Disaster mapping: High recall for affected areas (don't miss damage)\")\n",
    "print(\"   - Urban planning: High precision for built-up (avoid false alarms)\")\n",
    "print(\"   - Balanced: Use F1-score for overall assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise: Confusion Analysis\n",
    "\n",
    "**Task**: Analyze the confusion matrix and answer these questions:\n",
    "\n",
    "1. Which class has the highest recall (producer's accuracy)?\n",
    "2. Which class has the lowest precision (user's accuracy)?\n",
    "3. Which two classes are most often confused with each other?\n",
    "4. Why might this confusion occur from a spectral perspective?\n",
    "5. What could you do to improve classification of the weakest class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers here** (double-click to edit):\n",
    "\n",
    "1. Highest recall class:\n",
    "   - TODO: Identify and explain\n",
    "\n",
    "2. Lowest precision class:\n",
    "   - TODO: Identify and explain\n",
    "\n",
    "3. Most confused class pair:\n",
    "   - TODO: Identify the pair\n",
    "\n",
    "4. Spectral reason for confusion:\n",
    "   - TODO: Explain using spectral signature knowledge\n",
    "\n",
    "5. Improvement strategies:\n",
    "   - TODO: List 2-3 practical approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## F. Concept Check Quiz (10 minutes)\n",
    "\n",
    "Test your understanding of Random Forest concepts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Decision Tree Splitting\n",
    "\n",
    "**Q**: How does a decision tree decide where to split at each node?\n",
    "\n",
    "A) Randomly selects a feature and threshold  \n",
    "B) Uses the feature and threshold that maximizes information gain (or minimizes impurity)  \n",
    "C) Always splits at the median value of each feature  \n",
    "D) Splits based on alphabetical order of feature names\n",
    "\n",
    "**Your answer**: TODO (A, B, C, or D)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "Decision trees evaluate all possible splits and choose the one that best separates classes (maximizes information gain or minimizes Gini impurity). This greedy approach finds locally optimal splits at each node.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Bootstrap Sampling\n",
    "\n",
    "**Q**: In Random Forest, what is bootstrap sampling?\n",
    "\n",
    "A) Sampling pixels only from the edges of images  \n",
    "B) Sampling with replacement to create training subsets for each tree  \n",
    "C) Sampling only the most important features  \n",
    "D) Sampling validation data separately from training data\n",
    "\n",
    "**Your answer**: TODO (A, B, C, or D)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "Bootstrap sampling means randomly selecting samples WITH replacement. Each tree gets a different random subset of the training data (approximately 63.2% unique samples), which introduces diversity and reduces correlation between trees.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Random Feature Selection\n",
    "\n",
    "**Q**: At each split in a Random Forest tree, what does \"random feature selection\" mean?\n",
    "\n",
    "A) All features are considered for splitting  \n",
    "B) Features are selected in alphabetical order  \n",
    "C) Only a random subset of features is considered (typically ‚àön or log‚ÇÇn)  \n",
    "D) The most important feature is always selected\n",
    "\n",
    "**Your answer**: TODO (A, B, C, or D)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: C**\n",
    "\n",
    "At each split, Random Forest only considers a random subset of features (controlled by `max_features` parameter). Default is ‚àön for classification. This decorrelates trees and prevents dominant features from being used in every tree.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Feature Importance Interpretation\n",
    "\n",
    "**Q**: You're classifying land cover and find that NDVI has the highest feature importance. What should you conclude?\n",
    "\n",
    "A) NDVI is the only feature needed; remove all others  \n",
    "B) NDVI contributes most to reducing impurity, but other features may still be valuable  \n",
    "C) NDVI causes the land cover types (causal relationship)  \n",
    "D) All other features are completely irrelevant\n",
    "\n",
    "**Your answer**: TODO (A, B, C, or D)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "High importance means NDVI is most useful for discrimination, but:\n",
    "- Other features may capture complementary information\n",
    "- Importance doesn't imply causation\n",
    "- Correlated features share importance\n",
    "- Context and domain knowledge matter!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Confusion Matrix - Precision vs. Recall\n",
    "\n",
    "**Scenario**: You're mapping forest fire damage. The confusion matrix shows:\n",
    "- Actual Burned: 100 pixels\n",
    "- Predicted as Burned: 150 pixels\n",
    "- Correctly identified Burned: 90 pixels\n",
    "\n",
    "**Q**: Calculate precision and recall for the \"Burned\" class. Which is more important for this application?\n",
    "\n",
    "**Your calculations**:\n",
    "- Precision = TODO (show calculation)\n",
    "- Recall = TODO (show calculation)\n",
    "- More important: TODO (Precision or Recall, and why?)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answers**:\n",
    "- **Precision** = 90 / 150 = 0.60 (60%)\n",
    "  - Of pixels predicted as burned, 60% actually were\n",
    "- **Recall** = 90 / 100 = 0.90 (90%)\n",
    "  - Of actual burned pixels, we found 90%\n",
    "\n",
    "**More Important**: **Recall**\n",
    "\n",
    "For fire damage assessment:\n",
    "- **High recall** is critical: We don't want to miss burned areas (false negatives could delay aid)\n",
    "- Lower precision is acceptable: False alarms can be verified with field checks\n",
    "- It's better to overestimate damage than underestimate\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Overfitting in Random Forest\n",
    "\n",
    "**Q**: Which scenario is MOST likely to cause overfitting in Random Forest?\n",
    "\n",
    "A) Using 100 trees instead of 10  \n",
    "B) Setting max_depth=None (unlimited depth)  \n",
    "C) Using bootstrap sampling  \n",
    "D) Using random feature selection\n",
    "\n",
    "**Your answer**: TODO (A, B, C, or D)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Unlimited depth** allows trees to grow until leaves are pure (or nearly pure), memorizing training data. Signs of overfitting:\n",
    "- Very high training accuracy (>99%)\n",
    "- Much lower test accuracy\n",
    "- Overly complex decision boundaries\n",
    "\n",
    "**Prevention**:\n",
    "- Set `max_depth` (e.g., 10-20)\n",
    "- Set `min_samples_split` (e.g., 5-10)\n",
    "- Set `min_samples_leaf` (e.g., 2-5)\n",
    "\n",
    "Note: More trees (A) actually reduces overfitting! Bootstrap (C) and feature selection (D) also help prevent it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Decision Trees\n",
    "- Learn hierarchical decision rules through recursive splitting\n",
    "- Create axis-aligned decision boundaries\n",
    "- Prone to overfitting if too deep\n",
    "- Easy to interpret and visualize\n",
    "\n",
    "### Random Forest Ensemble\n",
    "- Combines many trees to reduce variance and improve stability\n",
    "- Uses bootstrap sampling (bagging) for training diversity\n",
    "- Uses random feature selection to decorrelate trees\n",
    "- Final prediction by majority voting (classification) or averaging (regression)\n",
    "- More robust than single trees, less prone to overfitting\n",
    "\n",
    "### Feature Importance\n",
    "- Measures contribution of each feature to reducing impurity\n",
    "- Helps identify most informative spectral bands/indices\n",
    "- Useful for feature selection and model interpretation\n",
    "- Should be interpreted with domain knowledge\n",
    "- Can be unstable with correlated features\n",
    "\n",
    "### Confusion Matrix & Metrics\n",
    "- Overall accuracy can hide class-specific problems\n",
    "- **Precision** (user's accuracy): Reliability of positive predictions\n",
    "- **Recall** (producer's accuracy): Completeness of detection\n",
    "- **F1-score**: Harmonic mean balancing precision and recall\n",
    "- Choice of metric depends on application cost (false positives vs. false negatives)\n",
    "\n",
    "### For Earth Observation\n",
    "- Random Forest works well with multi-spectral data\n",
    "- No feature scaling needed (unlike neural networks)\n",
    "- Feature importance reveals spectral signature insights\n",
    "- Confusion patterns often reflect spectral similarity\n",
    "- Fast training enables rapid iteration\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the **Hands-On Session**, you will:\n",
    "1. Load real Sentinel-2 data for Palawan, Philippines\n",
    "2. Extract training samples from land cover polygons\n",
    "3. Train Random Forest for multi-class land cover classification\n",
    "4. Optimize hyperparameters (n_estimators, max_depth, etc.)\n",
    "5. Generate wall-to-wall land cover maps\n",
    "6. Validate results and interpret errors\n",
    "\n",
    "**Prepare by reviewing**:\n",
    "- Sentinel-2 band characteristics (B2, B3, B4, B8, B11, B12)\n",
    "- Philippine land cover types (forest, mangrove, agriculture, urban, water)\n",
    "- Google Earth Engine Python API basics\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5-32.\n",
    "2. Belgiu, M., & DrƒÉgu≈£, L. (2016). Random forest in remote sensing: A review of applications and future directions. *ISPRS Journal of Photogrammetry and Remote Sensing*, 114, 24-31.\n",
    "3. Scikit-learn Documentation: [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "4. ESA Sentinel-2 User Handbook: [https://sentinels.copernicus.eu/documents/247904/685211/Sentinel-2_User_Handbook](https://sentinels.copernicus.eu/documents/247904/685211/Sentinel-2_User_Handbook)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Theory Notebook**\n",
    "\n",
    "*Developed for CoPhil 4-Day Advanced Online Training on AI/ML for Earth Observation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
