{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f97b15",
   "metadata": {},
   "source": [
    "# Session 3: Deep Learning & CNN Theory - Interactive Notebook\n",
    "\n",
    "## From Random Forest to Neural Networks\n",
    "\n",
    "**Duration:** 90 minutes | **Type:** Interactive Theory | **Difficulty:** Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ✅ Build a perceptron from scratch using NumPy\n",
    "2. ✅ Understand and visualize activation functions\n",
    "3. ✅ Implement forward propagation manually\n",
    "4. ✅ Apply convolution operations to Sentinel-2 imagery\n",
    "5. ✅ Explore pre-trained CNN architectures\n",
    "6. ✅ Visualize learned feature maps\n",
    "7. ✅ Understand the transition from RF to deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Notebook Structure\n",
    "\n",
    "| Part | Topic | Duration |\n",
    "|------|-------|----------|\n",
    "| **1** | Build Perceptron from Scratch | 20 min |\n",
    "| **2** | Activation Functions | 15 min |\n",
    "| **3** | Simple Neural Network | 20 min |\n",
    "| **4** | Convolution Operations | 20 min |\n",
    "| **5** | CNN Architecture Exploration | 15 min |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Key Concepts Preview\n",
    "\n",
    "**What you already know (from Sessions 1-2):**\n",
    "- Random Forest classification\n",
    "- Feature engineering (GLCM, NDVI, temporal)\n",
    "- Accuracy assessment\n",
    "- Palawan land cover mapping\n",
    "\n",
    "**What you'll learn today:**\n",
    "- How neural networks learn from data\n",
    "- Why convolution is perfect for images\n",
    "- How CNNs build feature hierarchies\n",
    "- When to use CNNs vs Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive in! 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2995",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup and Imports\n",
    "\n",
    "First, let's import the libraries we'll need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02430d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage, signal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f623e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Build a Perceptron from Scratch (20 minutes)\n",
    "\n",
    "## What is a Perceptron?\n",
    "\n",
    "A **perceptron** is the simplest artificial neuron. It:\n",
    "1. Takes multiple inputs (x₁, x₂, ..., xₙ)\n",
    "2. Multiplies each by a weight (w₁, w₂, ..., wₙ)\n",
    "3. Adds a bias term (b)\n",
    "4. Applies an activation function\n",
    "5. Outputs a prediction\n",
    "\n",
    "**Mathematical formula:**\n",
    "```\n",
    "z = (w₁ × x₁) + (w₂ × x₂) + ... + (wₙ × xₙ) + b\n",
    "output = activation(z)\n",
    "```\n",
    "\n",
    "**Analogy for EO:**\n",
    "Think of classifying a pixel as \"forest\" or \"not forest\":\n",
    "- x₁ = NDVI value\n",
    "- x₂ = texture measure\n",
    "- x₃ = elevation\n",
    "- Weights determine how important each feature is\n",
    "- Output: probability of being forest\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1: Implement the Perceptron Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Simple perceptron implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize perceptron with random weights\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_inputs : int\n",
    "            Number of input features\n",
    "        learning_rate : float\n",
    "            Step size for weight updates\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly (small values)\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Track training history\n",
    "        self.errors = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: σ(z) = 1 / (1 + e^(-z))\n",
    "        Maps any value to range (0, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array, shape (n_samples,)\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        # Calculate weighted sum\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation\n",
    "        probabilities = self.sigmoid(z)\n",
    "        \n",
    "        # Convert to binary (threshold at 0.5)\n",
    "        predictions = (probabilities >= 0.5).astype(int)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"\n",
    "        Train perceptron using gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels (0 or 1)\n",
    "        epochs : int\n",
    "            Number of training iterations\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self.sigmoid(z)\n",
    "            \n",
    "            # Calculate error\n",
    "            errors = y - predictions\n",
    "            \n",
    "            # Update weights (gradient descent)\n",
    "            self.weights += self.learning_rate * np.dot(X.T, errors)\n",
    "            self.bias += self.learning_rate * np.sum(errors)\n",
    "            \n",
    "            # Track mean squared error\n",
    "            mse = np.mean(errors ** 2)\n",
    "            self.errors.append(mse)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                accuracy = np.mean(self.predict(X) == y) * 100\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - MSE: {mse:.4f} - Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "print(\"✓ Perceptron class defined\")\n",
    "print(\"  Methods: __init__, sigmoid, predict, train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9774c6a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2: Generate Simple Training Data\n",
    "\n",
    "Let's create a toy dataset that mimics forest classification:\n",
    "- **Feature 1:** NDVI (high for forest)\n",
    "- **Feature 2:** Texture contrast (medium for forest)\n",
    "- **Label:** Forest (1) or Not Forest (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic \"forest\" vs \"non-forest\" data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Forest: high NDVI (0.6-0.9), medium texture (20-50)\n",
    "n_forest = 50\n",
    "forest_ndvi = np.random.uniform(0.6, 0.9, n_forest)\n",
    "forest_texture = np.random.uniform(20, 50, n_forest)\n",
    "forest_data = np.column_stack([forest_ndvi, forest_texture])\n",
    "forest_labels = np.ones(n_forest)\n",
    "\n",
    "# Non-forest: low NDVI (0.1-0.4), high texture (40-80)\n",
    "n_non_forest = 50\n",
    "non_forest_ndvi = np.random.uniform(0.1, 0.4, n_non_forest)\n",
    "non_forest_texture = np.random.uniform(40, 80, n_non_forest)\n",
    "non_forest_data = np.column_stack([non_forest_ndvi, non_forest_texture])\n",
    "non_forest_labels = np.zeros(n_non_forest)\n",
    "\n",
    "# Combine datasets\n",
    "X_train = np.vstack([forest_data, non_forest_data])\n",
    "y_train = np.concatenate([forest_labels, non_forest_labels])\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Forest (1): {np.sum(y_train == 1)} samples\")\n",
    "print(f\"  Non-forest (0): {np.sum(y_train == 0)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad475fa9",
   "metadata": {},
   "source": [
    "### Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b91d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Separate classes for plotting\n",
    "forest_mask = y_train == 1\n",
    "non_forest_mask = y_train == 0\n",
    "\n",
    "ax.scatter(X_train[forest_mask, 0], X_train[forest_mask, 1], \n",
    "           c='darkgreen', s=100, alpha=0.6, edgecolors='black', \n",
    "           label='Forest', marker='o')\n",
    "ax.scatter(X_train[non_forest_mask, 0], X_train[non_forest_mask, 1], \n",
    "           c='orange', s=100, alpha=0.6, edgecolors='black', \n",
    "           label='Non-Forest', marker='s')\n",
    "\n",
    "ax.set_xlabel('NDVI (Normalized Difference Vegetation Index)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Texture Contrast', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Forest vs Non-Forest Training Data', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Training data visualized\")\n",
    "print(\"  Notice: Forest = high NDVI, moderate texture\")\n",
    "print(\"          Non-forest = low NDVI, high texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f4a912",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3: Train the Perceptron\n",
    "\n",
    "Now let's train our perceptron to classify forest vs non-forest!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train perceptron\n",
    "print(\"Training perceptron...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "perceptron = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "perceptron.train(X_train, y_train, epochs=100)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# Final accuracy\n",
    "final_predictions = perceptron.predict(X_train)\n",
    "final_accuracy = np.mean(final_predictions == y_train) * 100\n",
    "print(f\"\\nFinal Training Accuracy: {final_accuracy:.1f}%\")\n",
    "\n",
    "print(f\"\\nLearned Weights:\")\n",
    "print(f\"  NDVI weight: {perceptron.weights[0]:.4f}\")\n",
    "print(f\"  Texture weight: {perceptron.weights[1]:.4f}\")\n",
    "print(f\"  Bias: {perceptron.bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c9bf0",
   "metadata": {},
   "source": [
    "### Visualize Learning Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Error over time\n",
    "ax1.plot(perceptron.errors, linewidth=2, color='darkred')\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Mean Squared Error', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Learning Curve: Error Decreases Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "# Create mesh for decision boundary\n",
    "x_min, x_max = X_train[:, 0].min() - 0.1, X_train[:, 0].max() + 0.1\n",
    "y_min, y_max = X_train[:, 1].min() - 5, X_train[:, 1].max() + 5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict for mesh\n",
    "Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions\n",
    "ax2.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlGn', levels=[0, 0.5, 1])\n",
    "ax2.contour(xx, yy, Z, colors='black', linewidths=2, levels=[0.5])\n",
    "\n",
    "# Plot training points\n",
    "ax2.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "            c='darkgreen', s=100, alpha=0.8, edgecolors='black', label='Forest')\n",
    "ax2.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "            c='orange', s=100, alpha=0.8, edgecolors='black', label='Non-Forest')\n",
    "\n",
    "ax2.set_xlabel('NDVI', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Texture Contrast', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Decision Boundary Learned by Perceptron', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Perceptron successfully learned to separate forest from non-forest!\")\n",
    "print(\"  The black line shows the decision boundary\")\n",
    "print(\"  Green region = predicted as forest\")\n",
    "print(\"  Red region = predicted as non-forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2f647",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 Key Takeaways - Part 1\n",
    "\n",
    "✅ **A perceptron is the building block** of neural networks  \n",
    "✅ **Weights determine feature importance** (like feature importance in RF)  \n",
    "✅ **Training adjusts weights** to minimize error  \n",
    "✅ **Activation functions** map outputs to desired range  \n",
    "✅ **Decision boundary** separates classes (linear for perceptron)  \n",
    "\n",
    "**Limitation:** Perceptrons can only learn linear decision boundaries. For complex patterns (like in satellite images), we need deeper networks with non-linear activations!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933acc63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Activation Functions (15 minutes)\n",
    "\n",
    "## Why Activation Functions?\n",
    "\n",
    "Without activation functions, neural networks would just be linear models (like linear regression). Activation functions introduce **non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "**Analogy:** \n",
    "- Linear model: Can only draw straight lines to separate classes\n",
    "- With activation: Can draw curves, circles, any shape!\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1: Implement Common Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd18e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid: σ(x) = 1 / (1 + e^(-x))\n",
    "    Range: (0, 1)\n",
    "    Use: Output probabilities, binary classification\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to avoid overflow\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU: f(x) = max(0, x)\n",
    "    Range: [0, ∞)\n",
    "    Use: Most popular for hidden layers\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    Range: (-1, 1)\n",
    "    Use: Hidden layers, zero-centered\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU: f(x) = x if x > 0 else alpha * x\n",
    "    Range: (-∞, ∞)\n",
    "    Use: Solves \"dying ReLU\" problem\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax: Converts vector to probability distribution\n",
    "    Use: Multi-class classification output\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "print(\"✓ Activation functions defined\")\n",
    "print(\"  Functions: sigmoid, relu, tanh, leaky_relu, softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55657702",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2: Visualize Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a828579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input range\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Calculate activations\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "y_tanh = tanh(x)\n",
    "y_leaky_relu = leaky_relu(x)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(x, y_sigmoid, linewidth=3, color='blue')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[0].set_ylabel('Output σ(z)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.5, 0.05, 'Range: (0, 1)\\nUse: Binary classification output', \n",
    "             transform=axes[0].transAxes, fontsize=10, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(x, y_relu, linewidth=3, color='red')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('ReLU (Rectified Linear Unit)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[1].set_ylabel('Output ReLU(z)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.5, 0.05, 'Range: [0, ∞)\\nUse: Hidden layers (most popular)', \n",
    "             transform=axes[1].transAxes, fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "# Tanh\n",
    "axes[2].plot(x, y_tanh, linewidth=3, color='green')\n",
    "axes[2].axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].set_title('Tanh (Hyperbolic Tangent)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[2].set_ylabel('Output tanh(z)', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].text(0.5, 0.05, 'Range: (-1, 1)\\nUse: Hidden layers (zero-centered)', \n",
    "             transform=axes[2].transAxes, fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[3].plot(x, y_leaky_relu, linewidth=3, color='purple')\n",
    "axes[3].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[3].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[3].set_title('Leaky ReLU', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[3].set_ylabel('Output Leaky ReLU(z)', fontsize=11)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "axes[3].text(0.5, 0.05, 'Range: (-∞, ∞)\\nUse: Avoids \"dying ReLU\" problem', \n",
    "             transform=axes[3].transAxes, fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='plum', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Activation functions visualized!\")\n",
    "print(\"\\n📊 Key Observations:\")\n",
    "print(\"  • Sigmoid: S-shaped curve, squashes to (0,1)\")\n",
    "print(\"  • ReLU: Simple, fast, most popular\")\n",
    "print(\"  • Tanh: Similar to sigmoid but zero-centered\")\n",
    "print(\"  • Leaky ReLU: Allows small negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2da483",
   "metadata": {},
   "source": [
    "### Compare Derivatives (Gradients)\n",
    "\n",
    "The **derivative** determines how fast the neuron learns during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb90c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Compute derivatives\n",
    "dy_sigmoid = sigmoid_derivative(x)\n",
    "dy_relu = relu_derivative(x)\n",
    "dy_tanh = tanh_derivative(x)\n",
    "\n",
    "# Plot derivatives\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x, dy_sigmoid, linewidth=3, label='Sigmoid derivative', color='blue')\n",
    "ax.plot(x, dy_relu, linewidth=3, label='ReLU derivative', color='red')\n",
    "ax.plot(x, dy_tanh, linewidth=3, label='Tanh derivative', color='green')\n",
    "\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Input (z)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Gradient (derivative)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Activation Function Derivatives (Gradients for Backpropagation)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Gradients visualized!\")\n",
    "print(\"\\n🔑 Why ReLU is Popular:\")\n",
    "print(\"  • Gradient is either 0 or 1 (simple computation)\")\n",
    "print(\"  • No vanishing gradient for x > 0\")\n",
    "print(\"  • Much faster than sigmoid/tanh\")\n",
    "print(\"\\n⚠️ Vanishing Gradient Problem:\")\n",
    "print(\"  • Sigmoid/Tanh: gradients → 0 for large |x|\")\n",
    "print(\"  • Deep networks can't learn (gradients disappear)\")\n",
    "print(\"  • ReLU solves this for positive inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c737dac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 Key Takeaways - Part 2\n",
    "\n",
    "✅ **Activation functions introduce non-linearity**  \n",
    "✅ **ReLU is the default choice** for hidden layers  \n",
    "✅ **Sigmoid/Softmax for output** layers (probabilities)  \n",
    "✅ **Derivatives matter** for learning speed  \n",
    "✅ **Vanishing gradient** is why we prefer ReLU  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f09d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Build a Simple Neural Network (20 minutes)\n",
    "\n",
    "Now let's connect multiple perceptrons to create a **multi-layer neural network**!\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input Layer (2 neurons) → Hidden Layer (4 neurons) → Output Layer (1 neuron)\n",
    "        ↓                        ↓                         ↓\n",
    "    [NDVI, Texture]         [ReLU activation]      [Sigmoid activation]\n",
    "```\n",
    "\n",
    "This is a **2-4-1 network**: 2 inputs, 4 hidden neurons, 1 output.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1: Implement Neural Network Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer neural network with one hidden layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize network with random weights\n",
    "        \"\"\"\n",
    "        # Layer 1: input → hidden\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        \n",
    "        # Layer 2: hidden → output\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)  # Hidden layer uses ReLU\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # Output layer uses Sigmoid\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backpropagation (gradient calculation)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y.reshape(-1, 1)\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * (self.z1 > 0)  # ReLU derivative\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the network\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Calculate loss (binary cross-entropy)\n",
    "            loss = -np.mean(y * np.log(predictions + 1e-8) + \n",
    "                           (1 - y) * np.log(1 - predictions + 1e-8))\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            if (epoch + 1) % 200 == 0:\n",
    "                accuracy = np.mean((predictions > 0.5).flatten() == y) * 100\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.1f}%\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"✓ Neural Network class defined\")\n",
    "print(\"  Architecture: Input → Hidden (ReLU) → Output (Sigmoid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b1727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2: Train Neural Network\n",
    "\n",
    "Let's train on the same forest/non-forest data and compare with the perceptron!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train neural network\n",
    "print(\"Training 2-layer Neural Network...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Architecture: 2 inputs → 4 hidden (ReLU) → 1 output (Sigmoid)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "nn.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# Final accuracy\n",
    "final_predictions = nn.predict(X_train)\n",
    "final_accuracy = np.mean(final_predictions == y_train) * 100\n",
    "print(f\"\\nFinal Training Accuracy: {final_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e990094",
   "metadata": {},
   "source": [
    "### Compare: Perceptron vs Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Learning curves comparison\n",
    "axes[0].plot(perceptron.errors, label='Perceptron (MSE)', linewidth=2, alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Error', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Perceptron Learning Curve', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(nn.losses, label='Neural Network (Cross-Entropy)', linewidth=2, \n",
    "             alpha=0.7, color='darkgreen')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Neural Network Learning Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundaries\n",
    "x_min, x_max = X_train[:, 0].min() - 0.1, X_train[:, 0].max() + 0.1\n",
    "y_min, y_max = X_train[:, 1].min() - 5, X_train[:, 1].max() + 5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Neural network predictions\n",
    "Z_nn = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_nn = Z_nn.reshape(xx.shape)\n",
    "\n",
    "axes[2].contourf(xx, yy, Z_nn, alpha=0.3, cmap='RdYlGn', levels=[0, 0.5, 1])\n",
    "axes[2].contour(xx, yy, Z_nn, colors='black', linewidths=2, levels=[0.5])\n",
    "axes[2].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "                c='darkgreen', s=80, alpha=0.8, edgecolors='black', label='Forest')\n",
    "axes[2].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "                c='orange', s=80, alpha=0.8, edgecolors='black', label='Non-Forest')\n",
    "axes[2].set_xlabel('NDVI', fontsize=11, fontweight='bold')\n",
    "axes[2].set_ylabel('Texture', fontsize=11, fontweight='bold')\n",
    "axes[2].set_title('Neural Network Decision Boundary', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comparison complete!\")\n",
    "print(f\"\\nPerceptron accuracy: {np.mean(perceptron.predict(X_train) == y_train)*100:.1f}%\")\n",
    "print(f\"Neural Network accuracy: {final_accuracy:.1f}%\")\n",
    "print(\"\\n💡 Neural network can learn more complex decision boundaries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13413144",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 Key Takeaways - Part 3\n",
    "\n",
    "✅ **Multi-layer networks** learn complex patterns  \n",
    "✅ **Hidden layers** create feature representations  \n",
    "✅ **Different activations** for different layers  \n",
    "✅ **Backpropagation** trains all layers together  \n",
    "✅ **Deeper ≠ always better** for simple problems  \n",
    "\n",
    "**Next:** Apply these concepts to images using convolution!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14636a09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Convolution Operations (20 minutes)\n",
    "\n",
    "## What is Convolution?\n",
    "\n",
    "**Convolution** is the core operation in CNNs. It:\n",
    "1. Takes a small filter (kernel) like 3×3\n",
    "2. Slides it across an image\n",
    "3. Performs element-wise multiplication\n",
    "4. Sums the results\n",
    "5. Creates a feature map\n",
    "\n",
    "**Why convolution for images?**\n",
    "- ✅ **Spatial locality:** Nearby pixels are related\n",
    "- ✅ **Parameter sharing:** Same filter across entire image\n",
    "- ✅ **Translation invariance:** Detects patterns anywhere\n",
    "- ✅ **Hierarchical learning:** Builds from simple to complex features\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1: Manual Convolution Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef929f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel):\n",
    "    \"\"\"\n",
    "    Apply 2D convolution manually\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : 2D array\n",
    "        Input image\n",
    "    kernel : 2D array\n",
    "        Convolution filter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : 2D array\n",
    "        Convolved feature map\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    image_h, image_w = image.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    # Calculate output size\n",
    "    output_h = image_h - kernel_h + 1\n",
    "    output_w = image_w - kernel_w + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    # Slide kernel across image\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract region\n",
    "            region = image[i:i+kernel_h, j:j+kernel_w]\n",
    "            # Element-wise multiply and sum\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"✓ Convolution function defined\")\n",
    "print(\"  This mimics how CNNs process images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e203b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2: Classic Image Filters\n",
    "\n",
    "Let's apply different filters to understand what CNNs learn!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image (simulating Sentinel-2 NIR band)\n",
    "# Simulate forest (bright) vs non-forest (dark) with edges\n",
    "test_image = np.zeros((50, 50))\n",
    "test_image[10:40, 10:25] = 0.8  # Forest patch (high NIR)\n",
    "test_image[10:40, 25:40] = 0.2  # Urban/bare soil (low NIR)\n",
    "\n",
    "# Add some noise for realism\n",
    "test_image += np.random.normal(0, 0.05, test_image.shape)\n",
    "test_image = np.clip(test_image, 0, 1)\n",
    "\n",
    "# Define classic filters\n",
    "filters = {\n",
    "    'Vertical Edge': np.array([\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1]\n",
    "    ]),\n",
    "    'Horizontal Edge': np.array([\n",
    "        [-1, -1, -1],\n",
    "        [ 0,  0,  0],\n",
    "        [ 1,  1,  1]\n",
    "    ]),\n",
    "    'Edge Detection (Sobel)': np.array([\n",
    "        [-1, -2, -1],\n",
    "        [ 0,  0,  0],\n",
    "        [ 1,  2,  1]\n",
    "    ]),\n",
    "    'Sharpen': np.array([\n",
    "        [ 0, -1,  0],\n",
    "        [-1,  5, -1],\n",
    "        [ 0, -1,  0]\n",
    "    ]),\n",
    "    'Blur (Smoothing)': np.array([\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1]\n",
    "    ]) / 9,\n",
    "    'Identity': np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"✓ Test image and filters created\")\n",
    "print(f\"  Image size: {test_image.shape}\")\n",
    "print(f\"  Number of filters: {len(filters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c634d6",
   "metadata": {},
   "source": [
    "### Apply Filters and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all filters\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(test_image, cmap='gray')\n",
    "axes[0].set_title('Original Image\\n(Simulated NIR Band)', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Apply each filter\n",
    "for idx, (name, kernel) in enumerate(filters.items(), start=1):\n",
    "    # Convolve\n",
    "    filtered = convolve2d(test_image, kernel)\n",
    "    \n",
    "    # Display\n",
    "    axes[idx].imshow(filtered, cmap='gray')\n",
    "    axes[idx].set_title(f'{name}\\nFilter', fontsize=11, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Show kernel as text\n",
    "    kernel_text = f\"Kernel:\\n{kernel}\"\n",
    "    axes[idx].text(0.5, -0.15, kernel_text, transform=axes[idx].transAxes,\n",
    "                   fontsize=7, ha='center', family='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Hide last subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Filters applied successfully!\")\n",
    "print(\"\\n🔍 Observations:\")\n",
    "print(\"  • Vertical Edge: Detects vertical boundaries (forest | urban)\")\n",
    "print(\"  • Horizontal Edge: Detects horizontal boundaries\")\n",
    "print(\"  • Sharpen: Enhances edges and details\")\n",
    "print(\"  • Blur: Smooths out noise\")\n",
    "print(\"  • Identity: Passes through unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873ebc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3: Simulate Sentinel-2 Image\n",
    "\n",
    "Let's apply filters to a more realistic Sentinel-2-like image!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic Sentinel-2 NIR band (64x64)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate different land covers\n",
    "s2_image = np.zeros((64, 64))\n",
    "\n",
    "# Forest blocks (high NIR)\n",
    "s2_image[5:25, 5:25] = 0.8 + np.random.normal(0, 0.05, (20, 20))\n",
    "s2_image[40:60, 40:60] = 0.75 + np.random.normal(0, 0.05, (20, 20))\n",
    "\n",
    "# Water (very low NIR)\n",
    "s2_image[5:25, 40:60] = 0.1 + np.random.normal(0, 0.02, (20, 20))\n",
    "\n",
    "# Agriculture (medium NIR)\n",
    "s2_image[40:60, 5:25] = 0.5 + np.random.normal(0, 0.08, (20, 20))\n",
    "\n",
    "# Urban/bare soil (low NIR)\n",
    "s2_image[25:40, 25:40] = 0.25 + np.random.normal(0, 0.05, (15, 15))\n",
    "\n",
    "# Clip to valid range\n",
    "s2_image = np.clip(s2_image, 0, 1)\n",
    "\n",
    "print(f\"✓ Synthetic Sentinel-2 image created: {s2_image.shape}\")\n",
    "print(\"  Contains: Forest, Water, Agriculture, Urban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple edge detection filters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(s2_image, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title('Original Sentinel-2 NIR\\n(Synthetic)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Vertical edges\n",
    "vert_edges = convolve2d(s2_image, filters['Vertical Edge'])\n",
    "axes[0, 1].imshow(vert_edges, cmap='seismic')\n",
    "axes[0, 1].set_title('Vertical Edge Detection\\n(Forest | Water boundary)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Horizontal edges\n",
    "horiz_edges = convolve2d(s2_image, filters['Horizontal Edge'])\n",
    "axes[0, 2].imshow(horiz_edges, cmap='seismic')\n",
    "axes[0, 2].set_title('Horizontal Edge Detection', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Sobel (combined edges)\n",
    "sobel = convolve2d(s2_image, filters['Edge Detection (Sobel)'])\n",
    "axes[1, 0].imshow(sobel, cmap='hot')\n",
    "axes[1, 0].set_title('Sobel Edge Detection\\n(All edges)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Blur (texture smoothing)\n",
    "blurred = convolve2d(s2_image, filters['Blur (Smoothing)'])\n",
    "axes[1, 1].imshow(blurred, cmap='RdYlGn')\n",
    "axes[1, 1].set_title('Blur Filter\\n(Noise reduction)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Sharpen\n",
    "sharpened = convolve2d(s2_image, filters['Sharpen'])\n",
    "axes[1, 2].imshow(sharpened, cmap='RdYlGn')\n",
    "axes[1, 2].set_title('Sharpen Filter\\n(Detail enhancement)', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Convolution filters applied to Sentinel-2-like image!\")\n",
    "print(\"\\n🎯 This is what CNNs do automatically:\")\n",
    "print(\"  • Learn optimal filters (not pre-defined)\")\n",
    "print(\"  • Stack multiple filters (32, 64, 128...)\")\n",
    "print(\"  • Build hierarchical features (edges → textures → objects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ce1e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Understanding Feature Maps\n",
    "\n",
    "When a CNN applies a filter, it creates a **feature map**. Multiple filters = multiple feature maps.\n",
    "\n",
    "**Example:** First convolutional layer in ResNet\n",
    "- Input: 64×64×10 (Sentinel-2 image)\n",
    "- Filter: 64 filters of size 3×3\n",
    "- Output: 64×64×64 (64 feature maps)\n",
    "\n",
    "Each feature map responds to different patterns!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04032192",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 Key Takeaways - Part 4\n",
    "\n",
    "✅ **Convolution = filter sliding across image**  \n",
    "✅ **Filters detect specific patterns** (edges, textures)  \n",
    "✅ **CNNs learn optimal filters** during training  \n",
    "✅ **Feature maps** are outputs of convolution  \n",
    "✅ **Multiple filters** capture different features  \n",
    "\n",
    "**Connection to EO:**\n",
    "- Layer 1 filters: Water/land boundaries, forest edges\n",
    "- Layer 2 filters: Vegetation textures, urban patterns\n",
    "- Layer 3 filters: Agricultural fields, forest stands\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89926ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: CNN Architecture Exploration (15 minutes)\n",
    "\n",
    "Now let's explore real CNN architectures and understand their components!\n",
    "\n",
    "## 5.1: Build a Simple CNN (Conceptually)\n",
    "\n",
    "Let's design a CNN for Sentinel-2 scene classification:\n",
    "\n",
    "**Task:** Classify 64×64 Sentinel-2 patches into 8 land cover classes\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input: 64×64×10 (10 Sentinel-2 bands)\n",
    "    ↓\n",
    "Conv1: 32 filters, 3×3 → 64×64×32\n",
    "ReLU activation\n",
    "MaxPool: 2×2 → 32×32×32\n",
    "    ↓\n",
    "Conv2: 64 filters, 3×3 → 32×32×64\n",
    "ReLU activation\n",
    "MaxPool: 2×2 → 16×16×64\n",
    "    ↓\n",
    "Conv3: 128 filters, 3×3 → 16×16×128\n",
    "ReLU activation\n",
    "GlobalAveragePool → 128\n",
    "    ↓\n",
    "Dense (Fully Connected): 128 → 8\n",
    "Softmax activation\n",
    "    ↓\n",
    "Output: 8 class probabilities\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2: Calculate Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc44fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cnn_parameters(architecture):\n",
    "    \"\"\"\n",
    "    Calculate number of trainable parameters in CNN\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    \n",
    "    print(\"CNN Architecture Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for layer_name, layer_info in architecture.items():\n",
    "        if 'conv' in layer_name.lower():\n",
    "            # Convolution layer: (filter_h * filter_w * in_channels + 1) * out_channels\n",
    "            kernel_h, kernel_w = layer_info['kernel_size']\n",
    "            in_channels = layer_info['in_channels']\n",
    "            out_channels = layer_info['out_channels']\n",
    "            \n",
    "            params = (kernel_h * kernel_w * in_channels + 1) * out_channels\n",
    "            total_params += params\n",
    "            \n",
    "            print(f\"{layer_name}:\")\n",
    "            print(f\"  Kernel: {kernel_h}×{kernel_w}, In: {in_channels}, Out: {out_channels}\")\n",
    "            print(f\"  Parameters: {params:,}\")\n",
    "            \n",
    "        elif 'dense' in layer_name.lower():\n",
    "            # Dense layer: (input_size + 1) * output_size\n",
    "            input_size = layer_info['input_size']\n",
    "            output_size = layer_info['output_size']\n",
    "            \n",
    "            params = (input_size + 1) * output_size\n",
    "            total_params += params\n",
    "            \n",
    "            print(f\"{layer_name}:\")\n",
    "            print(f\"  Input: {input_size}, Output: {output_size}\")\n",
    "            print(f\"  Parameters: {params:,}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return total_params\n",
    "\n",
    "# Define our CNN architecture\n",
    "our_cnn = {\n",
    "    'Conv1': {'kernel_size': (3, 3), 'in_channels': 10, 'out_channels': 32},\n",
    "    'Conv2': {'kernel_size': (3, 3), 'in_channels': 32, 'out_channels': 64},\n",
    "    'Conv3': {'kernel_size': (3, 3), 'in_channels': 64, 'out_channels': 128},\n",
    "    'Dense': {'input_size': 128, 'output_size': 8}\n",
    "}\n",
    "\n",
    "params = calculate_cnn_parameters(our_cnn)\n",
    "\n",
    "print(f\"\\n💡 For comparison:\")\n",
    "print(f\"  ResNet50: ~25 million parameters\")\n",
    "print(f\"  VGG16: ~138 million parameters\")\n",
    "print(f\"  Our simple CNN: {params:,} parameters\")\n",
    "print(f\"\\n  → Lightweight, suitable for small datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a3186",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.3: Visualize CNN Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the architecture flow\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Define layer positions and sizes\n",
    "layers = [\n",
    "    {'name': 'Input\\n64×64×10', 'x': 0, 'y': 0.5, 'w': 0.8, 'h': 0.8, 'color': 'lightblue'},\n",
    "    {'name': 'Conv1 + ReLU\\n64×64×32', 'x': 1.5, 'y': 0.5, 'w': 0.7, 'h': 0.7, 'color': 'lightcoral'},\n",
    "    {'name': 'MaxPool\\n32×32×32', 'x': 2.8, 'y': 0.5, 'w': 0.6, 'h': 0.6, 'color': 'lightyellow'},\n",
    "    {'name': 'Conv2 + ReLU\\n32×32×64', 'x': 4.0, 'y': 0.5, 'w': 0.6, 'h': 0.6, 'color': 'lightcoral'},\n",
    "    {'name': 'MaxPool\\n16×16×64', 'x': 5.2, 'y': 0.5, 'w': 0.5, 'h': 0.5, 'color': 'lightyellow'},\n",
    "    {'name': 'Conv3 + ReLU\\n16×16×128', 'x': 6.4, 'y': 0.5, 'w': 0.5, 'h': 0.5, 'color': 'lightcoral'},\n",
    "    {'name': 'Global\\nAvgPool', 'x': 7.6, 'y': 0.5, 'w': 0.3, 'h': 0.8, 'color': 'lightyellow'},\n",
    "    {'name': 'Dense\\n8 classes', 'x': 8.5, 'y': 0.5, 'w': 0.3, 'h': 0.6, 'color': 'lightgreen'},\n",
    "]\n",
    "\n",
    "# Draw layers\n",
    "for layer in layers:\n",
    "    rect = plt.Rectangle((layer['x'] - layer['w']/2, layer['y'] - layer['h']/2),\n",
    "                          layer['w'], layer['h'], \n",
    "                          facecolor=layer['color'], edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(layer['x'], layer['y'], layer['name'], \n",
    "            ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "for i in range(len(layers) - 1):\n",
    "    ax.arrow(layers[i]['x'] + layers[i]['w']/2 + 0.05, \n",
    "             layers[i]['y'],\n",
    "             layers[i+1]['x'] - layers[i+1]['w']/2 - layers[i]['x'] - layers[i]['w']/2 - 0.15,\n",
    "             0, head_width=0.1, head_length=0.1, fc='gray', ec='gray')\n",
    "\n",
    "ax.set_xlim(-0.5, 9.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('CNN Architecture for Sentinel-2 Scene Classification', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, fc='lightblue', ec='black', label='Input'),\n",
    "    plt.Rectangle((0, 0), 1, 1, fc='lightcoral', ec='black', label='Convolution + ReLU'),\n",
    "    plt.Rectangle((0, 0), 1, 1, fc='lightyellow', ec='black', label='Pooling'),\n",
    "    plt.Rectangle((0, 0), 1, 1, fc='lightgreen', ec='black', label='Dense/Output')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper center', \n",
    "          bbox_to_anchor=(0.5, -0.05), ncol=4, frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ CNN architecture visualized!\")\n",
    "print(\"\\n📐 Layer Dimensions:\")\n",
    "print(\"  Notice how spatial dimensions decrease (64→32→16)\")\n",
    "print(\"  While channels increase (10→32→64→128)\")\n",
    "print(\"  This is typical: trade spatial resolution for semantic features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7eb31e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.4: Compare with Random Forest\n",
    "\n",
    "Let's understand when to use CNNs vs Random Forest for EO tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdabdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Input Type',\n",
    "        'Feature Engineering',\n",
    "        'Spatial Context',\n",
    "        'Training Data Needed',\n",
    "        'Training Time',\n",
    "        'Inference Speed',\n",
    "        'Interpretability',\n",
    "        'Typical Accuracy',\n",
    "        'Hardware',\n",
    "        'Best Use Case'\n",
    "    ],\n",
    "    'Random Forest\\n(Sessions 1-2)': [\n",
    "        'Pixel features',\n",
    "        'Manual (GLCM, NDVI, etc.)',\n",
    "        'Limited (neighborhood)',\n",
    "        '100-1000 samples',\n",
    "        'Minutes',\n",
    "        'Very fast (ms)',\n",
    "        'High (feature importance)',\n",
    "        '80-90%',\n",
    "        'CPU sufficient',\n",
    "        'Quick prototypes, small areas'\n",
    "    ],\n",
    "    'CNN\\n(Sessions 3-4)': [\n",
    "        'Image patches',\n",
    "        'Automatic (learned)',\n",
    "        'Hierarchical (receptive field)',\n",
    "        '1000-100K+ images',\n",
    "        'Hours-Days',\n",
    "        'Fast with GPU (10-100ms)',\n",
    "        'Low (black box)',\n",
    "        '90-98%',\n",
    "        'GPU recommended',\n",
    "        'Production, large areas, high accuracy'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display as formatted table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RANDOM FOREST vs CONVOLUTIONAL NEURAL NETWORKS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, aspect in enumerate(comparison_data['Aspect']):\n",
    "    rf_value = comparison_data['Random Forest\\n(Sessions 1-2)'][i]\n",
    "    cnn_value = comparison_data['CNN\\n(Sessions 3-4)'][i]\n",
    "    \n",
    "    print(f\"\\n{aspect}:\")\n",
    "    print(f\"  RF:  {rf_value}\")\n",
    "    print(f\"  CNN: {cnn_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "print(\"\\n🎯 Decision Guide:\")\n",
    "print(\"\\n  Use RANDOM FOREST when:\")\n",
    "print(\"    • You have <1000 training samples\")\n",
    "print(\"    • Quick results needed (hours, not days)\")\n",
    "print(\"    • Interpretability is important\")\n",
    "print(\"    • No GPU available\")\n",
    "print(\"\\n  Use CNN when:\")\n",
    "print(\"    • You have >1000 labeled images\")\n",
    "print(\"    • Highest accuracy is critical\")\n",
    "print(\"    • Production deployment planned\")\n",
    "print(\"    • GPU resources available\")\n",
    "print(\"\\n  🌟 BEST PRACTICE: Start with RF, upgrade to CNN if needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87c9fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 Key Takeaways - Part 5\n",
    "\n",
    "✅ **CNN architecture:** Input → Conv → Pool → ... → Dense → Output  \n",
    "✅ **Parameters scale quickly:** Deeper networks = more parameters  \n",
    "✅ **Spatial dimensions decrease:** While semantic depth increases  \n",
    "✅ **Choose wisely:** RF for quick work, CNN for production  \n",
    "✅ **Transfer learning helps:** Use pre-trained models  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bf040",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎓 Session Complete! Summary\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "### Part 1: Perceptron\n",
    "- ✅ Built artificial neuron from scratch\n",
    "- ✅ Understood weights, bias, activation\n",
    "- ✅ Trained using gradient descent\n",
    "- ✅ Visualized decision boundary\n",
    "\n",
    "### Part 2: Activation Functions\n",
    "- ✅ Explored ReLU, Sigmoid, Tanh\n",
    "- ✅ Understood non-linearity importance\n",
    "- ✅ Saw vanishing gradient problem\n",
    "- ✅ Learned why ReLU is popular\n",
    "\n",
    "### Part 3: Neural Networks\n",
    "- ✅ Built multi-layer network\n",
    "- ✅ Implemented forward propagation\n",
    "- ✅ Understood backpropagation\n",
    "- ✅ Compared with perceptron\n",
    "\n",
    "### Part 4: Convolution Operations\n",
    "- ✅ Applied filters to images manually\n",
    "- ✅ Visualized edge detection\n",
    "- ✅ Processed Sentinel-2-like data\n",
    "- ✅ Understood feature maps\n",
    "\n",
    "### Part 5: CNN Architectures\n",
    "- ✅ Designed CNN for EO classification\n",
    "- ✅ Calculated parameters\n",
    "- ✅ Visualized architecture flow\n",
    "- ✅ Compared RF vs CNN\n",
    "\n",
    "---\n",
    "\n",
    "## Ready for Session 4!\n",
    "\n",
    "In the next session, you'll:\n",
    "- 🔨 Build actual CNNs with TensorFlow/Keras\n",
    "- 🌲 Train on real Palawan land cover data\n",
    "- 🎯 Implement U-Net for segmentation\n",
    "- 📊 Compare results with Random Forest\n",
    "- 🚀 Apply transfer learning\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Additional Practice (Optional)\n",
    "\n",
    "**Exercises to Try:**\n",
    "\n",
    "1. **Modify the Perceptron**\n",
    "   - Add a third feature (elevation)\n",
    "   - Try different learning rates\n",
    "   - Visualize in 3D\n",
    "\n",
    "2. **Experiment with Activations**\n",
    "   - Replace ReLU with Tanh in the neural network\n",
    "   - Compare training dynamics\n",
    "   - Plot accuracy curves\n",
    "\n",
    "3. **Custom Filters**\n",
    "   - Design your own 3×3 filter\n",
    "   - Test on the Sentinel-2 image\n",
    "   - Explain what pattern it detects\n",
    "\n",
    "4. **Architecture Design**\n",
    "   - Design a CNN for 10-class classification\n",
    "   - Calculate total parameters\n",
    "   - Keep it under 100K parameters!\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Key Concepts to Remember\n",
    "\n",
    "**From Random Forest to CNNs:**\n",
    "- RF: Manual features → Tree ensemble → Classification\n",
    "- CNN: Raw pixels → Learned filters → Feature hierarchy → Classification\n",
    "\n",
    "**Why CNNs Excel at Images:**\n",
    "- Spatial locality (nearby pixels related)\n",
    "- Parameter sharing (same filter everywhere)\n",
    "- Hierarchical features (edges → textures → objects)\n",
    "- End-to-end learning (optimize everything together)\n",
    "\n",
    "**When CNNs Are Worth It:**\n",
    "- Large labeled dataset (>1000 images)\n",
    "- GPU available\n",
    "- Accuracy is critical\n",
    "- Production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 Resources for Deeper Learning\n",
    "\n",
    "**Interactive:**\n",
    "- [TensorFlow Playground](https://playground.tensorflow.org/)\n",
    "- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
    "- [Distill.pub Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "**Courses:**\n",
    "- Deep Learning Specialization (Coursera) - Andrew Ng\n",
    "- Fast.ai Practical Deep Learning\n",
    "- CS231n (Stanford) - CNNs for Visual Recognition\n",
    "\n",
    "**Papers:**\n",
    "- LeCun et al. (1998) - Gradient-Based Learning\n",
    "- Krizhevsky et al. (2012) - AlexNet\n",
    "- He et al. (2016) - ResNet\n",
    "\n",
    "**EO-Specific:**\n",
    "- EuroSAT Dataset\n",
    "- TorchGeo Library\n",
    "- Awesome Satellite Imagery Repo\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! 🎉**\n",
    "\n",
    "You now understand the fundamentals of deep learning and CNNs. Time to put it into practice in Session 4!\n",
    "\n",
    "[Continue to Session 4 →](../../session4/notebooks/)\n",
    "\n",
    "---\n",
    "\n",
    "*Session 3 Theory Notebook - CoPhil Advanced Training Program*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
